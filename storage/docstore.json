{"docstore/data": {"4437d6b6-ead6-4a1d-9251-df93995d95b8": {"__data__": {"id_": "4437d6b6-ead6-4a1d-9251-df93995d95b8", "embedding": null, "metadata": {"filename": "docs\\DOCS_README.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a7df91f0acf9a51e5f123e147fd874bac68c34e", "node_type": null, "metadata": {"filename": "docs\\DOCS_README.md", "author": "LlamaIndex"}, "hash": "642730374fd08d587f79e3f08f692ca3029d6234a0139f00116c0ab4e3d74df3"}}, "hash": "54416aac34b61b6621f1454d655bdb090c5c581815eb503601402e6383d0a7aa", "text": "# Documentation Guide\n\n## A guide for docs contributors\n\nThe `docs` directory contains the sphinx source text for LlamaIndex docs, visit\nhttps://gpt-index.readthedocs.io/ to read the full documentation.\n\nThis guide is made for anyone who's interested in running LlamaIndex documentation locally,\nmaking changes to it and make contributions. LlamaIndex is made by the thriving community\nbehind it, and you're always welcome to make contributions to the project and the \ndocumentation. \n\n## Build Docs\n\nIf you haven't already, clone the LlamaIndex Github repo to a local directory:\n\n```bash\ngit clone https://github.com/jerryjliu/llama_index.git && cd llama_index\n```\n\nInstall all dependencies required for building docs (mainly `sphinx` and its extension):\n\n```bash\npip install -r docs/requirements.txt\n```\n\nBuild the sphinx docs:\n\n```bash\ncd docs\nmake html\n```\n\nThe docs HTML files are now generated under `docs/_build/html` directory, you can preview\nit locally with the following command:\n\n```bash\npython -m http.server 8000 -d _build/html\n```\n\nAnd open your browser at http://0.0.0.0:8000/ to view the generated docs.\n\n\n##### Watch Docs\n\nWe recommend using sphinx-autobuild during development, which provides a live-reloading \nserver, that rebuilds the documentation and refreshes any open pages automatically when \nchanges are saved. This enables a much shorter feedback loop which can help boost \nproductivity when writing documentation.\n\nSimply run the following command from LlamaIndex project's root directory: \n```bash\nmake watch-docs\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e6af2f51-3bf0-45b1-8297-620133b65139": {"__data__": {"id_": "e6af2f51-3bf0-45b1-8297-620133b65139", "embedding": null, "metadata": {"filename": "docs\\community\\app_showcase.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cac394d1eca7c5b0b16ee0b4d7e69393162fa47", "node_type": null, "metadata": {"filename": "docs\\community\\app_showcase.md", "author": "LlamaIndex"}, "hash": "d93c9ab8587cc7d6af62c97d6d61c3c770fcf96b343568f5c2fae6718ce0d73f"}, "3": {"node_id": "e75ac405-364a-43a9-b9c3-95e2bf6f4185", "node_type": null, "metadata": {"filename": "docs\\community\\app_showcase.md", "author": "LlamaIndex"}, "hash": "26a738b98bc633850a7729bdc68df43ec587206bacad1291e3aff4b78c058470"}}, "hash": "62d895fcae9e4d60968a63ed62b2f00bf5abf3e4599896874c52774e70e82e90", "text": "# App Showcase\n\nHere is a sample of some of the incredible applications and tools built on top of LlamaIndex! \n\n###### SEC Insights - Answer questions about SEC 10-K & 10-Q documents (built by LlamaIndex!)\nSEC Insights uses the Retrieval Augmented Generation (RAG) capabilities of LlamaIndex to answer questions about SEC 10-K & 10-Q documents.\n\nWe built and open-sourced SEC Insights so that we could provide our developer community with an example of a production-ready full-stack application that uses LlamaIndex. It comes with many product features that we think users will love as well as development features that we think developers will love. You can use the Github repo as a reference when building out your own LlamaIndex application or you can fork it to start your project off with a solid Next.js + FastAPI codebase.\n\n[[Website]](https://www.secinsights.ai/)\n[[Github]](https://github.com/run-llama/sec-insights/)\n[[Tweet thread]](https://twitter.com/jerryjliu0/status/1699119197190775084?s=20)\n\n###### Meru - Dense Data Retrieval API\n\nHosted API service. Includes a \"Dense Data Retrieval\" API built on top of LlamaIndex where users can upload their documents and query them.\n[[Website]](https://www.usemeru.com/densedataretrieval)\n\n###### Algovera\n\nBuild AI workflows using building blocks. Many workflows built on top of LlamaIndex.\n\n[[Website]](https://app.algovera.ai/workflows).\n\n###### SlideSpeak\n\nSummarize PowerPoint files and other documents with AI. SlideSpeak is an open source chatbot for presentations and other documents. Built on top of LlamaIndex, it utilizes Pinecone as a Vector Storage. We currently use ChatGPT Turbo 3.5 as a model for the chatbot. We're currently working on adding support for other document formats, which will allow you to summarize presentations, Word documents, Google Slides, PDFs and much more.\n\n[[Website]](https://slidespeak.co/?ref=llamaindex)\n[[GitHub]](https://github.com/SlideSpeak/slidespeak-backend)\n\n###### ChatGPT LlamaIndex\n\nInterface that allows users to upload long docs and chat with the bot.\n[[Tweet thread]](https://twitter.com/s_jobs6/status/1618346125697875968?s=20&t=RJhQu2mD0-zZNGfq65xodA)\n\n###### AgentHQ\n\nA web tool to build agents, interacting with LlamaIndex data structures.[[Website]](https://app.agent-hq.io/)\n\n###### SiteChatAI\n\nSiteChatAi is ChatGPT powered that can be integrated into any website. It is a simple chatbot that can be used to answer simple questions and can be trained to answer more complex questions. It provides human like conversation experience to the users. It can be used to answer questions related to the website or the business. It uses Llamma Index and LangChain\n  \nCurrent version of SiteChatAI support following features:  \n- Multi-lingual support  \n- Real time chat  \n- Easy to integrate  \n- Customizable  \n- Human like conversation experience  \n- Can be trained to answer complex questions\n- and more.\n\n  \n[[Website]](https://sitechatai.com/?utm_source=llama-index&utm_medium=app%20showcase&utm_campaign=promotion)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e75ac405-364a-43a9-b9c3-95e2bf6f4185": {"__data__": {"id_": "e75ac405-364a-43a9-b9c3-95e2bf6f4185", "embedding": null, "metadata": {"filename": "docs\\community\\app_showcase.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cac394d1eca7c5b0b16ee0b4d7e69393162fa47", "node_type": null, "metadata": {"filename": "docs\\community\\app_showcase.md", "author": "LlamaIndex"}, "hash": "d93c9ab8587cc7d6af62c97d6d61c3c770fcf96b343568f5c2fae6718ce0d73f"}, "2": {"node_id": "e6af2f51-3bf0-45b1-8297-620133b65139", "node_type": null, "metadata": {"filename": "docs\\community\\app_showcase.md", "author": "LlamaIndex"}, "hash": "62d895fcae9e4d60968a63ed62b2f00bf5abf3e4599896874c52774e70e82e90"}}, "hash": "26a738b98bc633850a7729bdc68df43ec587206bacad1291e3aff4b78c058470", "text": "###### PapersGPT\n\nFeed any of the following content into GPT to give it deep customized knowledge:\n- Scientific Papers\n- Substack Articles\n- Podcasts\n- Github Repos\nand more.\n\n[[Tweet thread]](https://twitter.com/thejessezhang/status/1615390646763945991?s=20&t=eHvhmIaaaoYFyPSzDRNGtA)\n[[Website]](https://jessezhang.org/llmdemo)\n\n###### VideoQues + DocsQues\n\n**VideoQues**: A tool that answers your queries on YouTube videos. \n[[LinkedIn post here]](https://www.linkedin.com/posts/ravidesetty_ai-ml-dl-activity-7020599110953050112-EJA_/?utm_source=share&utm_medium=member_desktop).\n\n**DocsQues**: A tool that answers your questions on longer documents (including .pdfs!)\n[[LinkedIn post here]](https://www.linkedin.com/posts/ravidesetty_artificialintelligence-machinelearning-recruiters-activity-7016972785293946880-rhKC?utm_source=share&utm_medium=member_desktop).\n\n###### PaperBrain\n\nA platform to access/understand research papers.\n\n[[Tweet thread]](https://twitter.com/mdarshad1000/status/1619824637898264578?s=20&t=eHvhmIaaaoYFyPSzDRNGtA).\n\n\n###### CACTUS\nContextual search on top of LinkedIn search results. \n[[LinkedIn post here]](https://www.linkedin.com/posts/mathewteoh_chromeextension-chatgpt-python-activity-7019362515566403584-ryqW?utm_source=share&utm_medium=member_desktop).\n\n\n###### Personal Note Chatbot\nA chatbot that can answer questions over a directory of Obsidian notes. \n[[Tweet thread]](https://twitter.com/Sarah_A_Bentley/status/1611069576099336207?s=20&t=IjPLK3msACQjEBYxJJxj4w).\n\n\n###### RHOBH AMA\n\nAsk questions about the Real Housewives of Beverly Hills.\n[[Tweet thread]](https://twitter.com/YourBuddyConner/status/1616504644439789568?s=20&t=bCHa3im7mjoIXLuKo5PttQ)\n[[Website]](https://realhousewivesai.com/)\n\n###### Mynd\n\nA journaling app that uses AI to uncover insights and patterns over time.\n[[Website]](https://mynd.so)\n\n###### CoFounder\nThe First AI Co-Founder for Your Start-up \ud83d\ude4c\n\n[CoFounder](https://co-founder.ai?utm_source=llama-index&utm_medium=gallary&utm_campaign=alpha) is a platform to revolutionize the start-up ecosystem by providing founders with unparalleled tools, resources, and support. We are changing how founders build their companies from 0-1\u2014productizing the accelerator/incubator programs using AI.\n\nCurrent features:\n\n* AI Investor Matching and Introduction and Tracking\n* AI Pitch Deck creation\n* Real-time Pitch Deck practice/feedback\n* Automatic Competitive Analysis / Watchlist\n* More coming soon...\n\n[[Website]](https://co-founder.ai?utm_source=llama-index&utm_medium=gallary&utm_campaign=alpha)\n\n###### Al-X by OpenExO\n\nYour Digital Transformation Co-Pilot\n[[Website]](https://chat.openexo.com)\n\n###### AnySummary\n\nSummarize any document, audio or video with AI\n[[Website]](https://anysummary.app)\n\n###### Blackmaria\n\nPython package for webscraping in Natural language.\n[[Tweet thread]](https://twitter.com/obonigwe1/status/1640080422661943298?t=aftqisb4vaudwrgwah_1oa&s=19)\n[[Github]](https://github.com/Smyja/blackmaria)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "679f0feb-a0e4-450e-9d1d-f93372bc8235": {"__data__": {"id_": "679f0feb-a0e4-450e-9d1d-f93372bc8235", "embedding": null, "metadata": {"filename": "docs\\community\\integrations.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e72ada566eb09c9ef1cf9fbf1019e9b5c166bcc8", "node_type": null, "metadata": {"filename": "docs\\community\\integrations.md", "author": "LlamaIndex"}, "hash": "b37b015e5f936969505a684dc4a4a7e2ea2e63ec2eff403f4aeb018ce7ce5343"}}, "hash": "a1054e802779c7c929f71f93080ad19647cbdc66580ef7d891ed14bed02ac019", "text": "# Integrations\n\nLlamaIndex has a number of community integrations, from vector stores, to prompt trackers, tracers, and more!\n\n## Data Loaders\n\nThe full set of data loaders are found on [LlamaHub](https://llamahub.ai/)\n\n## Agent Tools\nThe full set of agent tools are found on [LlamaHub](https://llamahub.ai/)\n\n## LLMs\nThe full set of supported LLMs are found [here](/core_modules/model_modules/llms/modules.md).\n\n\n## Observability/Tracing/Evaluation\n\nCheck out our [one-click observability](/end_to_end_tutorials/one_click_observability.md) page\nfor full tracing integrations.\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/end_to_end_tutorials/one_click_observability.md\nintegrations/graphsignal.md\nintegrations/trulens.md\nintegrations/deepeval.md\n\n```\n\n## Structured Outputs\n```{toctree}\n---\nmaxdepth: 1\n---\nintegrations/guidance.md\nGuardrails </examples/output_parsing/GuardrailsDemo.ipynb>\nOpenAI Function Calling </examples/output_parsing/openai_pydantic_program.ipynb>\n```\n\n## Storage and Managed Indexes\n```{toctree}\n---\nmaxdepth: 1\n---\nintegrations/vector_stores.md\nintegrations/graph_stores.md\nintegrations/managed_indices.md\n```\n\n## Application Frameworks\n```{toctree}\n---\nmaxdepth: 1\n---\nintegrations/using_with_langchain.md\nStreamlit <https://blog.streamlit.io/build-a-chatbot-with-custom-data-sources-powered-by-llamaindex/>\nChainlit <https://docs.chainlit.io/integrations/llama-index>\n```\n\n## Distributed Compute\n```{toctree}\n---\nmaxdepth: 1\n---\nLlamaIndex + Ray <https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray>\n\n```\n\n## Other\n```{toctree}\n---\nmaxdepth: 1\n---\nintegrations/chatgpt_plugins.md\nPoe <https://github.com/poe-platform/poe-protocol/tree/main/llama_poe>\nAirbyte <https://airbyte.com/tutorials/airbyte-and-llamaindex-elt-and-chat-with-your-data-warehouse-without-writing-sql>\n\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1e6e7b26-278d-414d-a59e-556fbe6e211f": {"__data__": {"id_": "1e6e7b26-278d-414d-a59e-556fbe6e211f", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\chatgpt_plugins.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37f793377e680750a22fe70a490a7f17f741b9ba", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\chatgpt_plugins.md", "author": "LlamaIndex"}, "hash": "3d689cc8deddd7497182cb11db52aa68a591dc013e69a5815e14976296c2c77f"}, "3": {"node_id": "afa369ba-648a-4758-803a-1386ec8dfff9", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\chatgpt_plugins.md", "author": "LlamaIndex"}, "hash": "2b3d84dc959562a13212748500bb80d6d93108172cd7ce6f010c1e9376406122"}}, "hash": "2982d52aabc142e93f025dd11666ac514edc50a20e73581b4956013392d6f1d4", "text": "# ChatGPT Plugin Integrations\n\n**NOTE**: This is a work-in-progress, stay tuned for more exciting updates on this front!## ChatGPT Retrieval Plugin Integrations\n\nThe [OpenAI ChatGPT Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin)\noffers a centralized API specification for any document storage system to interact \nwith ChatGPT.Since this can be deployed on any service, this means that more and more\ndocument retrieval services will implement this spec; this allows them to not only\ninteract with ChatGPT, but also interact with any LLM toolkit that may use \na retrieval service.LlamaIndex provides a variety of integrations with the ChatGPT Retrieval Plugin.### Loading Data from LlamaHub into the ChatGPT Retrieval Plugin\n\nThe ChatGPT Retrieval Plugin defines an `/upsert` endpoint for users to load\ndocuments.This offers a natural integration point with LlamaHub, which offers\nover 65 data loaders from various API's and document formats.Here is a sample code snippet of showing how to load a document from LlamaHub\ninto the JSON format that `/upsert` expects:\n\n```python\nfrom llama_index import download_loader, Document\nfrom typing import Dict, List\nimport json\n\n# download loader, load documents\nSimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\nloader = SimpleWebPageReader(html_to_text=True)\nurl = \"http://www.paulgraham.com/worked.html\"\ndocuments = loader.load_data(urls=[url])\n\n# Convert LlamaIndex Documents to JSON format\ndef dump_docs_to_json(documents: List[Document], out_path: str) -> Dict:\n    \"\"\"Convert LlamaIndex Documents to JSON format and save it.\"\"\"result_json = []\n    for doc in documents:\n        cur_dict = {\n            \"text\": doc.get_text(),\n            \"id\": doc.get_doc_id(),\n            # NOTE: feel free to customize the other fields as you wish\n            # fields taken from https://github.com/openai/chatgpt-retrieval-plugin/tree/main/scripts/process_json#usage\n            # \"source\": ...,\n            # \"source_id\": ...,\n            # \"url\": url,\n            # \"created_at\": ...,\n            # \"author\": \"Paul Graham\",\n        }\n        result_json.append(cur_dict)\n    \n    json.dump(result_json, open(out_path, 'w'))\n\n```\n\nFor more details, check out the [full example notebook](https://github.com/jerryjliu/llama_index/blob/main/examples/chatgpt_plugin/ChatGPT_Retrieval_Plugin_Upload.ipynb).### ChatGPT Retrieval Plugin Data Loader\n\nThe ChatGPT Retrieval Plugin data loader [can be accessed on LlamaHub](https://llamahub.ai/l/chatgpt_plugin).It allows you to easily load data from any docstore that implements the plugin API, into a LlamaIndex data structure.Example code:\n\n```python\nfrom llama_index.readers import ChatGPTRetrievalPluginReader\nimport os\n\n# load documents\nbearer_token = os.getenv(\"BEARER_TOKEN\")\nreader = ChatGPTRetrievalPluginReader(\n    endpoint_url=\"http://localhost:8000\",\n    bearer_token=bearer_token\n)\ndocuments = reader.load_data(\"What did the author do growing up?\")", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "afa369ba-648a-4758-803a-1386ec8dfff9": {"__data__": {"id_": "afa369ba-648a-4758-803a-1386ec8dfff9", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\chatgpt_plugins.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37f793377e680750a22fe70a490a7f17f741b9ba", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\chatgpt_plugins.md", "author": "LlamaIndex"}, "hash": "3d689cc8deddd7497182cb11db52aa68a591dc013e69a5815e14976296c2c77f"}, "2": {"node_id": "1e6e7b26-278d-414d-a59e-556fbe6e211f", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\chatgpt_plugins.md", "author": "LlamaIndex"}, "hash": "2982d52aabc142e93f025dd11666ac514edc50a20e73581b4956013392d6f1d4"}}, "hash": "2b3d84dc959562a13212748500bb80d6d93108172cd7ce6f010c1e9376406122", "text": "# build and query index\nfrom llama_index import SummaryIndex\nindex = SummaryIndex.from_documents(documents)\n# set Logging to DEBUG for more detailed outputs\nquery_engine = vector_index.as_query_engine(\n    response_mode=\"compact\"\n)\nresponse = query_engine.query(\n    \"Summarize the retrieved content and describe what the author did growing up\",\n) \n\n```\nFor more details, check out the [full example notebook](https://github.com/jerryjliu/llama_index/blob/main/examples/chatgpt_plugin/ChatGPTRetrievalPluginReaderDemo.ipynb).### ChatGPT Retrieval Plugin Index\n\nThe ChatGPT Retrieval Plugin Index allows you to easily build a vector index over any documents, with storage backed by a document store implementing the \nChatGPT endpoint.Note: this index is a vector index, allowing top-k retrieval.Example code:\n\n```python\nfrom llama_index.indices.vector_store import ChatGPTRetrievalPluginIndex\nfrom llama_index import SimpleDirectoryReader\nimport os\n\n# load documents\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n\n# build index\nbearer_token = os.getenv(\"BEARER_TOKEN\")\n# initialize without metadata filter\nindex = ChatGPTRetrievalPluginIndex(\n    documents, \n    endpoint_url=\"http://localhost:8000\",\n    bearer_token=bearer_token,\n)\n\n# query index\nquery_engine = vector_index.as_query_engine(\n    similarity_top_k=3,\n    response_mode=\"compact\",\n)\nresponse = query_engine.query(\"What did the author do growing up?\")```\n\nFor more details, check out the [full example notebook](https://github.com/jerryjliu/llama_index/blob/main/examples/chatgpt_plugin/ChatGPTRetrievalPluginIndexDemo.ipynb).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5bf8d116-1a65-47ea-b92c-dea5bee72bf3": {"__data__": {"id_": "5bf8d116-1a65-47ea-b92c-dea5bee72bf3", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\deepeval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77dce2cb9834e28bbcfe2f1a4a4ff4cd81bd7993", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\deepeval.md", "author": "LlamaIndex"}, "hash": "0db64c91ac05e7f4b1671606f1af0319c23ef91684ddaa1bb376b4b0c90fc5d2"}, "3": {"node_id": "b025c8a5-eb7a-481d-9fed-688ab2e976be", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\deepeval.md", "author": "LlamaIndex"}, "hash": "9f32801c51014e7f3215590820ee025df9c6c8571479a4aa15a18b1fb5f9a29a"}}, "hash": "0f673f6ceb9420d97a2ec06aeddff48b4cd0f87ba95963a9cfdbdde4d0a2a87b", "text": "# Unit Testing LLMs With DeepEval\n\n[DeepEval](https://github.com/confident-ai/deepeval) provides unit testing for AI agents and LLM-powered applications.It provides a really simple interface for LlamaIndex developers to write tests and helps developers ensure AI applications run as expected.DeepEval provides an opinionated framework to measure responses and is completely open-source.### Installation and Setup\n\nAdding [DeepEval](https://github.com/confident-ai/deepeval) is simple, just install and configure it:\n\n```sh\npip install -q -q llama-index\npip install -U deepeval\n```\n\nOnce installed , you can get set up and start writing tests.```sh\n# Optional step: Login to get a nice dashboard for your tests later!# During this step - make sure to save your project as llama\ndeepeval login\ndeepeval test generate test_sample.py\n```\n\nYou can then run tests as such:\n\n```bash\ndeepeval test run test_sample.py\n```\n\nAfter running this, you will get a beautiful dashboard like so:\n\n![Sample dashboard](https://raw.githubusercontent.com/confident-ai/deepeval/main/docs/assets/dashboard-screenshot.png)\n\n## Types of Tests\n\nDeepEval presents an opinionated framework for the types of tests that are being run.It breaks down LLM outputs into: \n- Answer Relevancy - [Read more here](https://docs.confident-ai.com/docs/measuring_llm_performance/answer_relevancy)\n- Factual Consistency (to measure the extent of hallucinations) - [Read more here](https://docs.confident-ai.com/docs/measuring_llm_performance/factual_consistency)\n- Conceptual Similarity (to know if answers are in line with expectations) - [Read more here](https://docs.confident-ai.com/docs/measuring_llm_performance/conceptual_similarity)\n- Toxicness - [Read more here](https://docs.confident-ai.com/docs/measuring_llm_performance/non_toxic)\n- Bias (can come up from finetuning) - [Read more here](https://docs.confident-ai.com/docs/measuring_llm_performance/debias)\n\nYou can more about the [DeepEval Framework](https://docs.confident-ai.com/docs/framework) here.## Use With Your LlamaIndex\n\nDeepEval integrates nicely with LlamaIndex's `ResponseEvaluator` class.Below is an example of the factual consistency documentation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b025c8a5-eb7a-481d-9fed-688ab2e976be": {"__data__": {"id_": "b025c8a5-eb7a-481d-9fed-688ab2e976be", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\deepeval.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77dce2cb9834e28bbcfe2f1a4a4ff4cd81bd7993", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\deepeval.md", "author": "LlamaIndex"}, "hash": "0db64c91ac05e7f4b1671606f1af0319c23ef91684ddaa1bb376b4b0c90fc5d2"}, "2": {"node_id": "5bf8d116-1a65-47ea-b92c-dea5bee72bf3", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\deepeval.md", "author": "LlamaIndex"}, "hash": "0f673f6ceb9420d97a2ec06aeddff48b4cd0f87ba95963a9cfdbdde4d0a2a87b"}}, "hash": "9f32801c51014e7f3215590820ee025df9c6c8571479a4aa15a18b1fb5f9a29a", "text": "```python\n\nfrom llama_index.response.schema import Response\nfrom typing import List\nfrom llama_index.schema import Document\nfrom deepeval.metrics.factual_consistency import FactualConsistencyMetric\n\nfrom llama_index import (\n    TreeIndex,\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext,\n    Response,\n)\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import ResponseEvaluator\n\nimport os\nimport openai\n\napi_key = \"sk-XXX\"\nopenai.api_key = api_key\n\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\", api_key=api_key)\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\nevaluator_gpt4 = ResponseEvaluator(service_context=service_context_gpt4)\n\n```\n\n#### Getting a lLamaHub Loader \n\n```python\nfrom llama_index import download_loader\n\nWikipediaReader = download_loader(\"WikipediaReader\")\n\nloader = WikipediaReader()\ndocuments = loader.load_data(pages=['Tokyo'])\ntree_index = TreeIndex.from_documents(documents=documents)\nvector_index = VectorStoreIndex.from_documents(\n    documents, service_context=service_context_gpt4\n)\n```\n\nWe then build an evaluator based on the `BaseEvaluator` class that requires an `evaluate` method.In this example, we show you how to write a factual consistency check.```python\nclass FactualConsistencyResponseEvaluator:\n  def get_context(self, response: Response) -> List[Document]:\n    \"\"\"Get context information from given Response object using source nodes.Args:\n        response (Response): Response object from an index based on the query.Returns:\n        List of Documents of source nodes information as context information.\n    \"\"\"context = []\n\n    for context_info in response.source_nodes:\n        context.append(Document(text=context_info.node.get_content()))\n\n    return context\n\n  def evaluate(self, response: Response) -> str:\n    \"\"\"Evaluate factual consistency metrics\n    \"\"\"\n    answer = str(response)\n    context = self.get_context(response)\n    metric = FactualConsistencyMetric()\n    context = \" \".join([d.text for d in context])\n    score = metric.measure(output=answer, context=context)\n    if metric.is_successful():\n        return \"YES\"\n    else:\n        return \"NO\"\n\nevaluator = FactualConsistencyResponseEvaluator()\n```\n\nYou can then evaluate as such:\n\n```python\nquery_engine = tree_index.as_query_engine()\nresponse = query_engine.query(\"How did Tokyo get its name?\")eval_result = evaluator.evaluate(response)\n```\n\n### Useful Links\n\n* [Read About The DeepEval Framework](https://docs.confident-ai.com/docs/framework)\n* [Answer Relevancy](https://docs.confident-ai.com/docs/measuring_llm_performance/answer_relevancy)\n* [Conceptual Similarity](https://docs.confident-ai.com/docs/measuring_llm_performance/conceptual_similarity) .* [Bias](https://docs.confident-ai.com/docs/measuring_llm_performance/debias)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "30e455b0-1e7b-43d2-a247-65f89200d59b": {"__data__": {"id_": "30e455b0-1e7b-43d2-a247-65f89200d59b", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\graph_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9587d18ae8acb48c4713cd3148ef022248288bf4", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\graph_stores.md", "author": "LlamaIndex"}, "hash": "a29c3de65f42d0e340e9d02ad17f10601366a4b79cb1522996ad7a7c755147fe"}}, "hash": "d2da005c5363afa2d7806914575af44dd75ff3ec57bd165cbb952692356b97f4", "text": "# Using Graph Stores\n\n## `Neo4jGraphStore`\n\n`Neo4j` is supported as a graph store integration. You can persist, visualze, and query graphs using LlamaIndex and Neo4j. Furthermore, existing Neo4j graphs are directly supported using `text2cypher` and the `KnowledgeGraphQueryEngine`.\n\nIf you've never used Neo4j before, you can download the desktop client [here](https://neo4j.com/download/).\n\nOnce you open the client, create a new project and install the `apoc` integration. Full instructions [here](https://neo4j.com/labs/apoc/4.1/installation/). Just click on your project, select `Plugins` on the left side menu, install APOC and restart your server.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nNeo4j Graph Store </examples/index_structs/knowledge_graph/Neo4jKGIndexDemo.ipynb>\n```\n\n## `NebulaGraphStore`\n\nWe support a `NebulaGraphStore` integration, for persisting graphs directly in Nebula! Furthermore, you can generate cypher queries and return natural language responses for your Nebula graphs using the `KnowledgeGraphQueryEngine`.\n\nSee the associated guides below:\n\n```{toctree}\n---\nmaxdepth: 1\n---\nNebula Graph Store </examples/index_structs/knowledge_graph/NebulaGraphKGIndexDemo.ipynb>\nKnowledge Graph Query Engine </examples/query_engine/knowledge_graph_query_engine.ipynb>\n```\n\n## `KuzuGraphStore`\n\nWe support a `KuzuGraphStore` integration, for persisting graphs directly in [Kuzu](https://kuzudb.com).\n\nSee the associated guides below:\n\n```{toctree}\n---\nmaxdepth: 1\n---\nKuzu Graph Store </examples/index_structs/knowledge_graph/KuzuGraphDemo.ipynb>\n```\n\n## `FalkorDBGraphStore`\n\nWe support a `FalkorDBGraphStore` integration, for persisting graphs directly in FalkorDB! Furthermore, you can generate cypher queries and return natural language responses for your FalkorDB graphs using the `KnowledgeGraphQueryEngine`.\n\nSee the associated guides below:\n\n```{toctree}\n---\nmaxdepth: 1\n---\nFalkorDB Graph Store </examples/index_structs/knowledge_graph/FalkorDBGraphDemo.ipynb>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5ebcf58d-2516-4f5e-8d00-2cb8d086edaa": {"__data__": {"id_": "5ebcf58d-2516-4f5e-8d00-2cb8d086edaa", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\graphsignal.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d37221ea4421e3af53adf48114a4bcd231d5375", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\graphsignal.md", "author": "LlamaIndex"}, "hash": "733f525b3ab1d62a309422c1d92601cb3efa20c398d48525cba46d81a397e50e"}}, "hash": "730cb957c364bbc2bc43d5731db74b325962d15fec1204edce402cb82ec562ee", "text": "# Tracing with Graphsignal\n\n[Graphsignal](https://graphsignal.com/) provides observability for AI agents and LLM-powered applications. It helps developers ensure AI applications run as expected and users have the best experience.\n\nGraphsignal **automatically** traces and monitors LlamaIndex. Traces and metrics provide execution details for query, retrieval, and index operations. These insights include **prompts**, **completions**, **embedding statistics**, **retrieved nodes**, **parameters**, **latency**, and **exceptions**.\n\nWhen OpenAI APIs are used, Graphsignal provides additional insights such as **token counts** and **costs** per deployment, model or any context.\n\n\n### Installation and Setup\n\nAdding [Graphsignal tracer](https://github.com/graphsignal/graphsignal-python) is simple, just install and configure it:\n\n```sh\npip install graphsignal\n```\n\n```python\nimport graphsignal\n\n# Provide an API key directly or via GRAPHSIGNAL_API_KEY environment variable\ngraphsignal.configure(api_key='my-api-key', deployment='my-llama-index-app-prod')\n```\n\nYou can get an API key [here](https://app.graphsignal.com/).\n\nSee the [Quick Start guide](https://graphsignal.com/docs/guides/quick-start/), [Integration guide](https://graphsignal.com/docs/integrations/llama-index/), and an [example app](https://github.com/graphsignal/examples/blob/main/llama-index-app/main.py) for more information.\n\n\n### Tracing Other Functions\n\nTo additionally trace any function or code, you can use a decorator or a context manager:\n\n```python\nwith graphsignal.start_trace('load-external-data'):\n    reader.load_data()\n```\n\nSee [Python API Reference](https://graphsignal.com/docs/reference/python-api/) for complete instructions.\n\n\n### Useful Links\n\n* [Tracing and Monitoring LlamaIndex Applications](https://graphsignal.com/blog/tracing-and-monitoring-llama-index-applications/)\n* [Monitor OpenAI API Latency, Tokens, Rate Limits, and More](https://graphsignal.com/blog/monitor-open-ai-api-latency-tokens-rate-limits-and-more/)\n* [OpenAI API Cost Tracking: Analyzing Expenses by Model, Deployment, and Context](https://graphsignal.com/blog/open-ai-api-cost-tracking-analyzing-expenses-by-model-deployment-and-context/)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8122e965-68d2-4c8c-be75-f230747c599e": {"__data__": {"id_": "8122e965-68d2-4c8c-be75-f230747c599e", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\guidance.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce73c9b78c31e9ee169fe6fa976142f455793da9", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\guidance.md", "author": "LlamaIndex"}, "hash": "63c244e04505bf17f70bed830af4b088df7bd484acc009bd89d251318aee9b2a"}, "3": {"node_id": "cce95214-17e7-4280-9ff7-f0339a6cc7b1", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\guidance.md", "author": "LlamaIndex"}, "hash": "3fd45a05a63f8d921b620bb65dbc0c8e0f04d1281d2d09db6b11f1fe333813c9"}}, "hash": "d0b7e22be1b1eac643edc2304ec62e010f3ee77d4ae2ed4456bb4290416f2efe", "text": "# Guidance\n\n[Guidance](https://github.com/microsoft/guidance) is a guidance language for controlling large language models developed by Microsoft.\n\nGuidance programs allow you to interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text.\n\n## Structured Output\nOne particularly exciting aspect of guidance is the ability to output structured objects (think JSON following a specific schema, or a pydantic object). Instead of just \"suggesting\" the desired output structure to the LLM, guidance can actually \"force\" the LLM output to follow the desired schema. This allows the LLM to focus on the content rather than the syntax, and completely eliminate the possibility of output parsing issues.\n\nThis is particularly powerful for weaker LLMs which be smaller in parameter count, and not trained on sufficient source code data to be able to reliably produce well-formed, hierarchical structured output.\n\n### Creating a guidance program to generate pydantic objects \nIn LlamaIndex, we provide an initial integration with guidance, to make it super easy for generating structured output (more specifically pydantic objects).\n\nFor example, if we want to generate an album of songs, with the following schema:\n\n```python\nclass Song(BaseModel):\n    title: str\n    length_seconds: int\n    \nclass Album(BaseModel):\n    name: str\n    artist: str\n    songs: List[Song]\n```\n\nIt's as simple as creating a `GuidancePydanticProgram`, specifying our desired pydantic class `Album`, \nand supplying a suitable prompt template.\n\n> Note: guidance uses handlebars-style templates, which uses double braces for variable substitution, and single braces for literal braces. This is the opposite convention of Python format strings. \n\n> Note: We provide an utility function `from llama_index.prompts.guidance_utils import convert_to_handlebars` that can convert from the Python format string style template to guidance handlebars-style template.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cce95214-17e7-4280-9ff7-f0339a6cc7b1": {"__data__": {"id_": "cce95214-17e7-4280-9ff7-f0339a6cc7b1", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\guidance.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce73c9b78c31e9ee169fe6fa976142f455793da9", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\guidance.md", "author": "LlamaIndex"}, "hash": "63c244e04505bf17f70bed830af4b088df7bd484acc009bd89d251318aee9b2a"}, "2": {"node_id": "8122e965-68d2-4c8c-be75-f230747c599e", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\guidance.md", "author": "LlamaIndex"}, "hash": "d0b7e22be1b1eac643edc2304ec62e010f3ee77d4ae2ed4456bb4290416f2efe"}}, "hash": "3fd45a05a63f8d921b620bb65dbc0c8e0f04d1281d2d09db6b11f1fe333813c9", "text": "```python\nprogram = GuidancePydanticProgram(\n    output_cls=Album, \n    prompt_template_str=\"Generate an example album, with an artist and a list of songs. Using the movie {{movie_name}} as inspiration\",\n    guidance_llm=OpenAI('text-davinci-003'),\n    verbose=True,\n)\n\n```\n\nNow we can run the program by calling it with additional user input. \nHere let's go for something spooky and create an album inspired by the Shining.\n```python\noutput = program(movie_name='The Shining')\n```\n\nWe have our pydantic object:\n```python\nAlbum(name='The Shining', artist='Jack Torrance', songs=[Song(title='All Work and No Play', length_seconds=180), Song(title='The Overlook Hotel', length_seconds=240), Song(title='The Shining', length_seconds=210)])\n```\n\nYou can play with [this notebook](/examples/output_parsing/guidance_pydantic_program.ipynb) for more details.\n\n### Using guidance to improve the robustness of our sub-question query engine.\nLlamaIndex provides a toolkit of advanced query engines for tackling different use-cases.\nSeveral relies on structured output in intermediate steps.\nWe can use guidance to improve the robustness of these query engines, by making sure the\nintermediate response has the expected structure (so that they can be parsed correctly to a structured object).\n\nAs an example, we implement a `GuidanceQuestionGenerator` that can be plugged into a `SubQuestionQueryEngine` to make it more robust than using the default setting.\n```python\nfrom llama_index.question_gen.guidance_generator import GuidanceQuestionGenerator\nfrom guidance.llms import OpenAI as GuidanceOpenAI\n\n# define guidance based question generator\nquestion_gen = GuidanceQuestionGenerator.from_defaults(guidance_llm=GuidanceOpenAI('text-davinci-003'), verbose=False)\n\n# define query engine tools\nquery_engine_tools = ...\n\n# construct sub-question query engine\ns_engine = SubQuestionQueryEngine.from_defaults(\n    question_gen=question_gen  # use guidance based question_gen defined above\n    query_engine_tools=query_engine_tools, \n)\n```\n\nSee [this notebook](/examples/output_parsing/guidance_sub_question.ipynb) for more details.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e2e0ae66-1caa-4ffb-b490-0e086abef319": {"__data__": {"id_": "e2e0ae66-1caa-4ffb-b490-0e086abef319", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\managed_indices.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b418ac328d3bbc3276a01383850e35961589c7a", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\managed_indices.md", "author": "LlamaIndex"}, "hash": "4a18ced337df547be1973e228bbd47ece978aeb2e42e546ae7d1bd9dd33b60c8"}}, "hash": "9e6a1e4bd7304b644c8579701d17fe732434551765cc5ed4c8ea91940e0f1de7", "text": "# Using Managed Indices\n\nLlamaIndex offers multiple integration points with Managed Indices. A managed index is a special type of index that is not managed locally as part of LlamaIndex but instead is managed via an API, such as [Vectara](https://vectara.com).\n\n## Using a Managed Index\n\nSimilar to any other index within LlamaIndex (tree, keyword table, list), any `ManagedIndex` can be constructed with a collection\nof documents. Once constructed, the index can be used for querying.\n\nIf the Index has been previously populated with documents - it can also be used directly for querying.\n\n`VectaraIndex` is currently the only supported managed index, although we expect more to be available soon.\nBelow we show how to use it.\n\n**Vectara Index Construction/Querying**\n\nUse the [Vectara Console](https://console.vectara.com/login) to create a corpus (aka Index), and add an API key for access. \nThen put the customer id, corpus id, and API key in your environment as shown below.\n\nThen construct the Vectara Index and query it as follows:\n\n```python\nfrom llama_index import ManagedIndex, SimpleDirectoryReade\nfrom llama_index.managed import VectaraIndex\n\n# Load documents and build index\nvectara_customer_id = os.environ.get(\"VECTARA_CUSTOMER_ID\")\nvectara_corpus_id = os.environ.get(\"VECTARA_CORPUS_ID\")\nvectara_api_key = os.environ.get(\"VECTARA_API_KEY\")\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectaraIndex.from_documents(documents, vectara_customer_id=vectara_customer_id, vectara_corpus_id=vectara_corpus_id, vectara_api_key=vectara_api_key)\n\n# Query index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n```\n\nNote that if the environment variables `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY` are in the environment already, you do not have to explicitly specifying them in your call and the VectaraIndex class will read them from the enviornment. For example this should be equivalent to the above, if these variables are in the environment already:\n\n```python\nfrom llama_index import ManagedIndex, SimpleDirectoryReade\nfrom llama_index.managed import VectaraIndex\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectaraIndex.from_documents(documents) \n\n# Query index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n```\n\n\n\n\n```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n../../examples/vector_stores/VectaraDemo.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3b4a69ca-9f98-4e7e-8444-82b567e8452d": {"__data__": {"id_": "3b4a69ca-9f98-4e7e-8444-82b567e8452d", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\trulens.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7727424611f613fda805d0a0f3ed0be0566f5c6", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\trulens.md", "author": "LlamaIndex"}, "hash": "c6061d6d77250208f230a7985d73c73d72e2617042eb001779b62d7e7b67b1ca"}}, "hash": "5cd3a265b1b1cf20d11c0a42065882f932b88ea631610a113f20a1f7d4f4333d", "text": "# Evaluating and Tracking with TruLens\n\nThis page covers how to use [TruLens](https://trulens.org) to evaluate and track LLM apps built on Llama-Index.\n\n## What is TruLens?\n\nTruLens is an [opensource](https://github.com/truera/trulens) package that provides instrumentation and evaluation tools for large language model (LLM) based applications. This includes feedback function evaluations of relevance, sentiment and more, plus in-depth tracing including cost and latency.\n\n![TruLens Architecture](https://www.trulens.org/Assets/image/TruLens_Architecture.png)\n\nAs you iterate on new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up. You'll also be able to view evaluations at a record level, and explore the app metadata for each record.\n\n### Installation and Setup\n\nAdding TruLens is simple, just install it from pypi!\n\n```sh\npip install trulens-eval\n```\n\n```python\nfrom trulens_eval import TruLlama\n\n```\n\n## Try it out!\n\n[llama_index_quickstart.ipynb](https://github.com/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb)\n\n## Read more\n\n* [Build and Evaluate LLM Apps with LlamaIndex and TruLens](https://medium.com/llamaindex-blog/build-and-evaluate-llm-apps-with-llamaindex-and-trulens-6749e030d83c)\n* [More examples](https://github.com/truera/trulens/tree/main/trulens_eval/examples/frameworks/llama_index)\n* [trulens.org](https://www.trulens.org/)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "219e3285-8514-4d57-aabc-b9be90bcee77": {"__data__": {"id_": "219e3285-8514-4d57-aabc-b9be90bcee77", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\using_with_langchain.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ddbbfe258246bdaf76b336208d662ffcc94029fd", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\using_with_langchain.md", "author": "LlamaIndex"}, "hash": "7b015955541f760fccc8b51ec0862599174e21bc926ff39bebc28590b65d543b"}}, "hash": "c19c07143c8cf3611b1d3d88bf9b8af72950d08cb19aa72ce03f5bdbc5370e6d", "text": "# Using with Langchain \ud83e\udd9c\ud83d\udd17\n\nLlamaIndex provides both Tool abstractions for a Langchain agent as well as a memory module.\n\nThe API reference of the Tool abstractions + memory modules are [here](/api_reference/langchain_integrations/base.rst).\n\n### Use any data loader as a Langchain Tool\n\nLlamaIndex allows you to use any data loader within the LlamaIndex core repo or in [LlamaHub](https://llamahub.ai/) as an \"on-demand\" data query Tool within a LangChain agent.\n\nThe Tool will 1) load data using the data loader, 2) index the data, and 3) query the data and return the response in an ad-hoc manner.\n\n**Resources**\n- [OnDemandLoaderTool Tutorial](/examples/tools/OnDemandLoaderTool.ipynb)\n\n\n### Use a query engine as a Langchain Tool\nLlamaIndex provides Tool abstractions so that you can use a LlamaIndex query engine along with a Langchain agent. \n\nFor instance, you can choose to create a \"Tool\" from an `QueryEngine` directly as follows:\n\n```python\nfrom llama_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool\n\ntool_config = IndexToolConfig(\n    query_engine=query_engine, \n    name=f\"Vector Index\",\n    description=f\"useful for when you want to answer queries about X\",\n    tool_kwargs={\"return_direct\": True}\n)\n\ntool = LlamaIndexTool.from_tool_config(tool_config)\n\n```\n\nYou can also choose to provide a `LlamaToolkit`:\n\n```python\ntoolkit = LlamaToolkit(\n    index_configs=index_configs,\n)\n```\n\nSuch a toolkit can be used to create a downstream Langchain-based chat agent through\nour `create_llama_agent` and `create_llama_chat_agent` commands:\n\n```python\nfrom llama_index.langchain_helpers.agents import create_llama_chat_agent\n\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    verbose=True\n)\n\nagent_chain.run(input=\"Query about X\")\n```\n\nYou can take a look at [the full tutorial notebook here](https://github.com/jerryjliu/llama_index/blob/main/examples/chatbot/Chatbot_SEC.ipynb).\n\n\n### Llama Demo Notebook: Tool + Memory module\n\nWe provide another demo notebook showing how you can build a chat agent with the following components.\n- Using LlamaIndex as a generic callable tool with a Langchain agent\n- Using LlamaIndex as a memory module; this allows you to insert arbitrary amounts of conversation history with a Langchain chatbot!\n\nPlease see the [notebook here](https://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f426cf7b-db7b-4b36-970b-111415f503fb": {"__data__": {"id_": "f426cf7b-db7b-4b36-970b-111415f503fb", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e592684371cf1fa151073a9305ce733372ac6ea0", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "dd5fa75fe48e3fedc4b8b25a8c668c75bcb2820e4ae05750fb4ed4c69cdf91e3"}, "3": {"node_id": "c0db3248-daff-412a-995f-9c26aee4eeb7", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "f0e1ccf3d420ef3c462b43de57cda1a7c36eb386c3f02ecd4c83e094985f15c2"}}, "hash": "885033109dfd3032b5b8a1300291101e790b618106e60b32efe17daa8416bc86", "text": "# Using Vector Stores\n\nLlamaIndex offers multiple integration points with vector stores / vector databases:\n\n1.LlamaIndex can use a vector store itself as an index.Like any other index, this index can store documents and be used to answer queries.2.LlamaIndex can load data from vector stores, similar to any other data connector.This data can then be used within LlamaIndex data structures.(vector-store-index)=\n\n## Using a Vector Store as an Index\n\nLlamaIndex also supports different vector stores\nas the storage backend for `VectorStoreIndex`.- Azure Cognitive Search (`CognitiveSearchVectorStore`).[Quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector)\n- [Apache Cassandra\u00ae](https://cassandra.apache.org/) and compatible databases such as [Astra DB](https://www.datastax.com/press-release/datastax-adds-vector-search-to-astra-db-on-google-cloud-for-building-real-time-generative-ai-applications) (`CassandraVectorStore`)\n- Chroma (`ChromaVectorStore`) [Installation](https://docs.trychroma.com/getting-started)\n- Epsilla (`EpsillaVectorStore`) [Installation/Quickstart](https://epsilla-inc.gitbook.io/epsilladb/quick-start)\n- DeepLake (`DeepLakeVectorStore`) [Installation](https://docs.deeplake.ai/en/latest/Installation.html)\n- Elasticsearch (`ElasticsearchStore`) [Installation](https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html)\n- Qdrant (`QdrantVectorStore`) [Installation](https://qdrant.tech/documentation/install/) [Python Client](https://qdrant.tech/documentation/install/#python-client)\n- Weaviate (`WeaviateVectorStore`).[Installation](https://weaviate.io/developers/weaviate/installation).[Python Client](https://weaviate.io/developers/weaviate/client-libraries/python).- Zep (`ZepVectorStore`).[Installation](https://docs.getzep.com/deployment/quickstart/).[Python Client](https://docs.getzep.com/sdk/).- Pinecone (`PineconeVectorStore`).[Installation/Quickstart](https://docs.pinecone.io/docs/quickstart).- Faiss (`FaissVectorStore`).[Installation](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md).- Milvus (`MilvusVectorStore`).[Installation](https://milvus.io/docs)\n- Zilliz (`MilvusVectorStore`).[Quickstart](https://zilliz.com/doc/quick_start)\n- MyScale (`MyScaleVectorStore`).[Quickstart](https://docs.myscale.com/en/quickstart/).[Installation/Python Client](https://docs.myscale.com/en/python-client/).- Supabase (`SupabaseVectorStore`).[Quickstart](https://supabase.github.io/vecs/api/).- DocArray (`DocArrayHnswVectorStore`, `DocArrayInMemoryVectorStore`).[Installation/Python Client](https://github.com/docarray/docarray#installation).- MongoDB Atlas (`MongoDBAtlasVectorSearch`).[Installation/Quickstart](https://www.mongodb.com/atlas/database).- Redis (`RedisVectorStore`).[Installation](https://redis.io/docs/getting-started/installation/).- Neo4j (`Neo4jVectorIndex`).[Installation](https://neo4j.com/docs/operations-manual/current/installation/).A detailed API reference is [found here](/api_reference/indices/vector_store.rst).Similar to any other index within LlamaIndex (tree, keyword table, list), `VectorStoreIndex` can be constructed upon any collection\nof documents.We use the vector store within the index to store embeddings for the input text chunks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c0db3248-daff-412a-995f-9c26aee4eeb7": {"__data__": {"id_": "c0db3248-daff-412a-995f-9c26aee4eeb7", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e592684371cf1fa151073a9305ce733372ac6ea0", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "dd5fa75fe48e3fedc4b8b25a8c668c75bcb2820e4ae05750fb4ed4c69cdf91e3"}, "2": {"node_id": "f426cf7b-db7b-4b36-970b-111415f503fb", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "885033109dfd3032b5b8a1300291101e790b618106e60b32efe17daa8416bc86"}, "3": {"node_id": "811a1f10-f697-4c37-9591-b6f8b87a9e11", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "4ee4961f638ce57e0f16d3899818ac67f897a8ab9972eca577457bacb00dccee"}}, "hash": "f0e1ccf3d420ef3c462b43de57cda1a7c36eb386c3f02ecd4c83e094985f15c2", "text": "Once constructed, the index can be used for querying.**Default Vector Store Index Construction/Querying**\n\nBy default, `VectorStoreIndex` uses a in-memory `SimpleVectorStore`\nthat's initialized as part of the default storage context.```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")```\n\n**Custom Vector Store Index Construction/Querying**\n\nWe can query over a custom vector store as follows:\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\nfrom llama_index.vector_stores import DeepLakeVectorStore\n\n# construct vector store and customize storage context\nstorage_context = StorageContext.from_defaults(\n    vector_store = DeepLakeVectorStore(dataset_path=\"<dataset_path>\")\n)\n\n# Load documents and build index\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\n# Query index\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")```\n\nBelow we show more examples of how to construct various vector stores we support.**Elasticsearch**\n\nFirst, you can start Elasticsearch either locally or on [Elastic cloud](https://cloud.elastic.co/registration?utm_source=llama-index&utm_content=documentation).To start Elasticsearch locally with docker, run the following command:\n\n```bash\ndocker run -p 9200:9200 \\\n  -e \"discovery.type=single-node\" \\\n  -e \"xpack.security.enabled=false\" \\\n  -e \"xpack.security.http.ssl.enabled=false\" \\\n  -e \"xpack.license.self_generated.type=trial\" \\\n  docker.elastic.co/elasticsearch/elasticsearch:8.9.0\n```\n\nThen connect and use Elasticsearch as a vector database with LlamaIndex\n\n```python\nfrom llama_index.vector_stores import ElasticsearchStore\nvector_store = ElasticsearchStore(\n    index_name=\"llm-project\",\n    es_url=\"http://localhost:9200\",\n    # Cloud connection options:\n    # es_cloud_id=\"<cloud_id>\",\n    # es_user=\"elastic\",\n    # es_password=\"<password>\",\n)\n```\n\nThis can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.**Redis**\n\nFirst, start Redis-Stack (or get url from Redis provider)\n\n```bash\ndocker run --name redis-vecdb -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n```\n\nThen connect and use Redis as a vector database with LlamaIndex\n\n```python\nfrom llama_index.vector_stores import RedisVectorStore\nvector_store = RedisVectorStore(\n    index_name=\"llm-project\",\n    redis_url=\"redis://localhost:6379\",\n    overwrite=True\n)\n```\n\nThis can be used with the `VectorStoreIndex` to provide a query interface for retrieval, querying, deleting, persisting the index, and more.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "811a1f10-f697-4c37-9591-b6f8b87a9e11": {"__data__": {"id_": "811a1f10-f697-4c37-9591-b6f8b87a9e11", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e592684371cf1fa151073a9305ce733372ac6ea0", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "dd5fa75fe48e3fedc4b8b25a8c668c75bcb2820e4ae05750fb4ed4c69cdf91e3"}, "2": {"node_id": "c0db3248-daff-412a-995f-9c26aee4eeb7", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "f0e1ccf3d420ef3c462b43de57cda1a7c36eb386c3f02ecd4c83e094985f15c2"}, "3": {"node_id": "9f280e4b-20e3-47bf-bef0-cff81dd64831", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "fc317673a98bc847f750feef8ed8e7d9b17b300d0ba6e625920d8aaa94310b33"}}, "hash": "4ee4961f638ce57e0f16d3899818ac67f897a8ab9972eca577457bacb00dccee", "text": "**DeepLake**\n\n```python\nimport os\nimport getpath\nfrom llama_index.vector_stores import DeepLakeVectorStore\n\nos.environ[\"OPENAI_API_KEY\"] = getpath.getpath(\"OPENAI_API_KEY: \")\nos.environ[\"ACTIVELOOP_TOKEN\"] = getpath.getpath(\"ACTIVELOOP_TOKEN: \")\ndataset_path = \"hub://adilkhan/paul_graham_essay\"\n\n# construct vector store\nvector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n```\n\n**Faiss**\n\n```python\nimport faiss\nfrom llama_index.vector_stores import FaissVectorStore\n\n# create faiss index\nd = 1536\nfaiss_index = faiss.IndexFlatL2(d)\n\n# construct vector store\nvector_store = FaissVectorStore(faiss_index)\n\n...\n\n# NOTE: since faiss index is in-memory, we need to explicitly call\n#       vector_store.persist() or storage_context.persist() to save it to disk.#       persist() takes in optional arg persist_path.If none give, will use default paths.storage_context.persist()\n```\n\n**Weaviate**\n\n```python\nimport weaviate\nfrom llama_index.vector_stores import WeaviateVectorStore\n\n# creating a Weaviate client\nresource_owner_config = weaviate.AuthClientPassword(\n    username=\"<username>\",\n    password=\"<password>\",\n)\nclient = weaviate.Client(\n    \"https://<cluster-id>.semi.network/\", auth_client_secret=resource_owner_config\n)\n\n# construct vector store\nvector_store = WeaviateVectorStore(weaviate_client=client)\n```\n\n**Zep**\n\nZep stores texts, metadata, and embeddings.All are returned in search results.```python\n\nfrom llama_index.vector_stores import ZepVectorStore\n\nvector_store = ZepVectorStore(\n    api_url=\"<api_url>\",\n    api_key=\"<api_key>\",\n    collection_name=\"<unique_collection_name>\",  # Can either be an existing collection or a new one\n    embedding_dimensions=1536 # Optional, required if creating a new collection\n)\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\n# Query index using both a text query and metadata filters\nfilters = MetadataFilters(filters=[ExactMatchFilter(key=\"theme\", value=\"Mafia\")])\nretriever = index.as_retriever(filters=filters)\nresult = retriever.retrieve(\"What is inception about?\")", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9f280e4b-20e3-47bf-bef0-cff81dd64831": {"__data__": {"id_": "9f280e4b-20e3-47bf-bef0-cff81dd64831", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e592684371cf1fa151073a9305ce733372ac6ea0", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "dd5fa75fe48e3fedc4b8b25a8c668c75bcb2820e4ae05750fb4ed4c69cdf91e3"}, "2": {"node_id": "811a1f10-f697-4c37-9591-b6f8b87a9e11", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "4ee4961f638ce57e0f16d3899818ac67f897a8ab9972eca577457bacb00dccee"}, "3": {"node_id": "474c71b0-ce49-4b88-85a4-e1ec94ce3738", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "45ff97913ead16fa1f3b7f9b7b108dc722d24f931f11d948cb939827fcbc3da9"}}, "hash": "fc317673a98bc847f750feef8ed8e7d9b17b300d0ba6e625920d8aaa94310b33", "text": "```\n\n**Pinecone**\n\n```python\nimport pinecone\nfrom llama_index.vector_stores import PineconeVectorStore\n\n# Creating a Pinecone index\napi_key = \"api_key\"\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\npinecone.create_index(\n    \"quickstart\",\n    dimension=1536,\n    metric=\"euclidean\",\n    pod_type=\"p1\"\n)\nindex = pinecone.Index(\"quickstart\")\n\n# can define filters specific to this vector index (so you can\n# reuse pinecone indexes)\nmetadata_filters = {\"title\": \"paul_graham_essay\"}\n\n# construct vector store\nvector_store = PineconeVectorStore(\n    pinecone_index=index,\n    metadata_filters=metadata_filters\n)\n```\n\n**Qdrant**\n\n```python\nimport qdrant_client\nfrom llama_index.vector_stores import QdrantVectorStore\n\n# Creating a Qdrant vector store\nclient = qdrant_client.QdrantClient(\n    host=\"<qdrant-host>\",\n    api_key=\"<qdrant-api-key>\",\n    https=True\n)\ncollection_name = \"paul_graham\"\n\n# construct vector store\nvector_store = QdrantVectorStore(\n    client=client,\n    collection_name=collection_name,\n)\n```\n\n**Cassandra** (covering DataStax Astra DB as well, which is built on Cassandra)\n\n```python\nfrom cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\nfrom llama_index.vector_stores import CassandraVectorStore\n\n# for a Cassandra cluster:\ncluster = Cluster([\"127.0.0.1\"])\n# for an Astra DB cloud instance:\ncluster = Cluster(\n  cloud={\"secure_connect_bundle\": \"/home/USER/secure-bundle.zip\"},\n  auth_provider=PlainTextAuthProvider(\"token\", \"AstraCS:...\")\n)\n#\nsession = cluster.connect()\nkeyspace = \"my_cassandra_keyspace\"\n\nvector_store = CassandraVectorStore(\n    session=session,\n    keyspace=keyspace,\n    table=\"llamaindex_vector_test_1\",\n    embedding_dimension=1536,\n    #insertion_batch_size=50,  # optional\n)\n```\n\n**Chroma**\n\n```python\nimport chromadb\nfrom llama_index.vector_stores import ChromaVectorStore\n\n# Creating a Chroma client\n# EphemeralClient operates purely in-memory, PersistentClient will also save to disk\nchroma_client = chromadb.EphemeralClient()\nchroma_collection = chroma_client.create_collection(\"quickstart\")\n\n# construct vector store\nvector_store = ChromaVectorStore(\n    chroma_collection=chroma_collection,\n)\n```\n\n**Epsilla**\n\n```python\nfrom pyepsilla import vectordb\nfrom llama_index.vector_stores import EpsillaVectorStore\n\n# Creating an Epsilla client\nepsilla_client = vectordb.Client()\n\n# Construct vector store\nvector_store = EpsillaVectorStore(client=epsilla_client)\n```\n\n**Note**: `EpsillaVectorStore` depends on the `pyepsilla` library and a running Epsilla vector database.Use `pip/pip3 install pyepsilla` if not installed yet.A running Epsilla vector database could be found through docker image.For complete instructions, see the following documentation:\nhttps://epsilla-inc.gitbook.io/epsilladb/quick-start\n\n**Milvus**\n\n- Milvus Index offers the ability to store both Documents and their embeddings.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "474c71b0-ce49-4b88-85a4-e1ec94ce3738": {"__data__": {"id_": "474c71b0-ce49-4b88-85a4-e1ec94ce3738", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e592684371cf1fa151073a9305ce733372ac6ea0", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "dd5fa75fe48e3fedc4b8b25a8c668c75bcb2820e4ae05750fb4ed4c69cdf91e3"}, "2": {"node_id": "9f280e4b-20e3-47bf-bef0-cff81dd64831", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "fc317673a98bc847f750feef8ed8e7d9b17b300d0ba6e625920d8aaa94310b33"}, "3": {"node_id": "f8101c5f-71f6-4812-9e3b-6547502b5211", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "fad7677c8d95fa28a0293e09ca4ed6e5550087feb045122f1e01bce0aabdfd80"}}, "hash": "45ff97913ead16fa1f3b7f9b7b108dc722d24f931f11d948cb939827fcbc3da9", "text": "```python\nimport pymilvus\nfrom llama_index.vector_stores import MilvusVectorStore\n\n# construct vector store\nvector_store = MilvusVectorStore(\n    uri='https://localhost:19530',\n    overwrite='True'\n)\n\n```\n\n**Note**: `MilvusVectorStore` depends on the `pymilvus` library.Use `pip install pymilvus` if not already installed.If you get stuck at building wheel for `grpcio`, check if you are using python 3.11\n(there's a known issue: https://github.com/milvus-io/pymilvus/issues/1308)\nand try downgrading.**Zilliz**\n\n- Zilliz Cloud (hosted version of Milvus) uses the Milvus Index with some extra arguments.```python\nimport pymilvus\nfrom llama_index.vector_stores import MilvusVectorStore\n\n\n# construct vector store\nvector_store = MilvusVectorStore(\n    uri='foo.vectordb.zillizcloud.com',\n    token=\"your_token_here\"\n    overwrite='True'\n)\n```\n\n**Note**: `MilvusVectorStore` depends on the `pymilvus` library.\nUse `pip install pymilvus` if not already installed.\nIf you get stuck at building wheel for `grpcio`, check if you are using python 3.11\n(there's a known issue: https://github.com/milvus-io/pymilvus/issues/1308)\nand try downgrading.\n\n**MyScale**\n\n```python\nimport clickhouse_connect\nfrom llama_index.vector_stores import MyScaleVectorStore\n\n# Creating a MyScale client\nclient = clickhouse_connect.get_client(\n    host='YOUR_CLUSTER_HOST',\n    port=8443,\n    username='YOUR_USERNAME',\n    password='YOUR_CLUSTER_PASSWORD'\n)\n\n\n# construct vector store\nvector_store = MyScaleVectorStore(\n    myscale_client=client\n)\n```\n\n**DocArray**\n\n```python\nfrom llama_index.vector_stores import (\n    DocArrayHnswVectorStore,\n    DocArrayInMemoryVectorStore,\n)\n\n# construct vector store\nvector_store = DocArrayHnswVectorStore(work_dir='hnsw_index')\n\n# alternatively, construct the in-memory vector store\nvector_store = DocArrayInMemoryVectorStore()\n```\n\n**MongoDBAtlas**\n\n```python\n# Provide URI to constructor, or use environment variable\nimport pymongo\nfrom llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\nfrom llama_index.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.storage.storage_context import StorageContext\nfrom llama_index.readers.file.base import SimpleDirectoryReader\n\n# mongo_uri = os.environ[\"MONGO_URI\"]\nmongo_uri = \"mongodb+srv://<username>:<password>@<host>?retryWrites=true&w=majority\"\nmongodb_client = pymongo.MongoClient(mongo_uri)\n\n# construct store\nstore = MongoDBAtlasVectorSearch(mongodb_client)\nstorage_context = StorageContext.from_defaults(vector_store=store)\nuber_docs = SimpleDirectoryReader(input_files=[\"../data/10k/uber_2021.pdf\"]).load_data()\n\n# construct index\nindex = VectorStoreIndex.from_documents(uber_docs, storage_context=storage_context)\n```\n\n**Neo4j**\n\n- Neo4j stores texts, metadata, and embeddings and can be customized to return graph data in the form of metadata.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f8101c5f-71f6-4812-9e3b-6547502b5211": {"__data__": {"id_": "f8101c5f-71f6-4812-9e3b-6547502b5211", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e592684371cf1fa151073a9305ce733372ac6ea0", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "dd5fa75fe48e3fedc4b8b25a8c668c75bcb2820e4ae05750fb4ed4c69cdf91e3"}, "2": {"node_id": "474c71b0-ce49-4b88-85a4-e1ec94ce3738", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "45ff97913ead16fa1f3b7f9b7b108dc722d24f931f11d948cb939827fcbc3da9"}, "3": {"node_id": "f7597301-8057-424e-9d46-f2e59157cd91", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "23210c17f3bd8c5528a762121fcff6ff063f369dc86eb4d50b5abdc94d5b58a4"}}, "hash": "fad7677c8d95fa28a0293e09ca4ed6e5550087feb045122f1e01bce0aabdfd80", "text": "```python\nfrom llama_index.vector_stores import Neo4jVectorStore\n\n# construct vector store\nneo4j_vector = Neo4jVectorStore(\n    username=\"neo4j\",\n    password=\"pleaseletmein\",\n    url=\"bolt://localhost:7687\",\n    embed_dim=1536\n)\n\n```\n\n**Azure Cognitive Search**\n\n```python\nfrom azure.search.documents import SearchClient\nfrom llama_index.vector_stores import ChromaVectorStore\nfrom azure.core.credentials import AzureKeyCredential\n\nservice_endpoint = f\"https://{search_service_name}.search.windows.net\"\nindex_name = \"quickstart\"\ncognitive_search_credential = AzureKeyCredential(\"<API key>\")\n\nsearch_client = SearchClient(\n    endpoint=service_endpoint,\n    index_name=index_name,\n    credential=cognitive_search_credential,\n)\n\n# construct vector store\nvector_store = CognitiveSearchVectorStore(\n    search_client,\n    id_field_key=\"id\",\n    chunk_field_key=\"content\",\n    embedding_field_key=\"embedding\",\n    metadata_field_key=\"li_jsonMetadata\",\n    doc_id_field_key=\"li_doc_id\",\n)\n```\n\n[Example notebooks can be found here](https://github.com/jerryjliu/llama_index/tree/main/docs/examples/vector_stores).## Loading Data from Vector Stores using Data Connector\n\nLlamaIndex supports loading data from the following sources.See [Data Connectors](../connector/root.md) for more details and API documentation.Chroma stores both documents and vectors.This is an example of how to use Chroma:\n\n```python\n\nfrom llama_index.readers.chroma import ChromaReader\nfrom llama_index.indices import SummaryIndex\n\n# The chroma reader loads data from a persisted Chroma collection.# This requires a collection name and a persist directory.reader = ChromaReader(\n    collection_name=\"chroma_collection\",\n    persist_directory=\"examples/data_connectors/chroma_collection\"\n)\n\nquery_vector=[n1, n2, n3, ...]\n\ndocuments = reader.load_data(collection_name=\"demo\", query_vector=query_vector, limit=5)\nindex = SummaryIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"<query_text>\")\ndisplay(Markdown(f\"<b>{response}</b>\"))\n```\n\nQdrant also stores both documents and vectors.This is an example of how to use Qdrant:\n\n```python\n\nfrom llama_index.readers.qdrant import QdrantReader\n\nreader = QdrantReader(host=\"localhost\")\n\n# the query_vector is an embedding representation of your query_vector\n# Example query_vector\n# query_vector = [0.3, 0.3, 0.3, 0.3, ...]\n\nquery_vector = [n1, n2, n3, ...]\n\n# NOTE: Required args are collection_name, query_vector.# See the Python client: https;//github.com/qdrant/qdrant_client\n# for more details\n\ndocuments = reader.load_data(collection_name=\"demo\", query_vector=query_vector, limit=5)\n\n```\n\nNOTE: Since Weaviate can store a hybrid of document and vector objects, the user may either choose to explicitly specify `class_name` and `properties` in order to query documents, or they may choose to specify a raw GraphQL query.See below for usage.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f7597301-8057-424e-9d46-f2e59157cd91": {"__data__": {"id_": "f7597301-8057-424e-9d46-f2e59157cd91", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e592684371cf1fa151073a9305ce733372ac6ea0", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "dd5fa75fe48e3fedc4b8b25a8c668c75bcb2820e4ae05750fb4ed4c69cdf91e3"}, "2": {"node_id": "f8101c5f-71f6-4812-9e3b-6547502b5211", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "fad7677c8d95fa28a0293e09ca4ed6e5550087feb045122f1e01bce0aabdfd80"}, "3": {"node_id": "f2053e36-16d6-4f9e-bd97-ee8e5e4faad9", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "096525cb79940d6921d11c57c61bb00f41e2e41ae08798040c789aef8e715303"}}, "hash": "23210c17f3bd8c5528a762121fcff6ff063f369dc86eb4d50b5abdc94d5b58a4", "text": "```python\n# option 1: specify class_name and properties\n\n# 1) load data using class_name and properties\ndocuments = reader.load_data(\n    class_name=\"<class_name>\",\n    properties=[\"property1\", \"property2\", \"...\"],\n    separate_documents=True\n)\n\n# 2) example GraphQL query\nquery = \"\"\"\n{\n    Get {\n        <class_name> {\n            <property1>\n            <property2>\n        }\n    }\n}\n\"\"\"\n\ndocuments = reader.load_data(graphql_query=query, separate_documents=True)\n```\n\nNOTE: Both Pinecone and Faiss data loaders assume that the respective data sources only store vectors; text content is stored elsewhere.Therefore, both data loaders require that the user specifies an `id_to_text_map` in the load_data call.For instance, this is an example usage of the Pinecone data loader `PineconeReader`:\n\n```python\n\nfrom llama_index.readers.pinecone import PineconeReader\n\nreader = PineconeReader(api_key=api_key, environment=\"us-west1-gcp\")\n\nid_to_text_map = {\n    \"id1\": \"text blob 1\",\n    \"id2\": \"text blob 2\",\n}\n\nquery_vector=[n1, n2, n3, ..]\n\ndocuments = reader.load_data(\n    index_name=\"quickstart\", id_to_text_map=id_to_text_map, top_k=3, vector=query_vector, separate_documents=True\n)\n\n```\n\n[Example notebooks can be found here](https://github.com/jerryjliu/llama_index/tree/main/docs/examples/data_connectors).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f2053e36-16d6-4f9e-bd97-ee8e5e4faad9": {"__data__": {"id_": "f2053e36-16d6-4f9e-bd97-ee8e5e4faad9", "embedding": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e592684371cf1fa151073a9305ce733372ac6ea0", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "dd5fa75fe48e3fedc4b8b25a8c668c75bcb2820e4ae05750fb4ed4c69cdf91e3"}, "2": {"node_id": "f7597301-8057-424e-9d46-f2e59157cd91", "node_type": null, "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}, "hash": "23210c17f3bd8c5528a762121fcff6ff063f369dc86eb4d50b5abdc94d5b58a4"}}, "hash": "096525cb79940d6921d11c57c61bb00f41e2e41ae08798040c789aef8e715303", "text": "```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n../../examples/vector_stores/Elasticsearch_demo.ipynb\n../../examples/vector_stores/SimpleIndexDemo.ipynb\n../../examples/vector_stores/SimpleIndexDemoMMR.ipynb\n../../examples/vector_stores/RedisIndexDemo.ipynb\n../../examples/vector_stores/QdrantIndexDemo.ipynb\n../../examples/vector_stores/FaissIndexDemo.ipynb\n../../examples/vector_stores/DeepLakeIndexDemo.ipynb\n../../examples/vector_stores/MyScaleIndexDemo.ipynb\n../../examples/vector_stores/MetalIndexDemo.ipynb\n../../examples/vector_stores/WeaviateIndexDemo.ipynb\n../../examples/vector_stores/ZepIndexDemo.ipynb\n../../examples/vector_stores/OpensearchDemo.ipynb\n../../examples/vector_stores/PineconeIndexDemo.ipynb\n../../examples/vector_stores/CassandraIndexDemo.ipynb\n../../examples/vector_stores/ChromaIndexDemo.ipynb\n../../examples/vector_stores/EpsillaIndexDemo.ipynb\n../../examples/vector_stores/LanceDBIndexDemo.ipynb\n../../examples/vector_stores/MilvusIndexDemo.ipynb\n../../examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb\n../../examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb\n../../examples/vector_stores/AsyncIndexCreationDemo.ipynb\n../../examples/vector_stores/SupabaseVectorIndexDemo.ipynb\n../../examples/vector_stores/DocArrayHnswIndexDemo.ipynb\n../../examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb\n../../examples/vector_stores/MongoDBAtlasVectorSearch.ipynb\n../../examples/vector_stores/postgres.ipynb\n../../examples/vector_stores/AwadbDemo.ipynb\n../../examples/vector_stores/Neo4jVectorDemo.ipynb\n../../examples/vector_stores/CognitiveSearchIndexDemo.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9f9535a8-5be2-48d7-bcfa-2345258ee81a": {"__data__": {"id_": "9f9535a8-5be2-48d7-bcfa-2345258ee81a", "embedding": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b260ba3a258f903d4d49ce82b595252218bb9d16", "node_type": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\modules.md", "author": "LlamaIndex"}, "hash": "ccd00c80d005b4e6a4dd14eb9422cf98ddd0f237e623a18f3855e7ffac0f873c"}}, "hash": "5dbe1cd99d65a6a770b0ec54584f301df281694216dc73b668cbf9a740f24626", "text": "# Module Guides\n\nThese guide provide an overview of how to use our agent classes.\n\nFor more detailed guides on how to use specific tools, check out our [tools module guides]().\n\n## OpenAI Agent\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/agent/openai_agent.ipynb\n/examples/agent/openai_agent_with_query_engine.ipynb\n/examples/agent/openai_agent_retrieval.ipynb\n/examples/agent/openai_agent_query_cookbook.ipynb\n/examples/agent/openai_agent_query_plan.ipynb\n/examples/agent/openai_agent_context_retrieval.ipynb\n/examples/query_engine/recursive_retriever_agents.ipynb\n```\n\n## ReAct Agent\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/agent/react_agent_with_query_engine.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bd56e86b-54cc-4e3a-baa0-5e37c98227aa": {"__data__": {"id_": "bd56e86b-54cc-4e3a-baa0-5e37c98227aa", "embedding": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e34b8d5855a000ee45bf3ecb04d36acc9ab8370", "node_type": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\root.md", "author": "LlamaIndex"}, "hash": "e054b0a879bf1088e1f1aacc74c83171df357ed62ed9b045ada1af5ff19ad3b5"}}, "hash": "8925161b32ca12ccffd02e42add71896b58aade3250afbf7e1712a9fa46c2ebb", "text": "# Data Agents\n\n## Concept\nData Agents are LLM-powered knowledge workers in LlamaIndex that can intelligently perform various tasks over your data, in both a \u201cread\u201d and \u201cwrite\u201d function. They are capable of the following:\n\n- Perform automated search and retrieval over different types of data - unstructured, semi-structured, and structured.\n- Calling any external service API in a structured fashion, and processing the response + storing it for later.\n\nIn that sense, agents are a step beyond our [query engines](/core_modules/query_modules/query_engine/root.md) in that they can not only \"read\" from a static source of data, but can dynamically ingest and modify data from a variety of different tools.\n\nBuilding a data agent requires the following core components:\n\n- A reasoning loop\n- Tool abstractions\n\nA data agent is initialized with set of APIs, or Tools, to interact with; these APIs can be called by the agent to return information or modify state. Given an input task, the data agent uses a reasoning loop to decide which tools to use, in which sequence, and the parameters to call each tool.\n\n### Reasoning Loop\nThe reasoning loop depends on the type of agent. We have support for the following agents: \n- OpenAI Function agent (built on top of the OpenAI Function API)\n- a ReAct agent (which works across any chat/text completion endpoint).\n\n### Tool Abstractions\n\nYou can learn more about our Tool abstractions in our [Tools section](/core_modules/agent_modules/tools/root.md).\n\n### Blog Post\n\nFor full details, please check out our detailed [blog post](https://medium.com/llamaindex-blog/data-agents-eed797d7972f).\n\n\n## Usage Pattern\n\nData agents can be used in the following manner (the example uses the OpenAI Function API)\n```python\nfrom llama_index.agent import OpenAIAgent\nfrom llama_index.llms import OpenAI\n\n# import and define tools\n...\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\n# initialize openai agent\nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n```\n\nSee our usage pattern guide for more details.\n```{toctree}\n---\nmaxdepth: 1\n---\nusage_pattern.md\n```\n\n## Modules\n\nLearn more about our different agent types in our module guides below.\n\nAlso take a look at our [tools section](/core_modules/agent_modules/tools/root.md)!\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "01e3977e-f6d5-45a2-b686-b5130b963879": {"__data__": {"id_": "01e3977e-f6d5-45a2-b686-b5130b963879", "embedding": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2e68fd0c5731804f83484ac6beeca31d139f005", "node_type": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "377638e11792fb953171b653155dfdf765549910d588d589b7334e08f58862a5"}, "3": {"node_id": "c8702fc4-96f5-4890-800b-41eab8d9e957", "node_type": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "a1810318ef9a0bdc529a784545edf4477f55394605aaaf5c2e9d7d3f2216984e"}}, "hash": "b2fa3b9ac077048c55f2e0321583f988fcf2feb82512df360419847388421063", "text": "# Usage Pattern\n\n## Get Started\n\nAn agent is initialized from a set of Tools. Here's an example of instantiating a ReAct\nagent from a set of Tools.\n\n```python\nfrom llama_index.tools import FunctionTool\nfrom llama_index.llms import OpenAI\nfrom llama_index.agent import ReActAgent\n\n# define sample Tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiple two integers and returns the result integer\"\"\"\n    return a * b\n\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\n\n# initialize llm\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n\n# initialize ReAct agent\nagent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True)\n```\n\nAn agent supports both `chat` and `query` endpoints, inheriting from our `ChatEngine` and `QueryEngine` respectively.\n\nExample usage:\n```python\nagent.chat(\"What is 2123 * 215123\")\n```\n\n\n## Query Engine Tools\n\nIt is easy to wrap query engines as tools for an agent as well.Simply do the following:\n\n```python\n\nfrom llama_index.agent import ReActAgent\nfrom llama_index.tools import QueryEngineTool\n\n# NOTE: lyft_index and uber_index are both SimpleVectorIndex instances\nlyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\nuber_engine = uber_index.as_query_engine(similarity_top_k=3)\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=lyft_engine,\n        metadata=ToolMetadata(\n            name=\"lyft_10k\",\n            description=\"Provides information about Lyft financials for year 2021. \"\"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n    QueryEngineTool(\n        query_engine=uber_engine,\n        metadata=ToolMetadata(\n            name=\"uber_10k\",\n            description=\"Provides information about Uber financials for year 2021. \"\"Use a detailed plain text question as input to the tool.\",\n        ),\n    ),\n]\n\n# initialize ReAct agent\nagent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n\n```\n\n## Use other agents as Tools\n\nA nifty feature of our agents is that since they inherit from `BaseQueryEngine`, you can easily define other agents as tools\nthrough our `QueryEngineTool`.```python\nfrom llama_index.tools import QueryEngineTool\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sql_agent,\n        metadata=ToolMetadata(\n            name=\"sql_agent\",\n            description=\"Agent that can execute SQL queries.\"),\n    ),\n    QueryEngineTool(\n        query_engine=gmail_agent,\n        metadata=ToolMetadata(\n            name=\"gmail_agent\",\n            description=\"Tool that can send emails on Gmail.\"),\n    ),\n]\n\nouter_agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n```\n\n## Advanced Concepts (for `OpenAIAgent`, in beta)\n\nYou can also use agents in more advanced settings.For instance, being able to retrieve tools from an index during query-time, and\nbeing able to perform query planning over an existing set of Tools.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c8702fc4-96f5-4890-800b-41eab8d9e957": {"__data__": {"id_": "c8702fc4-96f5-4890-800b-41eab8d9e957", "embedding": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2e68fd0c5731804f83484ac6beeca31d139f005", "node_type": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "377638e11792fb953171b653155dfdf765549910d588d589b7334e08f58862a5"}, "2": {"node_id": "01e3977e-f6d5-45a2-b686-b5130b963879", "node_type": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "b2fa3b9ac077048c55f2e0321583f988fcf2feb82512df360419847388421063"}}, "hash": "a1810318ef9a0bdc529a784545edf4477f55394605aaaf5c2e9d7d3f2216984e", "text": "These are largely implemented with our `OpenAIAgent` classes (which depend on the OpenAI Function API).Support\nfor our more general `ReActAgent` is something we're actively investigating.NOTE: these are largely still in beta.The abstractions may change and become more general over time.### Function Retrieval Agents\n\nIf the set of Tools is very large, you can create an `ObjectIndex` to index the tools, and then pass in an `ObjectRetriever` to the agent during query-time, to first dynamically retrieve the relevant tools before having the agent pick from the candidate tools.We first build an `ObjectIndex` over an existing set of Tools.```python\n# define an \"object\" index over these tools\nfrom llama_index import VectorStoreIndex\nfrom llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n\ntool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\nobj_index = ObjectIndex.from_objects(\n    all_tools,\n    tool_mapping,\n    VectorStoreIndex,\n)\n```\n\nWe then define our `FnRetrieverOpenAIAgent`:\n\n```python\nfrom llama_index.agent import FnRetrieverOpenAIAgent\n\nagent = FnRetrieverOpenAIAgent.from_retriever(obj_index.as_retriever(), verbose=True)\n```\n\n### Context Retrieval Agents\n\nOur context-augmented OpenAI Agent will always perform retrieval before calling any tools.This helps to provide additional context that can help the agent better pick Tools, versus\njust trying to make a decision without any context.```python\nfrom llama_index.schema import Document\nfrom llama_index.agent import ContextRetrieverOpenAIAgent\n\n\n# toy index - stores a list of abbreviations\ntexts = [\n    \"Abbrevation: X = Revenue\",\n    \"Abbrevation: YZ = Risk Factors\",\n    \"Abbreviation: Z = Costs\",\n]\ndocs = [Document(text=t) for t in texts]\ncontext_index = VectorStoreIndex.from_documents(docs)\n\n# add context agent\ncontext_agent = ContextRetrieverOpenAIAgent.from_tools_and_retriever(\n    query_engine_tools, context_index.as_retriever(similarity_top_k=1), verbose=True\n)\nresponse = context_agent.chat(\"What is the YZ of March 2022?\")\n```\n\n### Query Planning\n\nOpenAI Function Agents can be capable of advanced query planning. The trick is to provide the agent\nwith a `QueryPlanTool` - if the agent calls the QueryPlanTool, it is forced to infer a full Pydantic schema representing a query\nplan over a set of subtools.\n\n```python\n# define query plan tool\nfrom llama_index.tools import QueryPlanTool\nfrom llama_index import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(service_context=service_context)\nquery_plan_tool = QueryPlanTool.from_defaults(\n    query_engine_tools=[query_tool_sept, query_tool_june, query_tool_march],\n    response_synthesizer=response_synthesizer,\n)\n\n# initialize agent\nagent = OpenAIAgent.from_tools(\n    [query_plan_tool],\n    max_function_calls=10,\n    llm=OpenAI(temperature=0, model=\"gpt-4-0613\"),\n    verbose=True,\n)\n\n# should output a query plan to call march, june, and september tools\nresponse = agent.query(\"Analyze Uber revenue growth in March, June, and September\")\n\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1c9e7a9f-0ee3-4670-9074-978616b35410": {"__data__": {"id_": "1c9e7a9f-0ee3-4670-9074-978616b35410", "embedding": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eac4592e14222ac5b22febaf8c9678cb1592da15", "node_type": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md", "author": "LlamaIndex"}, "hash": "e56db19f835edb8e7a2330e608895cadc98e7faf66a1847313d9b308487a3d67"}}, "hash": "3bdc78218e89fe88a2d40a45393e320c5917df8347adb6305e09a23baaeb48c2", "text": "# LlamaHub Tools Guide\n\nWe offer a rich set of Tool Specs that are offered through [LlamaHub](https://llamahub.ai/) \ud83e\udd99. \n![](/_static/data_connectors/llamahub.png)\n\nThese tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions. \n\nWe also provide a list of **utility tools** that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data.\n\n## Tool Specs\n\nComing soon! \n\n## Utility Tools\n\nOftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using). \n\nTo tackle this, we\u2019ve provided an initial set of \u201cutility tools\u201d in LlamaHub Tools - utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that\u2019s returned from any API request.\n\nLet\u2019s walk through our two main utility tools below.\n\n### OnDemandLoaderTool\n\nThis tool turns any existing LlamaIndex data loader ( `BaseReader` class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it \u201con-demand\u201d. All three of these steps happen in a single tool call.\n\nOftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call. \n\nA usage example is given below:\n\n```python\nfrom llama_hub.wikipedia.base import WikipediaReader\nfrom llama_index.tools.on_demand_loader_tool import OnDemandLoaderTool\n\ntool = OnDemandLoaderTool.from_defaults(\n\treader,\n\tname=\"Wikipedia Tool\",\n\tdescription=\"A tool for loading data and querying articles from Wikipedia\"\n)\n```\n\n### LoadAndSearchToolSpec\n\nThe LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements `to_tool_list` , and when that function is called, two tools are returned: a `load` tool and then a `search` tool.\n\nThe `load` Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The `search` Tool execution would take in a query string as input and call the underlying index.\n\nThis is helpful for any API endpoint that will by default return large volumes of data - for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.\n\nExample usage is shown below:\n\n```python\nfrom llama_hub.tools.wikipedia.base import WikipediaToolSpec\nfrom llama_index.tools.tool_spec.load_and_search import LoadAndSearchToolSpec\n\nwiki_spec = WikipediaToolSpec()\n# Get the search wikipedia tool\ntool = wiki_spec.to_tool_list()[1]\n\n# Create the Agent with load/search tools\nagent = OpenAIAgent.from_tools(\n LoadAndSearchToolSpec.from_defaults(\n    tool\n ).to_tool_list(), verbose=True\n)\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6f9c0fbd-5a60-4de4-8edc-b0c70177461d": {"__data__": {"id_": "6f9c0fbd-5a60-4de4-8edc-b0c70177461d", "embedding": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\tools\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f86aa28d06ab040ca6f5a1a5d12361cbcd9dbf32", "node_type": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\tools\\root.md", "author": "LlamaIndex"}, "hash": "6ee89913f5ef0f0069e761b5061e1d791f8bfd74d9e41b0fb88d1170f1e34f38"}}, "hash": "d475798bc4269673d4b628fa9893612023dca9e1c624551133f8e0edbe9b2465", "text": "# Tools\n\n## Concept\n\nHaving proper tool abstractions is at the core of building [data agents](/core_modules/agent_modules/agents/root.md). Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a **Tool** as well as a **ToolSpec** containing a series of functions under the hood. \n\nA Tool implements a very generic interface - simply define `__call__` and also return some basic metadata (name, description, function schema).\n\nA Tool Spec defines a full API specification of any service that can be converted into a list of Tools.\n\nWe offer a few different types of Tools:\n- `FunctionTool`: A function tool allows users to easily convert any user-defined function into a Tool. It can also auto-infer the function schema.\n- `QueryEngineTool`: A tool that wraps an existing [query engine](/core_modules/query_modules/root.md). Note: since our agent abstractions inherit from `BaseQueryEngine`, these tools can also wrap other agents.\n\nWe offer a rich set of Tools and Tool Specs through [LlamaHub](https://llamahub.ai/) \ud83e\udd99. \n\n### Blog Post\n\nFor full details, please check out our detailed [blog post]().\n\n## Usage Pattern\n\nOur Tool Specs and Tools can be imported from the `llama-hub` package.\n\nTo use with our agent,\n```python\nfrom llama_index.agent import OpenAIAgent\nfrom llama_hub.tools.gmail.base import GmailToolSpec\n\ntool_spec = GmailToolSpec()\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), verbose=True)\n\n```\n\nSee our Usage Pattern Guide for more details.\n```{toctree}\n---\nmaxdepth: 1\n---\nusage_pattern.md\n```\n\n## LlamaHub Tools Guide \ud83d\udee0\ufe0f\n\nCheck out our guide for a full overview of the Tools/Tool Specs in LlamaHub! \n```{toctree}\n---\nmaxdepth: 1\n---\nllamahub_tools_guide.md\n```\n\n\n<!-- We offer a rich set of Tool Specs that are offered through [LlamaHub](https://llamahub.ai/) \ud83e\udd99. \nThese tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions. \n\n![](/_static/data_connectors/llamahub.png) -->\n\n\n<!-- ## Module Guides\n```{toctree}\n---\nmaxdepth: 1\n---\nmodules.md\n```\n\n## Tool Example Notebooks\n\nComing soon!  -->", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7c2aec87-a8df-4e9a-a304-1e62a164efce": {"__data__": {"id_": "7c2aec87-a8df-4e9a-a304-1e62a164efce", "embedding": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\tools\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "146d9440fa4554141c53de93868ec5fece9810a5", "node_type": null, "metadata": {"filename": "docs\\core_modules\\agent_modules\\tools\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "54fea7a7194f9a47d0c0bb313a77d29f66a317192bdfa585582e371d450002c6"}}, "hash": "298caacdf9081e4594a3b5fdea61136b628bc2b4aada3526265af1284f267905", "text": "# Usage Pattern\n\nYou can create custom LlamaHub Tool Specs and Tools or they can be imported from the `llama-hub` package. They can be plugged into our native agents, or LangChain agents.\n\n## Using with our Agents\n\nTo use with our OpenAIAgent,\n```python\nfrom llama_index.agent import OpenAIAgent\nfrom llama_hub.tools.gmail.base import GmailToolSpec\nfrom llama_index.tools.function_tool import FunctionTool\n\n# Use a tool spec from Llama-Hub\ntool_spec = GmailToolSpec()\n\n# Create a custom tool. Type annotations and docstring are used for the\n# tool definition sent to the Function calling API.\ndef add_numbers(x: int, y: int) -> int:\n    \"\"\"\n    Adds the two numbers together and returns the result.\n    \"\"\"\n    return x + y\n\nfunction_tool = FunctionTool.from_defaults(fn=add_numbers)\n\ntools = tool_spec.to_tool_list() + [function_tool]\nagent = OpenAIAgent.from_tools(tools, verbose=True)\n\n# use agent\nagent.chat(\"Can you create a new email to helpdesk and support @example.com about a service outage\")\n```\n\nFull Tool details can be found on our [LlamaHub](llamahub.ai) page. Each tool contains a \"Usage\" section showing how that tool can be used.\n\n\n## Using with LangChain\nTo use with a LangChain agent, simply convert tools to LangChain tools with `to_langchain_tool()`.\n\n```python\ntools = tool_spec.to_tool_list()\nlangchain_tools = [t.to_langchain_tool() for t in tools]\n# plug into LangChain agent\nfrom langchain.agents import initialize_agent\n\nagent_executor = initialize_agent(\n    langchain_tools, llm, agent=\"conversational-react-description\", memory=memory\n)\n\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e52f36a5-f33b-43c1-9cb8-bb1b85b5911b": {"__data__": {"id_": "e52f36a5-f33b-43c1-9cb8-bb1b85b5911b", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\connector\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5e2d1fa6c9b797a40ef3c4f19110a50dba0aa78", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\connector\\modules.md", "author": "LlamaIndex"}, "hash": "b932a166f934cd43f435d89b161325f118ce1205b10ecc4e9ce62cb847d86bb9"}}, "hash": "308b29ea8a798fedc91f6aae4aea92ee110d94f67fe78102c8193908320cd198", "text": "# Module Guides\n\n\n```{toctree}\n---\nmaxdepth: 1\n---\n../../../examples/data_connectors/simple_directory_reader.ipynb\n../../../examples/data_connectors/PsychicDemo.ipynb\n../../../examples/data_connectors/DeepLakeReader.ipynb\n../../../examples/data_connectors/QdrantDemo.ipynb\n../../../examples/data_connectors/DiscordDemo.ipynb\n../../../examples/data_connectors/MongoDemo.ipynb\n../../../examples/data_connectors/ChromaDemo.ipynb\n../../../examples/data_connectors/MyScaleReaderDemo.ipynb\n../../../examples/data_connectors/FaissDemo.ipynb\n../../../examples/data_connectors/ObsidianReaderDemo.ipynb\n../../../examples/data_connectors/SlackDemo.ipynb\n../../../examples/data_connectors/WebPageDemo.ipynb\n../../../examples/data_connectors/PineconeDemo.ipynb\n../../../examples/data_connectors/MboxReaderDemo.ipynb\n../../../examples/data_connectors/MilvusReaderDemo.ipynb\n../../../examples/data_connectors/NotionDemo.ipynb\n../../../examples/data_connectors/GithubRepositoryReaderDemo.ipynb\n../../../examples/data_connectors/GoogleDocsDemo.ipynb\n../../../examples/data_connectors/DatabaseReaderDemo.ipynb\n../../../examples/data_connectors/TwitterDemo.ipynb\n../../../examples/data_connectors/WeaviateDemo.ipynb\n../../../examples/data_connectors/MakeDemo.ipynb\n../../../examples/data_connectors/deplot/DeplotReader.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7dea053a-d622-4c32-b6ba-eb1b8c64a10f": {"__data__": {"id_": "7dea053a-d622-4c32-b6ba-eb1b8c64a10f", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\connector\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ceed552efc04544ff10ba9f5d5b178c2541e5d47", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\connector\\root.md", "author": "LlamaIndex"}, "hash": "cd6403dde9dbbc9364aca7239932fc99ae11d4d7d99b4b8914faa47af471921b"}}, "hash": "cfbe73d33c197dd74725e1f8736e9adbff4971f6750f38a13d2cf738cc3a5896", "text": "# Data Connectors (LlamaHub)\n\n## Concept\nA data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).\n\n```{tip}\nOnce you've ingested your data, you can build an [Index](/core_modules/data_modules/index/root.md) on top, ask questions using a [Query Engine](/core_modules/query_modules/query_engine/root.md), and have a conversation using a [Chat Engine](/core_modules/query_modules/chat_engines/root.md).\n```\n\n## LlamaHub\nOur data connectors are offered through [LlamaHub](https://llamahub.ai/) \ud83e\udd99. \nLlamaHub is an open-source repository containing data loaders that you can easily plug and play into any LlamaIndex application.\n\n![](/_static/data_connectors/llamahub.png)\n\n\n## Usage Pattern\nGet started with:\n```python\nfrom llama_index import download_loader\n\nGoogleDocsReader = download_loader('GoogleDocsReader')\nloader = GoogleDocsReader()\ndocuments = loader.load_data(document_ids=[...])\n```\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\n\n## Modules\n\nSome sample data connectors:\n- local file directory (`SimpleDirectoryReader`). Can support parsing a wide range of file types: `.pdf`, `.jpg`, `.png`, `.docx`, etc.\n- [Notion](https://developers.notion.com/) (`NotionPageReader`)\n- [Google Docs](https://developers.google.com/docs/api) (`GoogleDocsReader`)\n- [Slack](https://api.slack.com/) (`SlackReader`)\n- [Discord](https://discord.com/developers/docs/intro) (`DiscordReader`)\n- [Apify Actors](https://llamahub.ai/l/apify-actor) (`ApifyActor`). Can crawl the web, scrape webpages, extract text content, download files including `.pdf`, `.jpg`, `.png`, `.docx`, etc.\n\nSee below for detailed guides.\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.rst\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d73fccdc-8c0f-4b39-b308-1a16bb28b10c": {"__data__": {"id_": "d73fccdc-8c0f-4b39-b308-1a16bb28b10c", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\connector\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71447be69982fa3b15d4372930dc3e91a19047af", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\connector\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "ea5770c664105370fc9c21688a2c63e196025059e4d8bbcb3a5335e892d2179e"}}, "hash": "f772aff4118fa71966a3f267b7a286f0a8864daeebe5044bfdee22cff165c112", "text": "# Usage Pattern\n\n## Get Started\nEach data loader contains a \"Usage\" section showing how that loader can be used. At the core of using each loader is a `download_loader` function, which\ndownloads the loader file into a module that you can use within your application.\n\nExample usage:\n\n```python\nfrom llama_index import VectorStoreIndex, download_loader\n\nGoogleDocsReader = download_loader('GoogleDocsReader')\n\ngdoc_ids = ['1wf-y2pd9C878Oh-FmLH7Q_BQkljdm6TQal-c1pUfrec']\nloader = GoogleDocsReader()\ndocuments = loader.load_data(document_ids=gdoc_ids)\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nquery_engine.query('Where did the author go to school?')\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "07ec6934-f93b-4bae-828e-38808b9e6bc6": {"__data__": {"id_": "07ec6934-f93b-4bae-828e-38808b9e6bc6", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e23d3959e82ba207ff9430d50b00606f09c38cf", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\root.md", "author": "LlamaIndex"}, "hash": "bce865268ea8a8708096017aa6bbb4eed27857f622f18a8dab1858da7628a60c"}}, "hash": "88acb1de87361aceba1410426fe722e3c882c642c142cd9f5d0f44f0139829b9", "text": "# Documents / Nodes\n\n## Concept\n\nDocument and Node objects are core abstractions within LlamaIndex.\n\nA **Document** is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. They can be constructed manually, or created automatically via our data loaders. By default, a Document stores text along with some other attributes. Some of these are listed below.\n- `metadata` - a dictionary of annotations that can be appended to the text.\n- `relationships` - a dictionary containing relationships to other Documents/Nodes.\n\n*Note*: We have beta support for allowing Documents to store images, and are actively working on improving its multimodal capabilities.\n\nA **Node** represents a \"chunk\" of a source Document, whether that is a text chunk, an image, or other. Similar to Documents, they contain metadata and relationship information with other nodes.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes. By default every Node derived from a Document will inherit the same metadata from that Document (e.g. a \"file_name\" filed in the Document is propagated to every Node).\n\n\n## Usage Pattern\n\nHere are some simple snippets to get started with Documents and Nodes.\n\n#### Documents\n\n```python\nfrom llama_index import Document, VectorStoreIndex\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n\n```\n\n#### Nodes\n```python\n\nfrom llama_index.node_parser import SimpleNodeParser\n\n# load documents\n...\n\n# parse nodes\nparser = SimpleNodeParser.from_defaults()\nnodes = parser.get_nodes_from_documents(documents)\n\n# build index\nindex = VectorStoreIndex(nodes)\n\n```\n\n### Document/Node Usage\n\nTake a look at our in-depth guides for more details on how to use Documents/Nodes.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nusage_documents.md\nusage_nodes.md\nusage_metadata_extractor.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "67c83569-5531-4a11-b0cb-5d95f16b7450": {"__data__": {"id_": "67c83569-5531-4a11-b0cb-5d95f16b7450", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e73764aa8a96c83aee87723d56ab097f3af40fd0", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md", "author": "LlamaIndex"}, "hash": "eb8b3f50a8f85a21f991ded092c230986af2e4b9dfd3c9b5e5d3a36f2a628946"}, "3": {"node_id": "f9c4f6bb-62d7-42d6-8a92-5a1f035e0b70", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md", "author": "LlamaIndex"}, "hash": "56e933fc379a2659958376fb31d3c95b3e184c0ab5e2bd503ed583c64f40d91e"}}, "hash": "dd440f8383b1df4577f9d4068a2a593988eaa57f2d58a5a0e3a3d6a084563d62", "text": "# Defining and Customizing Documents\n\n\n## Defining Documents\n\nDocuments can either be created automatically via data loaders, or constructed manually.By default, all of our [data loaders](/core_modules/data_modules/connector/root.md) (including those offered on LlamaHub) return `Document` objects through the `load_data` function.```python\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('./data').load_data()\n```\n\nYou can also choose to construct documents manually.LlamaIndex exposes the `Document` struct.```python\nfrom llama_index import Document\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n```\n\nTo speed up prototyping and development, you can also quickly create a document using some default text:\n\n```python\ndocument = Document.example()\n```\n\n## Customizing Documents\n\nThis section covers various ways to customize `Document` objects.Since the `Document` object is a subclass of our `TextNode` object, all these settings and details apply to the `TextNode` object class as well.### Metadata\n\nDocuments also offer the chance to include useful metadata.Using the `metadata` dictionary on each document, additional information can be included to help inform responses and track down sources for query responses.This information can be anything, such as filenames or categories.If you are intergrating with a vector database, keep in mind that some vector databases require that the keys must be strings, and the values must be flat (either `str`, `float`, or `int`).Any information set in the `metadata` dictionary of each document will show up in the `metadata` of each source node created from the document.Additionaly, this information is included in the nodes, enabling the index to utilize it on queries and responses.By default, the metadata is injected into the text for both embedding and LLM model calls.There are a few ways to set up this dictionary:\n\n1.In the document constructor:\n\n```python\ndocument = Document(\n    text='text', \n    metadata={\n        'filename': '<doc_file_name>', \n        'category': '<category>'\n    }\n)\n```\n\n2.After the document is created:\n\n```python\ndocument.metadata = {'filename': '<doc_file_name>'}\n```\n\n3.Set the filename automatically using the `SimpleDirectoryReader` and `file_metadata` hook.This will automatically run the hook on each document to set the `metadata` field:\n\n```python\nfrom llama_index import SimpleDirectoryReader\nfilename_fn = lambda filename: {'file_name': filename}\n\n# automatically sets the metadata of each document according to filename_fn\ndocuments = SimpleDirectoryReader('./data', file_metadata=filename_fn).load_data()\n```\n\n### Customizing the id\n\nAs detailed in the section [Document Management](../index/document_management.md), the `doc_id` is used to enable effecient refreshing of documents in the index.When using the `SimpleDirectoryReader`, you can automatically set the doc `doc_id` to be the full path to each document:\n\n```python\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./data\", filename_as_id=True).load_data()\nprint([x.doc_id for x in documents])\n```\n\nYou can also set the `doc_id` of any `Document` directly!```python\ndocument.doc_id = \"My new document id!\"```\n\nNote: the ID can also be set through the `node_id` or `id_` property on a Document object, similar to a `TextNode` object.### Advanced - Metadata Customization\n\nA key detail mentioned above is that by default, any metadata you set is included in the embeddings generation and LLM.#### Customizing LLM Metadata Text\n\nTypically, a document might have many metadata keys, but you might not want all of them visibile to the LLM during response synthesis.In the above examples, we may not want the LLM to read the `file_name` of our document.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f9c4f6bb-62d7-42d6-8a92-5a1f035e0b70": {"__data__": {"id_": "f9c4f6bb-62d7-42d6-8a92-5a1f035e0b70", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e73764aa8a96c83aee87723d56ab097f3af40fd0", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md", "author": "LlamaIndex"}, "hash": "eb8b3f50a8f85a21f991ded092c230986af2e4b9dfd3c9b5e5d3a36f2a628946"}, "2": {"node_id": "67c83569-5531-4a11-b0cb-5d95f16b7450", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md", "author": "LlamaIndex"}, "hash": "dd440f8383b1df4577f9d4068a2a593988eaa57f2d58a5a0e3a3d6a084563d62"}}, "hash": "56e933fc379a2659958376fb31d3c95b3e184c0ab5e2bd503ed583c64f40d91e", "text": "However, the `file_name` might include information that will help generate better embeddings.A key advantage of doing this is to bias the embeddings for retrieval without changing what the LLM ends up reading.We can exclude it like so:\n\n```python\ndocument.excluded_llm_metadata_keys = ['file_name']\n```\n\nThen, we can test what the LLM will actually end up reading using the `get_content()` function and specifying `MetadataMode.LLM`:\n\n```python\nfrom llama_index.schema import MetadataMode\nprint(document.get_content(metadata_mode=MetadataMode.LLM))\n```\n\n#### Customizing Embedding Metadata Text\n\nSimilar to customing the metadata visibile to the LLM, we can also customize the metadata visible to emebddings.In this case, you can specifically exclude metadata visible to the embedding model, in case you DON'T want particular text to bias the embeddings.```python\ndocument.excluded_embed_metadata_keys = ['file_name']\n```\n\nThen, we can test what the embedding model will actually end up reading using the `get_content()` function and specifying `MetadataMode.EMBED`:\n\n```python\nfrom llama_index.schema import MetadataMode\nprint(document.get_content(metadata_mode=MetadataMode.EMBED))\n```\n\n#### Customizing Metadata Format\n\nAs you know by now, metadata is injected into the actual text of each document/node when sent to the LLM or embedding model.By default, the format of this metadata is controlled by three attributes:\n\n1.`Document.metadata_seperator` -> default = `\"\\n\"`\n\nWhen concatenating all key/value fields of your metadata, this field controls the seperator bewtween each key/value pair.2.`Document.metadata_template` -> default = `\"{key}: {value}\"`\n\nThis attribute controls how each key/value pair in your metadata is formatted.The two variables `key` and `value` string keys are required.3.`Document.text_template` -> default = `{metadata_str}\\n\\n{content}`\n\nOnce your metadata is converted into a string using `metadata_seperator` and `metadata_template`, this templates controls what that metadata looks like when joined with the text content of your document/node.The `metadata` and `content` string keys are required.### Summary\n\nKnowing all this, let's create a short example using all this power:\n\n```python\nfrom llama_index import Document\nfrom llama_index.schema import MetadataMode\n\ndocument = Document(\n    text=\"This is a super-customized document\",\n    metadata={\n        \"file_name\": \"super_secret_document.txt\",\n        \"category\": \"finance\",\n        \"author\": \"LlamaIndex\"    \n    },\n    excluded_llm_metadata_keys=['file_name'],\n    metadata_seperator=\"::\",\n    metadata_template=\"{key}=>{value}\",\n    text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n)\n\nprint(\"The LLM sees this: \\n\", document.get_content(metadata_mode=MetadataMode.LLM))\nprint(\"The Embedding model sees this: \\n\", document.get_content(metadata_mode=MetadataMode.EMBED))\n```\n\n\n### Advanced - Automatic Metadata Extraction\n\nWe have initial examples of using LLMs themselves to perform metadata extraction.\n\nTake a look here! \n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/metadata_extraction/MetadataExtractionSEC.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e22c2099-983c-4151-a2ef-b7e3ac87623a": {"__data__": {"id_": "e22c2099-983c-4151-a2ef-b7e3ac87623a", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "087ad205fe5bcd3338ced6aec685964837187258", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md", "author": "LlamaIndex"}, "hash": "a98936c6348e3b99e9f3118fc2e1946e63e93d377fbd01583874138ebdd4e803"}}, "hash": "d3b377d7aac1205a64dc5b54b3e4b9cf160e7bbb6e01fa913b9dce7d9a5b4264", "text": "# Automated Metadata Extraction for Nodes\n\nYou can use LLMs to automate metadata extraction with our `MetadataExtractor` modules.\n\nOur metadata extractor modules include the following \"feature extractors\":\n- `SummaryExtractor` - automatically extracts a summary over a set of Nodes\n- `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer\n- `TitleExtractor` - extracts a title over the context of each Node\n- `EntityExtractor` - extracts entities (i.e. names of places, people, things) mentioned in the content of each Node\n\nYou can use these feature extractors within our overall `MetadataExtractor` class. Then you can plug in the `MetadataExtractor` into our node parser:\n\n```python\nfrom llama_index.node_parser.extractors import (\n    MetadataExtractor,\n    TitleExtractor,\n    QuestionsAnsweredExtractor\n)\nfrom llama_index.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)\nmetadata_extractor = MetadataExtractor(\n    extractors=[\n        TitleExtractor(nodes=5),\n        QuestionsAnsweredExtractor(questions=3),\n    ],\n)\n\nnode_parser = SimpleNodeParser.from_defaults(\n    text_splitter=text_splitter,\n    metadata_extractor=metadata_extractor,\n)\n# assume documents are defined -> extract nodes\nnodes = node_parser.get_nodes_from_documents(documents)\n```\n\n\n```{toctree}\n---\ncaption: Metadata Extraction Guides\nmaxdepth: 1\n---\n/examples/metadata_extraction/MetadataExtractionSEC.ipynb\n/examples/metadata_extraction/MetadataExtraction_LLMSurvey.ipynb\n/examples/metadata_extraction/EntityExtractionClimate.ipynb\n/examples/metadata_extraction/MarvinMetadataExtractorDemo.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8dbd8b26-46bf-4ee8-866b-02e979cfea67": {"__data__": {"id_": "8dbd8b26-46bf-4ee8-866b-02e979cfea67", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aed5917630b9b7cb0db8a92e6978c52670a5dda4", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md", "author": "LlamaIndex"}, "hash": "c552a628570088ad4f6a396d55dab91806871e4eb28b91e1f7cc93a8fc28a303"}}, "hash": "72969e377a5eaa1c1c8a8e177cc1a0f02b4c8cd38e2241d6ff91abb7ff7a61df", "text": "# Defining and Customizing Nodes\n\nNodes represent \"chunks\" of source Documents, whether that is a text chunk, an image, or more. They also contain metadata and relationship information\nwith other nodes and index structures.\n\nNodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.\n\nFor instance, you can do\n\n```python\nfrom llama_index.node_parser import SimpleNodeParser\n\nparser = SimpleNodeParser.from_defaults()\n\nnodes = parser.get_nodes_from_documents(documents)\n```\n\nYou can also choose to construct Node objects manually and skip the first section. For instance,\n\n```python\nfrom llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n# set relationships\nnode1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id)\nnode2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id)\nnodes = [node1, node2]\n```\n\nThe `RelatedNodeInfo` class can also store additional `metadata` if needed:\n\n```python\nnode2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\"key\": \"val\"})\n```\n\n### Customizing the ID\n\nEach node has an `node_id` property that is automatically generated if not manually specified. This ID can be used for \na variety of purposes; this includes being able to update nodes in storage, being able to define relationships\nbetween nodes (through `IndexNode`), and more.\n\nYou can also get and set the `node_id` of any `TextNode` directly.\n\n```python\nprint(node.node_id)\nnode.node_id = \"My new node_id!\"\n\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "44715510-67b8-431c-9ad6-321b8cffc5d1": {"__data__": {"id_": "44715510-67b8-431c-9ad6-321b8cffc5d1", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\composability.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c36dc0ed7769cb44337179d2daede7635a4b6211", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\composability.md", "author": "LlamaIndex"}, "hash": "458804d4dfcee703389b92443b553dcae5d975adf44afa3327a43cc1eecc51bb"}, "3": {"node_id": "25180ddc-2ed7-409a-abb1-82f600017635", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\composability.md", "author": "LlamaIndex"}, "hash": "8a300ac07da7caea9e11b47cdf643613815f437d3138a97d5059e5463c8e2e6d"}}, "hash": "c87e991012490d16a5306a54d265a287e9e3153552e6ffacde1c136708d964ec", "text": "# Composability\n\n\nLlamaIndex offers **composability** of your indices, meaning that you can build indices on top of other indices. This allows you to more effectively index your entire document tree in order to feed custom knowledge to GPT.\n\nComposability allows you to to define lower-level indices for each document, and higher-order indices over a collection of documents. To see how this works, imagine defining 1) a tree index for the text within each document, and 2) a summary index over each tree index (one document) within your collection.\n\n### Defining Subindices\nTo see how this works, imagine you have 3 documents: `doc1`, `doc2`, and `doc3`.\n\n```python\nfrom llama_index import SimpleDirectoryReader\n\ndoc1 = SimpleDirectoryReader('data1').load_data()\ndoc2 = SimpleDirectoryReader('data2').load_data()\ndoc3 = SimpleDirectoryReader('data3').load_data()\n```\n\n![](/_static/composability/diagram_b0.png)\n\nNow let's define a tree index for each document. In order to persist the graph later, each index should share the same storage context.\n\nIn Python, we have:\n\n```python\nfrom llama_index import TreeIndex\n\nstorage_context = storage_context.from_defaults()\n\nindex1 = TreeIndex.from_documents(doc1, storage_context=storage_context)\nindex2 = TreeIndex.from_documents(doc2, storage_context=storage_context)\nindex3 = TreeIndex.from_documents(doc3, storage_context=storage_context)\n```\n\n![](/_static/composability/diagram_b1.png)\n\n### Defining Summary Text\n\nYou then need to explicitly define *summary text* for each subindex. This allows  \nthe subindices to be used as Documents for higher-level indices.\n\n```python\nindex1_summary = \"<summary1>\"\nindex2_summary = \"<summary2>\"\nindex3_summary = \"<summary3>\"\n```\n\nYou may choose to manually specify the summary text, or use LlamaIndex itself to generate\na summary, for instance with the following:\n\n```python\nsummary = index1.query(\n    \"What is a summary of this document?\", retriever_mode=\"all_leaf\"\n)\nindex1_summary = str(summary)\n```\n\n**If specified**, this summary text for each subindex can be used to refine the answer during query-time. \n\n### Creating a Graph with a Top-Level Index\n\nWe can then create a graph with a summary index on top of these 3 tree indices:\nWe can query, save, and load the graph to/from disk as any other index.\n\n```python\nfrom llama_index.indices.composability import ComposableGraph\n\ngraph = ComposableGraph.from_indices(\n    SummaryIndex,\n    [index1, index2, index3],\n    index_summaries=[index1_summary, index2_summary, index3_summary],\n    storage_context=storage_context,\n)\n\n```\n\n![](/_static/composability/diagram.png)\n\n\n### Querying the Graph\n\nDuring a query, we would start with the top-level summary index. Each node in the list corresponds to an underlying tree index. \nThe query will be executed recursively, starting from the root index, then the sub-indices.\nThe default query engine for each index is called under the hood (i.e. `index.as_query_engine()`), unless otherwise configured by passing `custom_query_engines` to the `ComposableGraphQueryEngine`.\nBelow we show an example that configure the tree index retrievers to use `child_branch_factor=2` (instead of the default `child_branch_factor=1`).\n\n\nMore detail on how to configure `ComposableGraphQueryEngine` can be found [here](/api_reference/query/query_engines/graph_query_engine.rst).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "25180ddc-2ed7-409a-abb1-82f600017635": {"__data__": {"id_": "25180ddc-2ed7-409a-abb1-82f600017635", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\composability.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c36dc0ed7769cb44337179d2daede7635a4b6211", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\composability.md", "author": "LlamaIndex"}, "hash": "458804d4dfcee703389b92443b553dcae5d975adf44afa3327a43cc1eecc51bb"}, "2": {"node_id": "44715510-67b8-431c-9ad6-321b8cffc5d1", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\composability.md", "author": "LlamaIndex"}, "hash": "c87e991012490d16a5306a54d265a287e9e3153552e6ffacde1c136708d964ec"}}, "hash": "8a300ac07da7caea9e11b47cdf643613815f437d3138a97d5059e5463c8e2e6d", "text": "```python\n# set custom retrievers. An example is provided below\ncustom_query_engines = {\n    index.index_id: index.as_query_engine(\n        child_branch_factor=2\n    ) \n    for index in [index1, index2, index3]\n}\nquery_engine = graph.as_query_engine(\n    custom_query_engines=custom_query_engines\n)\nresponse = query_engine.query(\"Where did the author grow up?\")\n```\n\n> Note that specifying custom retriever for index by id\n> might require you to inspect e.g., `index1.index_id`.\n> Alternatively, you can explicitly set it as follows:\n```python\nindex1.set_index_id(\"<index_id_1>\")\nindex2.set_index_id(\"<index_id_2>\")\nindex3.set_index_id(\"<index_id_3>\")\n```\n\n![](/_static/composability/diagram_q1.png)\n\nSo within a node, instead of fetching the text, we would recursively query the stored tree index to retrieve our answer.\n\n![](/_static/composability/diagram_q2.png)\n\nNOTE: You can stack indices as many times as you want, depending on the hierarchies of your knowledge base! \n\n\n### [Optional] Persisting the Graph\n\nThe graph can also be persisted to storage, and then loaded again when needed. Note that you'll need to set the \nID of the root index, or keep track of the default.\n\n```python\n# set the ID\ngraph.root_index.set_index_id(\"my_id\")\n\n# persist to storage\ngraph.root_index.storage_context.persist(persist_dir=\"./storage\")\n\n# load \nfrom llama_index import StorageContext, load_graph_from_storage\n\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\ngraph = load_graph_from_storage(storage_context, root_id=\"my_id\")\n```\n\n\nWe can take a look at a code example below as well. We first build two tree indices, one over the Wikipedia NYC page, and the other over Paul Graham's essay. We then define a keyword extractor index over the two tree indices.\n\n[Here is an example notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/ComposableIndices.ipynb).\n\n\n```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n../../../../examples/composable_indices/ComposableIndices-Prior.ipynb\n../../../../examples/composable_indices/ComposableIndices-Weaviate.ipynb\n../../../../examples/composable_indices/ComposableIndices.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6cd88956-15fe-496c-b768-9bdc4107c57c": {"__data__": {"id_": "6cd88956-15fe-496c-b768-9bdc4107c57c", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\document_management.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31d3a47c3db5e1bafe59905d24647bc931dcbae4", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\document_management.md", "author": "LlamaIndex"}, "hash": "7f215abd3b31305aad33d9efeba9b055eb96525b115875fa548bf32c93018a82"}, "3": {"node_id": "3a3b62b8-4da6-4dd5-ba96-338bbc9deb24", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\document_management.md", "author": "LlamaIndex"}, "hash": "7632c83e5026cdca36065cb6a93355d391e8853b85002e56917cc2c78884ed3c"}}, "hash": "b6bc21791fd1f1fa23638579b1dd7fa035e31931e8742e5645fafde9c1663be9", "text": "# Document Management\n\nMost LlamaIndex index structures allow for **insertion**, **deletion**, **update**, and **refresh** operations.## Insertion\n\nYou can \"insert\" a new Document into any index data structure, after building the index initially.This document will be broken down into nodes and ingested into the index.The underlying mechanism behind insertion depends on the index structure.For instance, for the summary index, a new Document is inserted as additional node(s) in the list.For the vector store index, a new Document (and embeddings) is inserted into the underlying document/embedding store.An example notebook showcasing our insert capabilities is given [here](https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/InsertDemo.ipynb).In this notebook we showcase how to construct an empty index, manually create Document objects, and add those to our index data structures.An example code snippet is given below:\n\n```python\nfrom llama_index import SummaryIndex, Document\n\nindex = SummaryIndex([])\ntext_chunks = ['text_chunk_1', 'text_chunk_2', 'text_chunk_3']\n\ndoc_chunks = []\nfor i, text in enumerate(text_chunks):\n    doc = Document(text=text, id_=f\"doc_id_{i}\")\n    doc_chunks.append(doc)\n\n# insert\nfor doc_chunk in doc_chunks:\n    index.insert(doc_chunk)\n```\n\n## Deletion\n\nYou can \"delete\" a Document from most index data structures by specifying a document_id.(**NOTE**: the tree index currently does not support deletion).All nodes corresponding to the document will be deleted.```python\nindex.delete_ref_doc(\"doc_id_0\", delete_from_docstore=True)\n```\n\n`delete_from_docstore` will default to `False` in case you are sharing nodes betweeen indexes using the same docstore.However, these nodes will not be used when querying when this is set to `False` as they will be deleted from the `index_struct` of the index, which keeps track of which nodes can be used for querying.## Update\n\nIf a Document is already present within an index, you can \"update\" a Document with the same doc `id_` (for instance, if the information in the Document has changed).```python\n# NOTE: the document has a `doc_id` specified\ndoc_chunks[0].text = \"Brand new document text\"\nindex.update_ref_doc(\n    doc_chunks[0], \n    update_kwargs={\"delete_kwargs\": {'delete_from_docstore': True}}\n)\n```\n\nHere, we passed some extra kwargs to ensure the document is deleted from the docstore.This is of course optional.## Refresh\n\nIf you set the doc `id_` of each document when loading your data, you can also automatically refresh the index.The `refresh()` function will only update documents who have the same doc `id_`, but different text contents.Any documents not present in the index at all will also be inserted.`refresh()` also returns a boolean list, indicating which documents in the input have been refreshed in the index.```python\n# modify first document, with the same doc_id\ndoc_chunks[0] = Document(text='Super new document text', id_=\"doc_id_0\")\n\n# add a new document\ndoc_chunks.append(Document(text=\"This isn't in the index yet, but it will be soon!\", id_=\"doc_id_3\"))\n\n# refresh the index\nrefreshed_docs = index.refresh_ref_docs(\n    doc_chunks,\n    update_kwargs={\"delete_kwargs\": {'delete_from_docstore': True}}\n)\n\n# refreshed_docs[0] and refreshed_docs[-1] should be true\n```\n\nAgain, we passed some extra kwargs to ensure the document is deleted from the docstore.This is of course optional.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3a3b62b8-4da6-4dd5-ba96-338bbc9deb24": {"__data__": {"id_": "3a3b62b8-4da6-4dd5-ba96-338bbc9deb24", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\document_management.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31d3a47c3db5e1bafe59905d24647bc931dcbae4", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\document_management.md", "author": "LlamaIndex"}, "hash": "7f215abd3b31305aad33d9efeba9b055eb96525b115875fa548bf32c93018a82"}, "2": {"node_id": "6cd88956-15fe-496c-b768-9bdc4107c57c", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\document_management.md", "author": "LlamaIndex"}, "hash": "b6bc21791fd1f1fa23638579b1dd7fa035e31931e8742e5645fafde9c1663be9"}}, "hash": "7632c83e5026cdca36065cb6a93355d391e8853b85002e56917cc2c78884ed3c", "text": "If you `print()` the output of `refresh()`, you would see which input documents were refreshed:\n\n```python\nprint(refreshed_docs)\n> [True, False, False, True]\n```\n\nThis is most useful when you are reading from a directory that is constantly updating with new information.To autmatically set the doc `id_` when using the `SimpleDirectoryReader`, you can set the `filename_as_id` flag.More details can be found [here](../documents_and_nodes/usage_documents.md).## Document Tracking\n\nAny index that uses the docstore (i.e.all indexes except for most vector store integrations), you can also see which documents you have inserted into the docstore.```python\nprint(index.ref_doc_info)\n> {'doc_id_1': RefDocInfo(node_ids=['071a66a8-3c47-49ad-84fa-7010c6277479'], metadata={}), \n   'doc_id_2': RefDocInfo(node_ids=['9563e84b-f934-41c3-acfd-22e88492c869'], metadata={}), \n   'doc_id_0': RefDocInfo(node_ids=['b53e6c2f-16f7-4024-af4c-42890e945f36'], metadata={}), \n   'doc_id_3': RefDocInfo(node_ids=['6bedb29f-15db-4c7c-9885-7490e10aa33f'], metadata={})}\n```\n\nEach entry in the output shows the ingested doc `id_`s as keys, and their associated `node_ids` of the nodes they were split into.Lastly, the original `metadata` dictionary of each input document is also tracked.You can read more about the `metadata` attribute in [Customizing Documents](../documents_and_nodes/usage_documents.md).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46562a93-963b-4a61-ba5c-c5cf7876b8b2": {"__data__": {"id_": "46562a93-963b-4a61-ba5c-c5cf7876b8b2", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\index_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a1aa2285c19382d89e58d784594751d622683655", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\index_guide.md", "author": "LlamaIndex"}, "hash": "d436fe3598a0bef38b87302a58ea6fec7125a8d9181589fb3fe1fe37e0e58e97"}}, "hash": "93e0769d8cf319dbf0a4b2b7366d94ebbc286a8a32bdf61762e3dea18f18485e", "text": "# How Each Index Works\n\nThis guide describes how each index works with diagrams. \n\nSome terminology:\n- **Node**: Corresponds to a chunk of text from a Document. LlamaIndex takes in Document objects and internally parses/chunks them into Node objects.\n- **Response Synthesis**: Our module which synthesizes a response given the retrieved Node. You can see how to \n    [specify different response modes](setting-response-mode) here. \n\n## Summary Index (formerly List Index)\n\nThe summary index simply stores Nodes as a sequential chain.\n\n![](/_static/indices/list.png)\n\n### Querying\n\nDuring query time, if no other query parameters are specified, LlamaIndex simply loads all Nodes in the list into\nour Response Synthesis module.\n\n![](/_static/indices/list_query.png)\n\nThe summary index does offer numerous ways of querying a summary index, from an embedding-based query which \nwill fetch the top-k neighbors, or with the addition of a keyword filter, as seen below:\n\n![](/_static/indices/list_filter_query.png)\n\n\n## Vector Store Index\n\nThe vector store index stores each Node and a corresponding embedding in a [Vector Store](vector-store-index).\n\n![](/_static/indices/vector_store.png)\n\n### Querying\n\nQuerying a vector store index involves fetching the top-k most similar Nodes, and passing\nthose into our Response Synthesis module.\n\n![](/_static/indices/vector_store_query.png)\n\n## Tree Index\n\nThe tree index builds a hierarchical tree from a set of Nodes (which become leaf nodes in this tree).\n\n![](/_static/indices/tree.png)\n\n### Querying\n\nQuerying a tree index involves traversing from root nodes down \nto leaf nodes. By default, (`child_branch_factor=1`), a query\nchooses one child node given a parent node. If `child_branch_factor=2`, a query\nchooses two child nodes per level.\n\n![](/_static/indices/tree_query.png)\n\n## Keyword Table Index\n\nThe keyword table index extracts keywords from each Node and builds a mapping from \neach keyword to the corresponding Nodes of that keyword.\n\n![](/_static/indices/keyword.png)\n\n### Querying\n\nDuring query time, we extract relevant keywords from the query, and match those with pre-extracted\nNode keywords to fetch the corresponding Nodes. The extracted Nodes are passed to our \nResponse Synthesis module.\n\n![](/_static/indices/keyword_query.png)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "27040026-fa6b-4c67-86b4-71232e5e56da": {"__data__": {"id_": "27040026-fa6b-4c67-86b4-71232e5e56da", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\metadata_extraction.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee73fa5bd93f4243e01056b1de61b3008d468363", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\metadata_extraction.md", "author": "LlamaIndex"}, "hash": "952d78f7209abe62739d7699362dbdca4c88299cccdcb90026caccc6e0319580"}, "3": {"node_id": "506de4fd-7b68-4a6b-ab6e-151903c85672", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\metadata_extraction.md", "author": "LlamaIndex"}, "hash": "e97cb0b67cc4f5db4f684d7b97895fa8318f05668268afe44a0e5050c1b184ca"}}, "hash": "675fadc48f45221b65641e1912c44f1711d87d3841b7850b4c2bb2af3660a377", "text": "# Metadata Extraction\n\n\n## Introduction\nIn many cases, especially with long documents, a chunk of text may lack the context necessary to disambiguate the chunk from other similar chunks of text.To combat this, we use LLMs to extract certain contextual information relevant to the document to better help the retrieval and language models disambiguate similar-looking passages.We show this in an [example notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/metadata_extraction/MetadataExtractionSEC.ipynb) and demonstrate its effectiveness in processing long documents.## Usage\n\nFirst, we define a metadata extractor that takes in a list of feature extractors that will be processed in sequence.We then feed this to the node parser, which will add the additional metadata to each node.```python\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.node_parser.extractors import (\n    MetadataExtractor,\n    SummaryExtractor,\n    QuestionsAnsweredExtractor,\n    TitleExtractor,\n    KeywordExtractor,\n    EntityExtractor,\n)\n\nmetadata_extractor = MetadataExtractor(\n    extractors=[\n        TitleExtractor(nodes=5),\n        QuestionsAnsweredExtractor(questions=3),\n        SummaryExtractor(summaries=[\"prev\", \"self\"]),\n        KeywordExtractor(keywords=10),\n        EntityExtractor(prediction_threshold=0.5),\n    ],\n)\n\nnode_parser = SimpleNodeParser.from_defaults(\n    metadata_extractor=metadata_extractor,\n)\n```\n\nHere is an sample of extracted metadata:\n\n```\n{'page_label': '2',\n 'file_name': '10k-132.pdf',\n 'document_title': 'Uber Technologies, Inc. 2019 Annual Report: Revolutionizing Mobility and Logistics Across 69 Countries and 111 Million MAPCs with $65 Billion in Gross Bookings',\n 'questions_this_excerpt_can_answer': '\\n\\n1.How many countries does Uber Technologies, Inc. operate in?\\n2.What is the total number of MAPCs served by Uber Technologies, Inc.?\\n3.How much gross bookings did Uber Technologies, Inc. generate in 2019?',\n 'prev_section_summary': \"\\n\\nThe 2019 Annual Report provides an overview of the key topics and entities that have been important to the organization over the past year.These include financial performance, operational highlights, customer satisfaction, employee engagement, and sustainability initiatives.It also provides an overview of the organization's strategic objectives and goals for the upcoming year.\",\n 'section_summary': '\\nThis section discusses a global tech platform that serves multiple multi-trillion dollar markets with products leveraging core technology and infrastructure.It enables consumers and drivers to tap a button and get a ride or work.The platform has revolutionized personal mobility with ridesharing and is now leveraging its platform to redefine the massive meal delivery and logistics industries.The foundation of the platform is its massive network, leading technology, operational excellence, and product expertise.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "506de4fd-7b68-4a6b-ab6e-151903c85672": {"__data__": {"id_": "506de4fd-7b68-4a6b-ab6e-151903c85672", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\metadata_extraction.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee73fa5bd93f4243e01056b1de61b3008d468363", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\metadata_extraction.md", "author": "LlamaIndex"}, "hash": "952d78f7209abe62739d7699362dbdca4c88299cccdcb90026caccc6e0319580"}, "2": {"node_id": "27040026-fa6b-4c67-86b4-71232e5e56da", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\metadata_extraction.md", "author": "LlamaIndex"}, "hash": "675fadc48f45221b65641e1912c44f1711d87d3841b7850b4c2bb2af3660a377"}}, "hash": "e97cb0b67cc4f5db4f684d7b97895fa8318f05668268afe44a0e5050c1b184ca", "text": "',\n 'excerpt_keywords': '\\nRidesharing, Mobility, Meal Delivery, Logistics, Network, Technology, Operational Excellence, Product Expertise, Point A, Point B'}\n```\n\n## Custom Extractors\n\nIf the provided extractors do not fit your needs, you can also define a custom extractor like so:\n```python\nfrom llama_index.node_parser.extractors import MetadataFeatureExtractor\n\nclass CustomExtractor(MetadataFeatureExtractor):\n    def extract(self, nodes) -> List[Dict]:\n        metadata_list = [\n            {\n                \"custom\": node.metadata[\"document_title\"]\n                + \"\\n\"\n                + node.metadata[\"excerpt_keywords\"]\n            }\n            for node in nodes\n        ]\n        return metadata_list\n```\n\nIn a more advanced example, it can also make use of an `llm` to extract features from the node content and the existing metadata.Refer to the [source code of the provided metadata extractors](https://github.com/jerryjliu/llama_index/blob/main/llama_index/node_parser/extractors/metadata_extractors.py) for more details.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f9f1105f-8f5d-4942-a700-13f2bb1a6abb": {"__data__": {"id_": "f9f1105f-8f5d-4942-a700-13f2bb1a6abb", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19db959161964ef1f07c9d41b941fe5d7d4dfec4", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\modules.md", "author": "LlamaIndex"}, "hash": "7723629d19d0c222a8adebeac5223958037369ead13c3b2821c6d3abfb2bb8e2"}}, "hash": "998f7395a9c7f3908b06e2233a5edf2c3be259bfed0fb8e9e1cb1ae1abc40af1", "text": "# Module Guides\n\n```{toctree}\n---\nmaxdepth: 1\n---\nvector_store_guide.ipynb\nSummary Index <./index_guide.md>\nTree Index <./index_guide.md>\nKeyword Table Index <./index_guide.md>\n/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.ipynb\n/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb\n/examples/query_engine/knowledge_graph_query_engine.ipynb\n/examples/query_engine/knowledge_graph_rag_query_engine.ipynb\nREBEL + Knowledge Graph Index <https://colab.research.google.com/drive/1G6pcR0pXvSkdMQlAK_P-IrYgo-_staxd?usp=sharing>\nSQL Index </examples/index_structs/struct_indices/SQLIndexDemo.ipynb>\n/examples/index_structs/struct_indices/duckdb_sql_query.ipynb\n/examples/index_structs/doc_summary/DocSummary.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b6977624-9a24-46bb-a840-4d6132bdfe80": {"__data__": {"id_": "b6977624-9a24-46bb-a840-4d6132bdfe80", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa048880f3a151c73ee65e041ad052064a76d4c1", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\root.md", "author": "LlamaIndex"}, "hash": "eed7f8f30fc9c0e4dfbfde8831310b19575c0c24c977275f3129433c5e32e1bf"}}, "hash": "99d18c1387735a31631f7d27a3ff2ac6e3648fede012e8c9cab30d541d1342f3", "text": "# Indexes\n\n## Concept\nAn `Index` is a data structure that allows us to quickly retrieve relevant context for a user query.\nFor LlamaIndex, it's the core foundation for retrieval-augmented generation (RAG) use-cases.\n\n\nAt a high-level, `Indices` are built from [Documents](/core_modules/data_modules/documents_and_nodes/root.md).\nThey are used to build [Query Engines](/core_modules/query_modules/query_engine/root.md) and [Chat Engines](/core_modules/query_modules/chat_engines/root.md)\nwhich enables question & answer and chat over your data.  \n\nUnder the hood, `Indices` store data in `Node` objects (which represent chunks of the original documents), and expose a [Retriever](/core_modules/query_modules/retriever/root.md) interface that supports additional configuration and automation.\n\nFor a more in-depth explanation, check out our guide below:\n```{toctree}\n---\nmaxdepth: 1\n---\nindex_guide.md\n```\n\n\n\n## Usage Pattern\nGet started with:\n```python\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(docs)\n```\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\n\n## Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```\n\n## Advanced Concepts\n\n```{toctree}\n---\nmaxdepth: 1\n---\ncomposability.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "735bc8a0-d89a-422d-8b09-53b423ad58fd": {"__data__": {"id_": "735bc8a0-d89a-422d-8b09-53b423ad58fd", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b387014631ce191702efc3c3fb9caef3c8272025", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "e3fbf876ee8c661b30c825fb2f3daf42f35f05aaebcbe6b1560d07d2b55b07b4"}}, "hash": "15365a17d4b2d9edabc270353a7702faeabdee7fcace0d42eb9107ce4865336f", "text": "# Usage Pattern\n\n## Get Started\n\nBuild an index from documents:\n\n```python\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(docs)\n```\n\n```{tip}\nTo learn how to load documents, see [Data Connectors](/core_modules/data_modules/connector/root.md)\n```\n\n### What is happening under the hood?\n\n1. Documents are chunked up and parsed into `Node` objects (which are lightweight abstractions over text str that additionally keep track of metadata and relationships).\n2. Additional computation is performed to add `Node` into index data structure\n   > Note: the computation is index-specific.\n   >\n   > - For a vector store index, this means calling an embedding model (via API or locally) to compute embedding for the `Node` objects\n   > - For a document summary index, this means calling an LLM to generate a summary\n\n## Configuring Document Parsing\n\nThe most common configuration you might want to change is how to parse document into `Node` objects.\n\n### High-Level API\n\nWe can configure our service context to use the desired chunk size and set `show_progress` to display a progress bar during index construction.\n\n```python\nfrom llama_index import ServiceContext, VectorStoreIndex\n\nservice_context = ServiceContext.from_defaults(chunk_size=512)\nindex = VectorStoreIndex.from_documents(\n    docs,\n    service_context=service_context,\n    show_progress=True\n)\n```\n\n> Note: While the high-level API optimizes for ease-of-use, it does _NOT_ expose full range of configurability.\n\n### Low-Level API\n\nYou can use the low-level composition API if you need more granular control.\n\nHere we show an example where you want to both modify the text chunk size, disable injecting metadata, and disable creating `Node` relationships.  \nThe steps are:\n\n1. Configure a node parser\n\n```python\nfrom llama_index.node_parser import SimpleNodeParser\n\nparser = SimpleNodeParser.from_defaults(\n    chunk_size=512,\n    include_extra_info=False,\n    include_prev_next_rel=False,\n)\n```\n\n2. Parse document into `Node` objects\n\n```python\nnodes = parser.get_nodes_from_documents(documents)\n```\n\n3. build index from `Node` objects\n\n```python\nindex = VectorStoreIndex(nodes)\n```\n\n## Handling Document Update\n\nRead more about how to deal with data sources that change over time with `Index` **insertion**, **deletion**, **update**, and **refresh** operations.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nmetadata_extraction.md\ndocument_management.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bfb19442-7092-4b42-a9a7-9c064aa615f8": {"__data__": {"id_": "bfb19442-7092-4b42-a9a7-9c064aa615f8", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3b3a1fd1da1034beecfcefaaba3657a21c7ab71", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\root.md", "author": "LlamaIndex"}, "hash": "d91d348536518a34ddff60a056d483ee431d3354c0e7743c1ecc75a66b48caf0"}}, "hash": "00a75bc30d4be1e04ae16627eb3bc33b8aa0203e739595d5349d5b26aa14ff5c", "text": "# Node Parser\n\n## Concept\n\nNode parsers are a simple abstraction that take a list of documents, and chunk them into `Node` objects, such that each node is a specific size. When a document is broken into nodes, all of it's attributes are inherited to the children nodes (i.e. `metadata`, text and metadata templates, etc.). You can read more about `Node` and `Document` properties [here](/core_modules/data_modules/documents_and_nodes/root.md).\n\nA node parser can configure the chunk size (in tokens) as well as any overlap between chunked nodes. The chunking is done by using a `TokenTextSplitter`, which default to a chunk size of 1024 and a default chunk overlap of 20 tokens.\n\n## Usage Pattern\n\n```python\nfrom llama_index.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n```\n\nYou can find more usage details and availbale customization options below.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nusage_pattern.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e8f9b733-7c89-4eca-b99d-9a07fcf20152": {"__data__": {"id_": "e8f9b733-7c89-4eca-b99d-9a07fcf20152", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "959043b9ea13ce947637c8838704ba205ea07dfc", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "369356f90425a9394de656c1a47c7a51bcd80e5208087430cb54f7b72edf1e28"}, "3": {"node_id": "1308b8dc-b079-497f-b853-0b147528dfd2", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "491efdd4c28f0b684acfc283fcd117fa5920e15bce84611665b6470d3736c41a"}}, "hash": "7a133c105334ea3659ace9e2276e79e7b34c582acc5e6760058063f8f557c20f", "text": "# Usage Pattern\n\n## Getting Started\n\nNode parsers can be used on their own:\n\n```python\nfrom llama_index import Document\nfrom llama_index.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n\nnodes = node_parser.get_nodes_from_documents([Document(text=\"long text\")], show_progress=False)\n```\n\nOr set inside a `ServiceContext` to be used automatically when an index is constructed using `.from_documents()`:\n\n```python\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext\nfrom llama_index.node_parser import SimpleNodeParser\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\nservice_context = ServiceContext.from_defaults(node_parser=node_parser)\n\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n```\n\n## Customization\n\nThere are several options available to customize:\n\n- `text_splitter` (defaults to `TokenTextSplitter`) - the text splitter used to split text into chunks.- `include_metadata` (defaults to `True`) - whether or not `Node`s should inherit the document metadata.- `include_prev_next_rel` (defaults to `True`) - whether or not to include previous/next relationships between chunked `Node`s\n- `metadata_extractor` (defaults to `None`) - extra processing to extract helpful metadata.See [here for details](/core_modules/data_modules/documents_and_nodes/usage_metadata_extractor.md).If you don't want to change the `text_splitter`, you can use `SimpleNodeParser.from_defaults()` to easily change the chunk size and chunk overlap.The defaults are 1024 and 20 respectively.```python\nfrom llama_index.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n```\n\n### Text Splitter Customization\n\nIf you do customize the `text_splitter` from the default `SentenceSplitter`, you can use any splitter from langchain, or optionally our `TokenTextSplitter` or `CodeSplitter`.Each text splitter has options for the default separator, as well as options for additional config.These are useful for languages that are sufficiently different from English.`SentenceSplitter` default configuration:\n\n```python\nimport tiktoken\nfrom llama_index.text_splitter import SentenceSplitter\n\ntext_splitter = SentenceSplitter(\n  separator=\" \",\n  chunk_size=1024,\n  chunk_overlap=20,\n  paragraph_separator=\"\\n\\n\\n\",\n  secondary_chunking_regex=\"[^,.;\u3002]+[,.;\u3002]?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1308b8dc-b079-497f-b853-0b147528dfd2": {"__data__": {"id_": "1308b8dc-b079-497f-b853-0b147528dfd2", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "959043b9ea13ce947637c8838704ba205ea07dfc", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "369356f90425a9394de656c1a47c7a51bcd80e5208087430cb54f7b72edf1e28"}, "2": {"node_id": "e8f9b733-7c89-4eca-b99d-9a07fcf20152", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "7a133c105334ea3659ace9e2276e79e7b34c582acc5e6760058063f8f557c20f"}}, "hash": "491efdd4c28f0b684acfc283fcd117fa5920e15bce84611665b6470d3736c41a", "text": "\",\n  tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n)\n\nnode_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter)\n```\n\n`TokenTextSplitter` default configuration:\n\n```python\nimport tiktoken\nfrom llama_index.text_splitter import TokenTextSplitter\n\ntext_splitter = TokenTextSplitter(\n  separator=\" \",\n  chunk_size=1024,\n  chunk_overlap=20,\n  backup_separators=[\"\\n\"],\n  tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n)\n\nnode_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter)\n```\n\n`CodeSplitter` configuration:\n\n```python\nfrom llama_index.text_splitter import CodeSplitter\n\ntext_splitter = CodeSplitter(\n  language=\"python\",\n  chunk_lines=40,\n  chunk_lines_overlap=15,\n  max_chars=1500,\n)\n\nnode_parser = SimpleNodeParser.from_defaults(text_splitter=text_splitter)\n```\n\n## SentenceWindowNodeParser\n\nThe `SentenceWindowNodeParser` is similar to the `SimpleNodeParser`, except that it splits all documents into individual sentences.The resulting nodes also contain the surrounding \"window\" of sentences around each node in the metadata.Note that this metadata will not be visible to the LLM or embedding model.This is most useful for generating embeddings that have a very specific scope.Then, combined with a `MetadataReplacementNodePostProcessor`, you can replace the sentence with it's surrounding context before sending the node to the LLM.An example of setting up the parser with default settings is below.In practice, you would usually only want to adjust the window size of sentences.```python\nimport nltk\nfrom llama_index.node_parser import SentenceWindowNodeParser\n\nnode_parser = SentenceWindowNodeParser.from_defaults(\n  # how many sentences on either side to capture\n  window_size=3,  \n  # the metadata key that holds the window of surrounding sentences\n  window_metadata_key=\"window\",  \n  # the metadata key that holds the original sentence\n  original_text_metadata_key=\"original_sentence\"\n)\n```\n\nA full example can be found [here in combination with the `MetadataReplacementNodePostProcessor`](/examples/node_postprocessor/MetadataReplacementDemo.ipynb).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "811b2cb7-db86-4a14-a8a9-46b0a494c161": {"__data__": {"id_": "811b2cb7-db86-4a14-a8a9-46b0a494c161", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\customization.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9018b3ef050474178de0ce352ac24e9a4ad25394", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\customization.md", "author": "LlamaIndex"}, "hash": "24de1a7cb74c2910355527e037ace2901b01023c99770ce9de936ba90fb41351"}, "3": {"node_id": "c1162b51-89f2-4f63-947a-8e53179cea32", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\customization.md", "author": "LlamaIndex"}, "hash": "1d691bbcfbf65e5c36644196bd6edc66b0276b7b18ef7242a31bb9add6d2f564"}}, "hash": "8888a9ba121f361b339d1af0a31e2faf7ff13dc566cc83e0e6205839c68f9411", "text": "# Customizing Storage\n\nBy default, LlamaIndex hides away the complexities and let you query your data in under 5 lines of code:\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Summarize the documents.\")```\n\nUnder the hood, LlamaIndex also supports a swappable **storage layer** that allows you to customize where ingested documents (i.e., `Node` objects), embedding vectors, and index metadata are stored.![](/_static/storage/storage.png)\n\n### Low-Level API\n\nTo do this, instead of the high-level API,\n\n```python\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nwe use a lower-level API that gives more granular control:\n\n```python\nfrom llama_index.storage.docstore import SimpleDocumentStore\nfrom llama_index.storage.index_store import SimpleIndexStore\nfrom llama_index.vector_stores import SimpleVectorStore\nfrom llama_index.node_parser import SimpleNodeParser\n\n# create parser and parse document into nodes\nparser = SimpleNodeParser.from_defaults()\nnodes = parser.get_nodes_from_documents(documents)\n\n# create storage context using default stores\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore(),\n    vector_store=SimpleVectorStore(),\n    index_store=SimpleIndexStore(),\n)\n\n# create (or load) docstore and add nodes\nstorage_context.docstore.add_documents(nodes)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\n# save index\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\n# can also set index_id to save multiple indexes to the same folder\nindex.set_index_id = \"<index_id>\"\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n\n# to load index later, make sure you setup the storage context\n# this will loaded the persisted stores from persist_dir\nstorage_context = StorageContext.from_defaults(\n    persist_dir=\"<persist_dir>\"\n)\n\n# then load the index object\nfrom llama_index import load_index_from_storage\nloaded_index = load_index_from_storage(storage_context)\n\n# if loading an index from a persist_dir containing multiple indexes\nloaded_index = load_index_from_storage(storage_context, index_id=\"<index_id>\")\n\n# if loading multiple indexes from a persist dir\nloaded_indicies = load_index_from_storage(storage_context, index_ids=[\"<index_id>\", ...])\n```\n\nYou can customize the underlying storage with a one-line change to instantiate different document stores, index stores, and vector stores.See [Document Stores](./docstores.md), [Vector Stores](./vector_stores.md), [Index Stores](./index_stores.md) guides for more details.For saving and loading a graph/composable index, see the [full guide here](../index/composability.md).### Vector Store Integrations and Storage\n\nMost of our vector store integrations store the entire index (vectors + text) in the vector store itself.This comes with the major benefit of not having to exlicitly persist the index as shown above, since the vector store is already hosted and persisting the data in our index.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1162b51-89f2-4f63-947a-8e53179cea32": {"__data__": {"id_": "c1162b51-89f2-4f63-947a-8e53179cea32", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\customization.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9018b3ef050474178de0ce352ac24e9a4ad25394", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\customization.md", "author": "LlamaIndex"}, "hash": "24de1a7cb74c2910355527e037ace2901b01023c99770ce9de936ba90fb41351"}, "2": {"node_id": "811b2cb7-db86-4a14-a8a9-46b0a494c161", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\customization.md", "author": "LlamaIndex"}, "hash": "8888a9ba121f361b339d1af0a31e2faf7ff13dc566cc83e0e6205839c68f9411"}}, "hash": "1d691bbcfbf65e5c36644196bd6edc66b0276b7b18ef7242a31bb9add6d2f564", "text": "The vector stores that support this practice are:\n\n- CognitiveSearchVectorStore\n- ChatGPTRetrievalPluginClient\n- CassandraVectorStore\n- ChromaVectorStore\n- EpsillaVectorStore\n- DocArrayHnswVectorStore\n- DocArrayInMemoryVectorStore\n- LanceDBVectorStore\n- MetalVectorStore\n- MilvusVectorStore\n- MyScaleVectorStore\n- OpensearchVectorStore\n- PineconeVectorStore\n- QdrantVectorStore\n- RedisVectorStore\n- WeaviateVectorStore\n\nA small example using Pinecone is below:\n\n```python\nimport pinecone\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.vector_stores import PineconeVectorStore\n\n# Creating a Pinecone index\napi_key = \"api_key\"\npinecone.init(api_key=api_key, environment=\"us-west1-gcp\")\npinecone.create_index(\n    \"quickstart\",\n    dimension=1536,\n    metric=\"euclidean\",\n    pod_type=\"p1\"\n)\nindex = pinecone.Index(\"quickstart\")\n\n# construct vector store\nvector_store = PineconeVectorStore(pinecone_index=index)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# create index, which will insert documents/vectors to pinecone\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n```\n\nIf you have an existing vector store with data already loaded in,\nyou can connect to it and directly create a `VectorStoreIndex` as follows:\n\n```python\nindex = pinecone.Index(\"quickstart\")\nvector_store = PineconeVectorStore(pinecone_index=index)\nloaded_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d8f9b13b-6791-42b3-bd6e-b000d8b06da2": {"__data__": {"id_": "d8f9b13b-6791-42b3-bd6e-b000d8b06da2", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\docstores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5029286744cf4d84add453002b699319fcfe1eba", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\docstores.md", "author": "LlamaIndex"}, "hash": "04f045d7424cf4d54455fa367a85e87195f75d574a4a92244d7da438d8abf4aa"}, "3": {"node_id": "9b96a9ee-9180-4349-a49c-01882c8c2379", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\docstores.md", "author": "LlamaIndex"}, "hash": "70259ccca05c88762fc860bd44e7fa4a17aa598364b47d83d8016603c30cc2d1"}}, "hash": "57bea01a9062d5537a4004457ceba2a5213803372803fc499fe8a935fb07beda", "text": "# Document Stores\nDocument stores contain ingested document chunks, which we call `Node` objects.\n\nSee the [API Reference](/api_reference/storage/docstore.rst) for more details.\n\n\n### Simple Document Store\nBy default, the `SimpleDocumentStore` stores `Node` objects in-memory.They can be persisted to (and loaded from) disk by calling `docstore.persist()` (and `SimpleDocumentStore.from_persist_path(...)` respectively).A more complete example can be found [here](../../examples/docstore/DocstoreDemo.ipynb)\n\n### MongoDB Document Store\nWe support MongoDB as an alternative document store backend that persists data as `Node` objects are ingested.```python\nfrom llama_index.storage.docstore import MongoDocumentStore\nfrom llama_index.node_parser import SimpleNodeParser\n\n# create parser and parse document into nodes \nparser = SimpleNodeParser.from_defaults()\nnodes = parser.get_nodes_from_documents(documents)\n\n# create (or load) docstore and add nodes\ndocstore = MongoDocumentStore.from_uri(uri=\"<mongodb+srv://...>\")\ndocstore.add_documents(nodes)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n```\n\nUnder the hood, `MongoDocumentStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your nodes.> Note: You can configure the `db_name` and `namespace` when instantiating `MongoDocumentStore`, otherwise they default to `db_name=\"db_docstore\"` and `namespace=\"docstore\"`.Note that it's not necessary to call `storage_context.persist()` (or `docstore.persist()`) when using an `MongoDocumentStore`\nsince data is persisted by default.You can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoDocumentStore` with an existing `db_name` and `collection_name`.A more complete example can be found [here](../../examples/docstore/MongoDocstoreDemo.ipynb)\n\n### Redis Document Store\n\nWe support Redis as an alternative document store backend that persists data as `Node` objects are ingested.```python\nfrom llama_index.storage.docstore import RedisDocumentStore\nfrom llama_index.node_parser import SimpleNodeParser\n\n# create parser and parse document into nodes \nparser = SimpleNodeParser.from_defaults()\nnodes = parser.get_nodes_from_documents(documents)\n\n# create (or load) docstore and add nodes\ndocstore = RedisDocumentStore.from_host_and_port(\n  host=\"127.0.0.1\", \n  port=\"6379\", \n  namespace='llama_index'\n)\ndocstore.add_documents(nodes)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n```\n\nUnder the hood, `RedisDocumentStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/docs`.> Note: You can configure the `namespace` when instantiating `RedisDocumentStore`, otherwise it defaults `namespace=\"docstore\"`.You can easily reconnect to your Redis client and reload the index by re-initializing a `RedisDocumentStore` with an existing `host`, `port`, and `namespace`.A more complete example can be found [here](../../examples/docstore/RedisDocstoreIndexStoreDemo.ipynb)\n\n### Firestore Document Store\n\nWe support Firestore as an alternative document store backend that persists data as `Node` objects are ingested.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9b96a9ee-9180-4349-a49c-01882c8c2379": {"__data__": {"id_": "9b96a9ee-9180-4349-a49c-01882c8c2379", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\docstores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5029286744cf4d84add453002b699319fcfe1eba", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\docstores.md", "author": "LlamaIndex"}, "hash": "04f045d7424cf4d54455fa367a85e87195f75d574a4a92244d7da438d8abf4aa"}, "2": {"node_id": "d8f9b13b-6791-42b3-bd6e-b000d8b06da2", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\docstores.md", "author": "LlamaIndex"}, "hash": "57bea01a9062d5537a4004457ceba2a5213803372803fc499fe8a935fb07beda"}}, "hash": "70259ccca05c88762fc860bd44e7fa4a17aa598364b47d83d8016603c30cc2d1", "text": "```python\nfrom llama_index.storage.docstore import FirestoreDocumentStore\nfrom llama_index.node_parser import SimpleNodeParser\n\n# create parser and parse document into nodes\nparser = SimpleNodeParser.from_defaults()\nnodes = parser.get_nodes_from_documents(documents)\n\n# create (or load) docstore and add nodes\ndocstore = FirestoreDocumentStore.from_dataabse(\n  project=\"project-id\",\n  database=\"(default)\",\n)\ndocstore.add_documents(nodes)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(docstore=docstore)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n```\n\nUnder the hood, `FirestoreDocumentStore` connects to a firestore database in Google Cloud and adds your nodes to a namespace stored under `{namespace}/docs`.> Note: You can configure the `namespace` when instantiating `FirestoreDocumentStore`, otherwise it defaults `namespace=\"docstore\"`.You can easily reconnect to your Firestore database and reload the index by re-initializing a `FirestoreDocumentStore` with an existing `project`, `database`, and `namespace`.A more complete example can be found [here](../../examples/docstore/FirestoreDemo.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7fcb8820-8880-4da2-a3a4-89c1899defc9": {"__data__": {"id_": "7fcb8820-8880-4da2-a3a4-89c1899defc9", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\index_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a09614387a4f717b34205dfe0248c46bc13580f", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\index_stores.md", "author": "LlamaIndex"}, "hash": "baa3dc58b8009e4cb30e44b2248184af46c547c635bfadc12aaef57568e3dcb4"}}, "hash": "507fdc76bb2a59d946f6b27b1b8ec29fd9ffdb017eaf1f5542db26f0eae5098f", "text": "# Index Stores\n\nIndex stores contains lightweight index metadata (i.e. additional state information created when building an index).\n\nSee the [API Reference](/api_reference/storage/index_store.rst) for more details.\n\n### Simple Index Store\nBy default, LlamaIndex uses a simple index store backed by an in-memory key-value store.\nThey can be persisted to (and loaded from) disk by calling `index_store.persist()` (and `SimpleIndexStore.from_persist_path(...)` respectively).\n\n\n### MongoDB Index Store\nSimilarly to document stores, we can also use `MongoDB` as the storage backend of the index store.\n\n\n```python\nfrom llama_index.storage.index_store import MongoIndexStore\nfrom llama_index import VectorStoreIndex\n\n# create (or load) index store\nindex_store = MongoIndexStore.from_uri(uri=\"<mongodb+srv://...>\")\n\n# create storage context\nstorage_context = StorageContext.from_defaults(index_store=index_store)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\n# or alternatively, load index\nfrom llama_index import load_index_from_storage\nindex = load_index_from_storage(storage_context)\n```\n\nUnder the hood, `MongoIndexStore` connects to a fixed MongoDB database and initializes new collections (or loads existing collections) for your index metadata.\n> Note: You can configure the `db_name` and `namespace` when instantiating `MongoIndexStore`, otherwise they default to `db_name=\"db_docstore\"` and `namespace=\"docstore\"`.\n\nNote that it's not necessary to call `storage_context.persist()` (or `index_store.persist()`) when using an `MongoIndexStore`\nsince data is persisted by default. \n\nYou can easily reconnect to your MongoDB collection and reload the index by re-initializing a `MongoIndexStore` with an existing `db_name` and `collection_name`.\n\nA more complete example can be found [here](../../examples/docstore/MongoDocstoreDemo.ipynb)\n\n### Redis Index Store\n\nWe support Redis as an alternative document store backend that persists data as `Node` objects are ingested.\n\n```python\nfrom llama_index.storage.index_store import RedisIndexStore\nfrom llama_index import VectorStoreIndex\n\n# create (or load) docstore and add nodes\nindex_store = RedisIndexStore.from_host_and_port(\n  host=\"127.0.0.1\", \n  port=\"6379\", \n  namespace='llama_index'\n)\n\n# create storage context\nstorage_context = StorageContext.from_defaults(index_store=index_store)\n\n# build index\nindex = VectorStoreIndex(nodes, storage_context=storage_context)\n\n# or alternatively, load index\nfrom llama_index import load_index_from_storage\nindex = load_index_from_storage(storage_context)\n```\n\nUnder the hood, `RedisIndexStore` connects to a redis database and adds your nodes to a namespace stored under `{namespace}/index`.\n> Note: You can configure the `namespace` when instantiating `RedisIndexStore`, otherwise it defaults `namespace=\"index_store\"`.\n\nYou can easily reconnect to your Redis client and reload the index by re-initializing a `RedisIndexStore` with an existing `host`, `port`, and `namespace`.\n\nA more complete example can be found [here](../../examples/docstore/RedisDocstoreIndexStoreDemo.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d82c4727-508f-414e-bd64-5a5e6ee875c7": {"__data__": {"id_": "d82c4727-508f-414e-bd64-5a5e6ee875c7", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\kv_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e03ed535bc23260d16c06aa9f38ebfa5a41f719", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\kv_stores.md", "author": "LlamaIndex"}, "hash": "57333b6c6934830209016b2dd3c6f1dcf7f86a3f3fec22fe95139865b5c28b2c"}}, "hash": "26c110b1cf86f50e665345421180f45f1294bee74b63c3c4867a0f7bdc7b6d39", "text": "# Key-Value Stores\n\nKey-Value stores are the underlying storage abstractions that power our [Document Stores](./docstores.md) and [Index Stores](./index_stores.md).\n\nWe provide the following key-value stores:\n- **Simple Key-Value Store**: An in-memory KV store. The user can choose to call `persist` on this kv store to persist data to disk.\n- **MongoDB Key-Value Store**: A MongoDB KV store.\n\nSee the [API Reference](/api_reference/storage/kv_store.rst) for more details.\n\nNote: At the moment, these storage abstractions are not externally facing.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5576d7f8-447c-4cbb-84c3-314a78d44f8f": {"__data__": {"id_": "5576d7f8-447c-4cbb-84c3-314a78d44f8f", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b47e16f4293839f02416d7fb9158cc92ab030f7f", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\root.md", "author": "LlamaIndex"}, "hash": "322945abd0c02d474b9201a2019612b7bbddad545df418f08bd99a53282834af"}}, "hash": "ced1facc3f0adba3e1c986d5aa1087f87d04435a01bebeb4bd2332bcc3a6077c", "text": "# Storage\n\n## Concept\n\nLlamaIndex provides a high-level interface for ingesting, indexing, and querying your external data.\n\nUnder the hood, LlamaIndex also supports swappable **storage components** that allows you to customize:\n\n- **Document stores**: where ingested documents (i.e., `Node` objects) are stored,\n- **Index stores**: where index metadata are stored,\n- **Vector stores**: where embedding vectors are stored.\n- **Graph stores**: where knowledge graphs are stored (i.e. for `KnowledgeGraphIndex`).\n\nThe Document/Index stores rely on a common Key-Value store abstraction, which is also detailed below.\n\nLlamaIndex supports persisting data to any storage backend supported by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/index.html). \nWe have confirmed support for the following storage backends:\n\n- Local filesystem\n- AWS S3\n- Cloudflare R2\n\n\n![](/_static/storage/storage.png)\n\n## Usage Pattern\n\nMany vector stores (except FAISS) will store both the data as well as the index (embeddings). This means that you will not need to use a separate document store or index store. This *also* means that you will not need to explicitly persist this data - this happens automatically. Usage would look something like the following to build a new index / reload an existing one.\n\n```python\n\n## build a new index\nfrom llama_index import VectorStoreIndex, StorageContext\nfrom llama_index.vector_stores import DeepLakeVectorStore\n# construct vector store and customize storage context\nvector_store = DeepLakeVectorStore(dataset_path=\"<dataset_path>\")\nstorage_context = StorageContext.from_defaults(\n    vector_store = vector_store\n)\n# Load documents and build index\nindex = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n\n\n## reload an existing one\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n```\n\nSee our [Vector Store Module Guide](vector_stores.md) below for more details.\n\n\nNote that in general to use storage abstractions, you need to define a `StorageContext` object:\n\n```python\nfrom llama_index.storage.docstore import SimpleDocumentStore\nfrom llama_index.storage.index_store import SimpleIndexStore\nfrom llama_index.vector_stores import SimpleVectorStore\nfrom llama_index.storage import StorageContext\n\n# create storage context using default stores\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore(),\n    vector_store=SimpleVectorStore(),\n    index_store=SimpleIndexStore(),\n)\n```\n\nMore details on customization/persistence can be found in the guides below.\n\n\n```{toctree}\n---\nmaxdepth: 1\n---\ncustomization.md\nsave_load.md\n```\n\n\n\n## Modules\n\nWe offer in-depth guides on the different storage components.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nvector_stores.md\ndocstores.md\nindex_stores.md\nkv_stores.md\n/community/integrations/graph_stores.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e48b0d18-726d-471d-9702-0a63215a1d2e": {"__data__": {"id_": "e48b0d18-726d-471d-9702-0a63215a1d2e", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\save_load.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "426fce42ccb2d16c3128dd7e64e3e5cecb989028", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\save_load.md", "author": "LlamaIndex"}, "hash": "fb92faf195f6da29e7dcb0215645882b4742d227f2b65d922c762e150b1abdba"}, "3": {"node_id": "7acbc142-6cba-4d56-8100-21465098753a", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\save_load.md", "author": "LlamaIndex"}, "hash": "5ec48d018f2b45c1067463f413a7dd85b79582ae45c15946f8cb5d9e4b28003f"}}, "hash": "f796c13402bc57fa63538c399bf42057966d4da112ecadaa59e7a251b083b279", "text": "# Persisting & Loading Data\n\n## Persisting Data\nBy default, LlamaIndex stores data in-memory, and this data can be explicitly persisted if desired:\n```python\nstorage_context.persist(persist_dir=\"<persist_dir>\")\n```\nThis will persist data to disk, under the specified `persist_dir` (or `./storage` by default).\n\nMultiple indexes can be persisted and loaded from the same directory, assuming you keep track of index ID's for loading.\n\nUser can also configure alternative storage backends (e.g. `MongoDB`) that persist data by default.\nIn this case, calling `storage_context.persist()` will do nothing.\n\n## Loading Data\nTo load data, user simply needs to re-create the storage context using the same configuration (e.g. pass in the same `persist_dir` or vector store client).\n\n```python\nstorage_context = StorageContext.from_defaults(\n    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n    vector_store=SimpleVectorStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"<persist_dir>\"),\n)\n```\n\nWe can then load specific indices from the `StorageContext` through some convenience functions below.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7acbc142-6cba-4d56-8100-21465098753a": {"__data__": {"id_": "7acbc142-6cba-4d56-8100-21465098753a", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\save_load.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "426fce42ccb2d16c3128dd7e64e3e5cecb989028", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\save_load.md", "author": "LlamaIndex"}, "hash": "fb92faf195f6da29e7dcb0215645882b4742d227f2b65d922c762e150b1abdba"}, "2": {"node_id": "e48b0d18-726d-471d-9702-0a63215a1d2e", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\save_load.md", "author": "LlamaIndex"}, "hash": "f796c13402bc57fa63538c399bf42057966d4da112ecadaa59e7a251b083b279"}}, "hash": "5ec48d018f2b45c1067463f413a7dd85b79582ae45c15946f8cb5d9e4b28003f", "text": "```python\nfrom llama_index import load_index_from_storage, load_indices_from_storage, load_graph_from_storage\n\n# load a single index\n# need to specify index_id if multiple indexes are persisted to the same directory\nindex = load_index_from_storage(storage_context, index_id=\"<index_id>\") \n\n# don't need to specify index_id if there's only one index in storage context\nindex = load_index_from_storage(storage_context) \n\n# load multiple indices\nindices = load_indices_from_storage(storage_context) # loads all indices\nindices = load_indices_from_storage(storage_context, index_ids=[index_id1, ...]) # loads specific indices\n\n# load composable graph\ngraph = load_graph_from_storage(storage_context, root_id=\"<root_id>\") # loads graph with the specified root_id\n```\n\nHere's the full [API Reference on saving and loading](/api_reference/storage/indices_save_load.rst).\n\n## Using a remote backend\n\nBy default, LlamaIndex uses a local filesystem to load and save files. However, you can override this by passing a `fsspec.AbstractFileSystem` object.\n\nHere's a simple example, instantiating a vector store:\n```python\nimport dotenv\nimport s3fs\nimport os\ndotenv.load_dotenv(\"../../../.env\")\n\n# load documents\ndocuments = SimpleDirectoryReader('../../../examples/paul_graham_essay/data/').load_data()\nprint(len(documents))\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nAt this point, everything has been the same. Now - let's instantiate a S3 filesystem and save / load from there.\n\n```python\n# set up s3fs\nAWS_KEY = os.environ['AWS_ACCESS_KEY_ID']\nAWS_SECRET = os.environ['AWS_SECRET_ACCESS_KEY']\nR2_ACCOUNT_ID = os.environ['R2_ACCOUNT_ID']\n\nassert AWS_KEY is not None and AWS_KEY != \"\"\n\ns3 = s3fs.S3FileSystem(\n   key=AWS_KEY,\n   secret=AWS_SECRET,\n   endpoint_url=f'https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com',\n   s3_additional_kwargs={'ACL': 'public-read'}\n)\n\n# save index to remote blob storage\nindex.set_index_id(\"vector_index\")\n# this is {bucket_name}/{index_name}\nindex.storage_context.persist('llama-index/storage_demo', fs=s3)\n\n# load index from s3\nsc = StorageContext.from_defaults(persist_dir='llama-index/storage_demo', fs=s3)\nindex2 = load_index_from_storage(sc, 'vector_index')\n```\n\nBy default, if you do not pass a filesystem, we will assume a local filesystem.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "626a612e-5a29-45d3-a4e0-bf4e7c89b2d5": {"__data__": {"id_": "626a612e-5a29-45d3-a4e0-bf4e7c89b2d5", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19d595ddd7cebd27fdcb8993e483cc23affad291", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "0856d698629f2ff6bfca10635bd33200487d51241335a6a4fe7cec0b5ff316e6"}, "3": {"node_id": "f73077f6-5d32-4c13-af90-2452491bcd54", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "81c59a4a55cd357190215c931696226c117ca45b955e9bb13a3b115127e38bdb"}}, "hash": "af87fdcc81c205688cbd83b403923fd1dfdd8f3c4e281f660bb2ab145d3461c7", "text": "# Vector Stores\n\nVector stores contain embedding vectors of ingested document chunks\n(and sometimes the document chunks as well).## Simple Vector Store\n\nBy default, LlamaIndex uses a simple in-memory vector store that's great for quick experimentation.They can be persisted to (and loaded from) disk by calling `vector_store.persist()` (and `SimpleVectorStore.from_persist_path(...)` respectively).## Vector Store Options & Feature Support\n\nLlamaIndex supports over 20 different vector store options.We are actively adding more integrations and improving feature coverage for each.| Vector Store             | Type                | Metadata Filtering | Hybrid Search | Delete | Store Documents | Async |\n| ------------------------ | ------------------- | ------------------ | ------------- | ------ | --------------- | ----- |\n| Elasticsearch            | self-hosted / cloud | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Pinecone                 | cloud               | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Weaviate                 | self-hosted / cloud | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n| Postgres                 | self-hosted / cloud | \u2713                  | \u2713             | \u2713      | \u2713               | \u2713     |\n| Cassandra                | self-hosted / cloud | \u2713                  |               | \u2713      | \u2713               |       |\n| Qdrant                   | self-hosted / cloud | \u2713                  |               | \u2713      | \u2713               |       |\n| Chroma                   | self-hosted         | \u2713                  |               | \u2713      | \u2713               |       |\n| Milvus / Zilliz          | self-hosted / cloud | \u2713                  |               | \u2713      | \u2713               |       |\n| Typesense                | self-hosted / cloud", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f73077f6-5d32-4c13-af90-2452491bcd54": {"__data__": {"id_": "f73077f6-5d32-4c13-af90-2452491bcd54", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19d595ddd7cebd27fdcb8993e483cc23affad291", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "0856d698629f2ff6bfca10635bd33200487d51241335a6a4fe7cec0b5ff316e6"}, "2": {"node_id": "626a612e-5a29-45d3-a4e0-bf4e7c89b2d5", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "af87fdcc81c205688cbd83b403923fd1dfdd8f3c4e281f660bb2ab145d3461c7"}, "3": {"node_id": "4dab9719-d28e-4d95-a862-eb6e16b925c7", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "99a91306e77afa74ca17bc7ed130a09e2d7d2394c03262b1b84402ef164440c1"}}, "hash": "81c59a4a55cd357190215c931696226c117ca45b955e9bb13a3b115127e38bdb", "text": "| \u2713                  |               | \u2713      | \u2713               |       |\n| Supabase                 | self-hosted / cloud | \u2713                  |               | \u2713      | \u2713               |       |\n| MongoDB Atlas            | self-hosted / cloud | \u2713                  |               | \u2713      | \u2713               |       |\n| Redis                    | self-hosted / cloud | \u2713                  |               | \u2713      | \u2713               |       |\n| Deeplake                 | self-hosted / cloud | \u2713                  |               | \u2713      | \u2713               |       |\n| OpenSearch               | self-hosted / cloud | \u2713                  |               | \u2713      | \u2713               |       |\n| Neo4jVector              | self-hosted / cloud |                    |               | \u2713      | \u2713               |       |\n| Azure Cognitive Search   | cloud               |                    | \u2713             | \u2713      | \u2713               |       |\n| DynamoDB                 | cloud               |                    |               | \u2713      |                 |       |\n| LanceDB                  | cloud               | \u2713                  |               | \u2713      | \u2713               |       |\n| Metal                    | cloud               | \u2713                  |", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4dab9719-d28e-4d95-a862-eb6e16b925c7": {"__data__": {"id_": "4dab9719-d28e-4d95-a862-eb6e16b925c7", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19d595ddd7cebd27fdcb8993e483cc23affad291", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "0856d698629f2ff6bfca10635bd33200487d51241335a6a4fe7cec0b5ff316e6"}, "2": {"node_id": "f73077f6-5d32-4c13-af90-2452491bcd54", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "81c59a4a55cd357190215c931696226c117ca45b955e9bb13a3b115127e38bdb"}, "3": {"node_id": "d8206bbc-6e15-4345-964f-0db151da7a97", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "f84674fc000606238dbb887f74bdd6c09b3fe720785d8217978f1e25afc5d5f0"}}, "hash": "99a91306e77afa74ca17bc7ed130a09e2d7d2394c03262b1b84402ef164440c1", "text": "| \u2713      | \u2713               |       |\n| MyScale                  | cloud               |                    |               |        | \u2713               |       |\n| Tair                     | cloud               | \u2713                  |               | \u2713      | \u2713               |       |\n| Simple                   | in-memory           |                    |               | \u2713      |                 |       |\n| FAISS                    | in-memory           |                    |               |        |                 |       |\n| ChatGPT Retrieval Plugin | aggregator          |                    |               | \u2713      | \u2713               |       |\n| DocArray                 | aggregator          | \u2713                  |               | \u2713      | \u2713               |       |\n| Azure Cognitive Search   | cloud               | \u2713                  | \u2713             | \u2713      | \u2713               |       |\n\nFor more details, see [Vector Store Integrations](/community/integrations/vector_stores.md).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d8206bbc-6e15-4345-964f-0db151da7a97": {"__data__": {"id_": "d8206bbc-6e15-4345-964f-0db151da7a97", "embedding": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19d595ddd7cebd27fdcb8993e483cc23affad291", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "0856d698629f2ff6bfca10635bd33200487d51241335a6a4fe7cec0b5ff316e6"}, "2": {"node_id": "4dab9719-d28e-4d95-a862-eb6e16b925c7", "node_type": null, "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}, "hash": "99a91306e77afa74ca17bc7ed130a09e2d7d2394c03262b1b84402ef164440c1"}}, "hash": "f84674fc000606238dbb887f74bdd6c09b3fe720785d8217978f1e25afc5d5f0", "text": "```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n/examples/vector_stores/ElasticsearchIndexDemo.ipynb\n/examples/vector_stores/SimpleIndexDemo.ipynb\n/examples/vector_stores/RocksetIndexDemo.ipynb\n/examples/vector_stores/QdrantIndexDemo.ipynb\n/examples/vector_stores/FaissIndexDemo.ipynb\n/examples/vector_stores/DeepLakeIndexDemo.ipynb\n/examples/vector_stores/MyScaleIndexDemo.ipynb\n/examples/vector_stores/MetalIndexDemo.ipynb\n/examples/vector_stores/WeaviateIndexDemo.ipynb\n/examples/vector_stores/OpensearchDemo.ipynb\n/examples/vector_stores/PineconeIndexDemo.ipynb\n/examples/vector_stores/ChromaIndexDemo.ipynb\n/examples/vector_stores/LanceDBIndexDemo.ipynb\n/examples/vector_stores/MilvusIndexDemo.ipynb\n/examples/vector_stores/RedisIndexDemo.ipynb\n/examples/vector_stores/WeaviateIndexDemo-Hybrid.ipynb\n/examples/vector_stores/ZepIndexDemo.ipynb\n/examples/vector_stores/PineconeIndexDemo-Hybrid.ipynb\n/examples/vector_stores/AsyncIndexCreationDemo.ipynb\n/examples/vector_stores/TairIndexDemo.ipynb\n/examples/vector_stores/SupabaseVectorIndexDemo.ipynb\n/examples/vector_stores/DocArrayHnswIndexDemo.ipynb\n/examples/vector_stores/DocArrayInMemoryIndexDemo.ipynb\n/examples/vector_stores/MongoDBAtlasVectorSearch.ipynb\n/examples/vector_stores/CassandraIndexDemo.ipynb\n/examples/vector_stores/Neo4jVectorDemo.ipynb\n/examples/vector_stores/CognitiveSearchIndexDemo.ipynb\n/examples/vector_stores/EpsillaIndexDemo.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e0caea2b-f168-4262-9b95-224c6b8996b2": {"__data__": {"id_": "e0caea2b-f168-4262-9b95-224c6b8996b2", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28292083a2785e839264533d778b316bba73207a", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\modules.md", "author": "LlamaIndex"}, "hash": "df54e564c422421d9050b5e4b6e9c0e65de54e5e2a29321a06998e925d1ccdec"}}, "hash": "7a2804805ff7d212e32db65ee720f036028f184b7447a906e2c7703a4f2f205b", "text": "# Modules\n\nWe support integrations with OpenAI, Azure, and anything LangChain offers.\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/embeddings/OpenAI.ipynb\n/examples/embeddings/Langchain.ipynb\n/examples/customization/llms/AzureOpenAI.ipynb\n/examples/embeddings/custom_embeddings.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c2b9f3c1-0183-4266-8ee0-e3b421a1e7f0": {"__data__": {"id_": "c2b9f3c1-0183-4266-8ee0-e3b421a1e7f0", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc2c971016bc70e8e1399bdcb84e7aee7dea5b84", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\root.md", "author": "LlamaIndex"}, "hash": "3079d216f76be1c3f43159731b21b51c63224a77965af9389cd1009a5588bd2f"}}, "hash": "6cf44e6e934cfe00c52a3cef8fe485e30c578ebd2a64542dc56b4e57ef208126", "text": "# Embeddings\n\n## Concept\nEmbeddings are used in LlamaIndex to represent your documents using a sophisticated numerical representation. Embedding models take text as input, and return a long list of numbers used to capture the semantics of the text. These embedding models have been trained to represent text this way, and help enable many applications, including search!\n\nAt a high level, if a user asks a question about dogs, then the embedding for that question will be highly similar to text that talks about dogs.\n\nWhen calculating the similarity between embeddings, there are many methods to use (dot product, cosine similarity, etc.). By default, LlamaIndex uses cosine similarity when comparing embeddings.\n\nThere are many embedding models to pick from. By default, LlamaIndex uses `text-embedding-ada-002` from OpenAI. We also support any embedding model offered by Langchain [here](https://python.langchain.com/docs/modules/data_connection/text_embedding/), as well as providing an easy to extend base class for implementing your own embeddings.\n\n## Usage Pattern\n\nMost commonly in LlamaIndex, embedding models will be specified in the `ServiceContext` object, and then used in a vector index. The embedding model will be used to embed the documents used during index construction, as well as embedding any queries you make using the query engine later on.\n\n```python\nfrom llama_index import ServiceContext\nfrom llama_index.embeddings import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding()\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n```\n\nTo save costs, you may want to use a local model.\n```python\nfrom llama_index import ServiceContext\nservice_context = ServiceContext.from_defaults(embed_model=\"local\")\n```\nThis will use a well-performing and fast default from Hugging Face.\n\nYou can find more usage details and available customization options below.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nusage_pattern.md\n```\n\n## Modules\n\nWe support integrations with OpenAI, Azure, and anything LangChain offers. Details below.\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7d0f937c-a29c-4a7c-9ef3-682d8123adab": {"__data__": {"id_": "7d0f937c-a29c-4a7c-9ef3-682d8123adab", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae8128392f38a8f48feacb5048750e968617c647", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "cf6aeb04672ea9964eb7f5fc1f9be311eb5827c48198c2f469001218c715c3f2"}, "3": {"node_id": "38868d0e-6a25-4e21-a37c-139717704171", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "b35ee73e6ffaa3296908288636fd8d6acd5303619c9e7d93e60301b64cc327c1"}}, "hash": "a61af16a5b233eadc7a4c9cd1761a75fb24a115084fc064d4e0a33bf204bac8c", "text": "# Usage Pattern\n\n## Getting Started\n\nThe most common usage for an embedding model will be setting it in the service context object, and then using it to construct an index and query.The input documents will be broken into nodes, and the emedding model will generate an embedding for each node.By default, LlamaIndex will use `text-embedding-ada-002`, which is what the example below manually sets up for you.```python\nfrom llama_index import ServiceContext, VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding()\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n\n# optionally set a global service context to avoid passing it into other objects every time\nfrom llama_index import set_global_service_context\nset_global_service_context(service_context)\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nThen, at query time, the embedding model will be used again to embed the query text.```python\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"query string\")\n```\n\n## Customization\n\n### Batch Size\n\nBy default, embeddings requests are sent to OpenAI in batches of 10.For some users, this may (rarely) incur a rate limit.For other users embedding many documents, this batch size may be too small.```python\n# set the batch size to 42\nembed_model = OpenAIEmbedding(embed_batch_size=42)\n```\n\n(local-embedding-models)=\n\n### Local Embedding Models\n\nThe easiest way to use a local model is:\n\n```python\nfrom llama_index import ServiceContext\nservice_context = ServiceContext.from_defaults(embed_model=\"local\")\n```\n\nTo configure the model used (from Hugging Face hub), add the model name separated by a colon:\n\n```python\nfrom llama_index import ServiceContext\n\nservice_context = ServiceContext.from_defaults(\n  embed_model=\"local:BAAI/bge-large-en\"\n)\n```\n\n### Embedding Model Integrations\n\nWe also support any embeddings offered by Langchain [here](https://python.langchain.com/docs/modules/data_connection/text_embedding/).The example below loads a model from Hugging Face, using Langchain's embedding class.```python\nfrom langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\nfrom llama_index import ServiceContext\n\nembed_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en\")\n\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n```\n\n### Custom Embedding Model\n\nIf you wanted to use embeddings not offered by LlamaIndex or Langchain, you can also extend our base embeddings class and implement your own!The example below uses Instructor Embeddings ([install/setup details here](https://huggingface.co/hkunlp/instructor-large)), and implements a custom embeddings class.Instructor embeddings work by providing text, as well as \"instructions\" on the domain of the text to embed.This is helpful when embedding text from a very specific and specialized topic.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "38868d0e-6a25-4e21-a37c-139717704171": {"__data__": {"id_": "38868d0e-6a25-4e21-a37c-139717704171", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae8128392f38a8f48feacb5048750e968617c647", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "cf6aeb04672ea9964eb7f5fc1f9be311eb5827c48198c2f469001218c715c3f2"}, "2": {"node_id": "7d0f937c-a29c-4a7c-9ef3-682d8123adab", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "a61af16a5b233eadc7a4c9cd1761a75fb24a115084fc064d4e0a33bf204bac8c"}}, "hash": "b35ee73e6ffaa3296908288636fd8d6acd5303619c9e7d93e60301b64cc327c1", "text": "```python\nfrom typing import Any, List\nfrom InstructorEmbedding import INSTRUCTOR\nfrom llama_index.embeddings.base import BaseEmbedding\n\nclass InstructorEmbeddings(BaseEmbedding):\n  def __init__(\n    self, \n    instructor_model_name: str = \"hkunlp/instructor-large\",\n    instruction: str = \"Represent the Computer Science documentation or question:\",\n    **kwargs: Any,\n  ) -> None:\n    self._model = INSTRUCTOR(instructor_model_name)\n    self._instruction = instruction\n    super().__init__(**kwargs)\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n      embeddings = self._model.encode([[self._instruction, query]])\n      return embeddings[0]\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n      embeddings = self._model.encode([[self._instruction, text]])\n      return embeddings[0] \n\n    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n      embeddings = self._model.encode([[self._instruction, text] for text in texts])\n      return embeddings\n```\n\n## Standalone Usage\n\nYou can also use embeddings as a standalone module for your project, existing application, or general testing and exploration.```python\nembeddings = embed_model.get_text_embedding(\"It is raining cats and dogs here!\")```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a9858431-5d39-4132-8408-d13cfcd97201": {"__data__": {"id_": "a9858431-5d39-4132-8408-d13cfcd97201", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60006b15efd1cdbb60bc4a787f1cb859931fb1b0", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\modules.md", "author": "LlamaIndex"}, "hash": "81ac2a1d46a4ae4d4a13f820ccfa60a0fa5a04220f8b0e9c68a82cbe02173d73"}}, "hash": "ed4716a2ac2d98378083356f1cf96a759d0948ff59e9e2f63dc2b9c8452c7176", "text": "# Modules\n\nWe support integrations with OpenAI, Anthropic, Hugging Face, PaLM, and more.\n\n## OpenAI\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/openai.ipynb\n/examples/llm/azure_openai.ipynb\n\n```\n\n## Anthropic\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/anthropic.ipynb\n\n```\n\n## Hugging Face\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb\n/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb\n/examples/vector_stores/SimpleIndexDemoLlama-Local.ipynb\n\n```\n\n\n## PaLM\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/palm.ipynb\n\n```\n\n## Predibase\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/predibase.ipynb\n\n```\n\n\n## Replicate\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/llama_2.ipynb\n/examples/llm/vicuna.ipynb\n/examples/vector_stores/SimpleIndexDemoLlama2.ipynb\n```\n\n## LangChain\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/langchain.ipynb\n```\n\n## Llama API\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/llama_api.ipynb\n```\n\n## Llama CPP\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/llama_2_llama_cpp.ipynb\n```\n\n## Xorbits Inference\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/XinferenceLocalDeployment.ipynb\n```\n\n## MonsterAPI\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/monsterapi.ipynb\n```\n\n## RunGPT\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/rungpt.ipynb\n```\n\n## Portkey\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/portkey.ipynb\n```\n\n## AnyScale\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/llm/anyscale.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "985acb4b-717c-4e17-b03f-4cad246da1c7": {"__data__": {"id_": "985acb4b-717c-4e17-b03f-4cad246da1c7", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cacbec8e223592c7809ce4ab3c2bbaac75ad1c4b", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\root.md", "author": "LlamaIndex"}, "hash": "2909c74288d396228df041f44b09cfd2b382eea724ff0523d8820c19d2df38b5"}}, "hash": "8c5d6b104512a2e8b22c67e7d334f6af126294c967036242dac1f4046b6a86fc", "text": "# LLM\n\n## Concept\nPicking the proper Large Language Model (LLM) is one of the first steps you need to consider when building any LLM application over your data.\n\nLLMs are a core component of LlamaIndex. They can be used as standalone modules or plugged into other core LlamaIndex modules (indices, retrievers, query engines). They are always used during the response synthesis step (e.g. after retrieval). Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n\nLlamaIndex provides a unified interface for defining LLM modules, whether it's from OpenAI, Hugging Face, or LangChain, so that you \ndon't have to write the boilerplate code of defining the LLM interface yourself. This interface consists of the following (more details below):\n- Support for **text completion** and **chat** endpoints (details below)\n- Support for **streaming** and **non-streaming** endpoints\n- Support for **synchronous** and **asynchronous** endpoints\n\n\n## Usage Pattern\n\nThe following code snippet shows how you can get started using LLMs.\n\n```python\nfrom llama_index.llms import OpenAI\n\n# non-streaming\nresp = OpenAI().complete('Paul Graham is ')\nprint(resp)\n```\n\n```{toctree}\n---\nmaxdepth: 1\n---\nusage_standalone.md\nusage_custom.md\n```\n\n\n## Modules\n\nWe support integrations with OpenAI, Hugging Face, PaLM, and more.\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a612f8cc-967f-4289-bd5c-b1fd178bcd40": {"__data__": {"id_": "a612f8cc-967f-4289-bd5c-b1fd178bcd40", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b905380f49f65854b7375f1eba5576f6c82785a", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "b5f7ff6ecffb37f5be1c8d5277fcdd27705b2051898591c6be7c1c12fd196408"}, "3": {"node_id": "28c994a0-580e-4746-a12e-d5ac10b1f26f", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "63bd734407e4feb612a33a6b27a5e7e11dc2306dfa6c7074a2d0244202de71ff"}}, "hash": "e51eb91b538755e438e62eddac79408e2fdf873cabdda771cb5e66f775ce9708", "text": "# Customizing LLMs within LlamaIndex Abstractions\n\nYou can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.\n\nBy default, we use OpenAI's `text-davinci-003` model. But you may choose to customize\nthe underlying LLM being used.\n\nBelow we show a few examples of LLM customization. This includes\n\n- changing the underlying LLM\n- changing the number of output tokens (for OpenAI, Cohere, or AI21)\n- having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\n\n## Example: Changing the underlying LLM\n\nAn example snippet of customizing the LLM being used is shown below.\nIn this example, we use `gpt-4` instead of `text-davinci-003`. Available models include `gpt-3.5-turbo`, `gpt-3.5-turbo-16k`, `gpt-4`, `gpt-4-32k`, `text-davinci-003`, and `text-davinci-002`. \n\nNote that\nyou may also plug in any LLM shown on Langchain's\n[LLM](https://python.langchain.com/docs/integrations/llms/) page.\n\n```python\n\nfrom llama_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    LLMPredictor,\n    ServiceContext\n)\nfrom llama_index.llms import OpenAI\n# alternatively\n# from langchain.llms import ...\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\n# define LLM\nllm = OpenAI(temperature=0.1, model=\"gpt-4\")\nservice_context = ServiceContext.from_defaults(llm=llm)\n\n# build index\nindex = KeywordTableIndex.from_documents(documents, service_context=service_context)\n\n# get response from query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do after his time at Y Combinator?\")\n\n```\n\n## Example: Changing the number of output tokens (for OpenAI, Cohere, AI21)\n\nThe number of output tokens is usually set to some low number by default (for instance,\nwith OpenAI the default is 256).\n\nFor OpenAI, Cohere, AI21, you just need to set the `max_tokens` parameter\n(or maxTokens for AI21). We will handle text chunking/calculations under the hood.\n\n```python\n\nfrom llama_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama_index.llms import OpenAI\n\ndocuments = SimpleDirectoryReader('data').load_data()\n\n# define LLM\nllm = OpenAI(temperature=0, model=\"text-davinci-002\", max_tokens=512)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\n```\n\n## Example: Explicitly configure `context_window` and `num_output`\n\nIf you are using other LLM classes from langchain, you may need to explicitly configure the `context_window` and `num_output` via the `ServiceContext` since the information is not available by default.\n\n```python\n\nfrom llama_index import (\n    KeywordTableIndex,\n    SimpleDirectoryReader,\n    ServiceContext\n)\nfrom llama_index.llms import OpenAI\n# alternatively\n# from langchain.llms import ...\n\ndocuments = SimpleDirectoryReader('data').load_data()", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "28c994a0-580e-4746-a12e-d5ac10b1f26f": {"__data__": {"id_": "28c994a0-580e-4746-a12e-d5ac10b1f26f", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b905380f49f65854b7375f1eba5576f6c82785a", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "b5f7ff6ecffb37f5be1c8d5277fcdd27705b2051898591c6be7c1c12fd196408"}, "2": {"node_id": "a612f8cc-967f-4289-bd5c-b1fd178bcd40", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "e51eb91b538755e438e62eddac79408e2fdf873cabdda771cb5e66f775ce9708"}, "3": {"node_id": "caa6858d-0b2d-462a-9e7d-a6b5ddfdbf0c", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "36005b0dc69bc595f46e851e623ddbe0cb71a1603b407b8dbd6e7ce01e0a78d2"}}, "hash": "63bd734407e4feb612a33a6b27a5e7e11dc2306dfa6c7074a2d0244202de71ff", "text": "# set context window\ncontext_window = 4096\n# set number of output tokens\nnum_output = 256\n\n# define LLM\nllm = OpenAI(\n    temperature=0, \n    model=\"text-davinci-002\", \n    max_tokens=num_output,\n)\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm,\n    context_window=context_window,\n    num_output=num_output,\n)\n\n```\n\n## Example: Using a HuggingFace LLM\n\nLlamaIndex supports using LLMs from HuggingFace directly.Note that for a completely private experience, also setup a local embedding model as in [this example](embeddings.md#custom-embeddings).Many open-source models from HuggingFace require either some preamble before each prompt, which is a `system_prompt`.Additionally, queries themselves may need an additional wrapper around the `query_str` itself.All this information is usually available from the HuggingFace model card for the model you are using.Below, this example uses both the `system_prompt` and `query_wrapper_prompt`, using specific prompts from the model card found [here](https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b).```python\nfrom llama_index.prompts import PromptTemplate\n\nsystem_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.- StableLM will refuse to participate in anything that could harm a human.\n\"\"\"# This will wrap the default prompts that are internal to llama-index\nquery_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n\nimport torch\nfrom llama_index.llms import HuggingFaceLLM\nllm = HuggingFaceLLM(\n    context_window=4096, \n    max_new_tokens=256,\n    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n    system_prompt=system_prompt,\n    query_wrapper_prompt=query_wrapper_prompt,\n    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n    device_map=\"auto\",\n    stopping_ids=[50278, 50279, 50277, 1, 0],\n    tokenizer_kwargs={\"max_length\": 4096},\n    # uncomment this if using CUDA to reduce memory usage\n    # model_kwargs={\"torch_dtype\": torch.float16}\n)\nservice_context = ServiceContext.from_defaults(\n    chunk_size=1024, \n    llm=llm,\n)\n```\n\nSome models will raise errors if all the keys from the tokenizer are passed to the model.A common tokenizer output that causes issues is `token_type_ids`.Below is an example of configuring the predictor to remove this before passing the inputs to the model:\n\n```python\nHuggingFaceLLM(\n    ...\n    tokenizer_outputs_to_remove=[\"token_type_ids\"]\n)\n```\n\nA full API reference can be found [here](../../../api_reference/llms/huggingface.rst).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "caa6858d-0b2d-462a-9e7d-a6b5ddfdbf0c": {"__data__": {"id_": "caa6858d-0b2d-462a-9e7d-a6b5ddfdbf0c", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b905380f49f65854b7375f1eba5576f6c82785a", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "b5f7ff6ecffb37f5be1c8d5277fcdd27705b2051898591c6be7c1c12fd196408"}, "2": {"node_id": "28c994a0-580e-4746-a12e-d5ac10b1f26f", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "63bd734407e4feb612a33a6b27a5e7e11dc2306dfa6c7074a2d0244202de71ff"}, "3": {"node_id": "5a2d46f6-4ddf-48ce-b498-1b0d24cc2cb2", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "54dd80858c2fdbab27029223e55fc3a09811a416b18b6b548d365d09a8f0082f"}}, "hash": "36005b0dc69bc595f46e851e623ddbe0cb71a1603b407b8dbd6e7ce01e0a78d2", "text": "Several example notebooks are also listed below:\n\n- [StableLM](/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb)\n- [Camel](/examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb)\n\n## Example: Using a Custom LLM Model - Advanced\n\nTo use a custom LLM model, you only need to implement the `LLM` class (or `CustomLLM` for a simpler interface)\nYou will be responsible for passing the text to the model and returning the newly generated tokens.Note that for a completely private experience, also setup a local embedding model (example [here](embeddings.md#custom-embeddings)).Here is a small example using locally running facebook/OPT model and Huggingface's pipeline abstraction:\n\n```python\nimport torch\nfrom transformers import pipeline\nfrom typing import Optional, List, Mapping, Any\n\nfrom llama_index import (\n    ServiceContext, \n    SimpleDirectoryReader, \n    LangchainEmbedding, \n    SummaryIndex\n)\nfrom llama_index.callbacks import CallbackManager\nfrom llama_index.llms import (\n    CustomLLM, \n    CompletionResponse, \n    CompletionResponseGen,\n    LLMMetadata,\n)\nfrom llama_index.llms.base import llm_completion_callback", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5a2d46f6-4ddf-48ce-b498-1b0d24cc2cb2": {"__data__": {"id_": "5a2d46f6-4ddf-48ce-b498-1b0d24cc2cb2", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b905380f49f65854b7375f1eba5576f6c82785a", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "b5f7ff6ecffb37f5be1c8d5277fcdd27705b2051898591c6be7c1c12fd196408"}, "2": {"node_id": "caa6858d-0b2d-462a-9e7d-a6b5ddfdbf0c", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}, "hash": "36005b0dc69bc595f46e851e623ddbe0cb71a1603b407b8dbd6e7ce01e0a78d2"}}, "hash": "54dd80858c2fdbab27029223e55fc3a09811a416b18b6b548d365d09a8f0082f", "text": "# set context window size\ncontext_window = 2048\n# set number of output tokens\nnum_output = 256\n\n# store the pipeline/model outisde of the LLM class to avoid memory issues\nmodel_name = \"facebook/opt-iml-max-30b\"\npipeline = pipeline(\"text-generation\", model=model_name, device=\"cuda:0\", model_kwargs={\"torch_dtype\":torch.bfloat16})\n\nclass OurLLM(CustomLLM):\n\n    @property\n    def metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        return LLMMetadata(\n            context_window=context_window,\n            num_output=num_output,\n            model_name=model_name\n        )\n\n    @llm_completion_callback()\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        prompt_length = len(prompt)\n        response = pipeline(prompt, max_new_tokens=num_output)[0][\"generated_text\"]\n\n        # only return newly generated tokens\n        text = response[prompt_length:]\n        return CompletionResponse(text=text)\n    \n    @llm_completion_callback()\n    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n        raise NotImplementedError()\n\n# define our LLM\nllm = OurLLM()\n\nservice_context = ServiceContext.from_defaults(\n    llm=llm, \n    context_window=context_window, \n    num_output=num_output\n)\n\n# Load the your data\ndocuments = SimpleDirectoryReader('./data').load_data()\nindex = SummaryIndex.from_documents(documents, service_context=service_context)\n\n# Query and print response\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"<query_text>\")\nprint(response)\n```\n\nUsing this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\n\nThe decorator is optional, but provides observability via callbacks on the LLM calls.\n\nNote that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it's capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\n\nA list of all default internal prompts is available [here](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompts.py), and chat-specific prompts are listed [here](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/chat_prompts.py). You can also implement your own custom prompts, as described [here](/core_modules/service_modules/prompts.md).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bccebf27-b755-4da4-9864-3e2098e6da3e": {"__data__": {"id_": "bccebf27-b755-4da4-9864-3e2098e6da3e", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_standalone.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3beb398c11263f63cc6e311727ca48d9a658e036", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_standalone.md", "author": "LlamaIndex"}, "hash": "72c98ef70dbfdd2f72cc0027a014c9d355ad1ec70eb2d5f8f2e1de3fe1279744"}}, "hash": "6eecf62895fd5634a121884fd10e106a61c1bec4e8dd2ab9e4d451c08d08f77d", "text": "# Using LLMs as standalone modules\n\nYou can use our LLM modules on their own.\n\n## Text Completion Example\n\n```python\nfrom llama_index.llms import OpenAI\n\n# non-streaming\nresp = OpenAI().complete('Paul Graham is ')\nprint(resp)\n\n# using streaming endpoint\nfrom llama_index.llms import OpenAI\nllm = OpenAI()\nresp = llm.stream_complete('Paul Graham is ')\nfor delta in resp:\n    print(delta, end='')\n```\n\n## Chat Example\n\n```python\nfrom llama_index.llms import ChatMessage, OpenAI\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a pirate with a colorful personality\"),\n    ChatMessage(role=\"user\", content=\"What is your name\"),\n]\nresp = OpenAI().chat(messages)\nprint(resp)\n```\n\nCheck out our [modules section](modules.md) for usage guides for each LLM.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ac8b9270-ffc5-4520-b9d2-5b2834a8b691": {"__data__": {"id_": "ac8b9270-ffc5-4520-b9d2-5b2834a8b691", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\prompts.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d078d2a781feb5172a10b102a6b0740cc9dd121", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\prompts.md", "author": "LlamaIndex"}, "hash": "3b837f39e9167f98af17475ba7f2843e27dcc8c244580daf180b9cf27d2be820"}, "3": {"node_id": "05d8bf6c-6183-4751-8a8a-1ae83a70a652", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\prompts.md", "author": "LlamaIndex"}, "hash": "5de57d308268658a1385fac170764434a652febd6baea8e5174b7992e79d94ad"}}, "hash": "43699fe770c7efe131be1c9a1b7cc16f2f5768b7d5ae0cc7b2c8bc52cbf6d770", "text": "# Prompts\n\n## Concept\n\nPrompting is the fundamental input that gives LLMs their expressive power.LlamaIndex uses prompts to build the index, do insertion, \nperform traversal during querying, and to synthesize the final answer.LlamaIndex uses a set of [default prompt templates](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompts.py) that work well out of the box.In addition, there are some prompts written and used specifically for chat models like `gpt-3.5-turbo` [here](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/chat_prompts.py).Users may also provide their own prompt templates to further customize the behavior of the framework.The best method for customizing is copying the default prompt from the link above, and using that as the base for any modifications.## Usage Pattern\n\n### Defining a custom prompt\n\nDefining a custom prompt is as simple as creating a format string\n\n```python\nfrom llama_index.prompts import PromptTemplate\n\ntemplate = (\n    \"We have provided context information below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\"\n    \"\\n---------------------\\n\"\n    \"Given this information, please answer the question: {query_str}\\n\"\n)\nqa_template = PromptTemplate(template)\n\n# you can create text prompt (for completion API) \nprompt = qa_template.format(context_str=..., query_str=...)\n\n# or easily convert to message prompts (for chat API)\nmessages = qa_template.format_messages(context_str=..., query_str=...)\n```\n\n> Note: you may see references to legacy prompt subclasses such as `QuestionAnswerPrompt`, `RefinePrompt`.These have been deprecated (and now are type aliases of `PromptTemplate`).Now you can directly specify `PromptTemplate(template)` to construct custom prompts.But you still have to make sure the template string contains the expected parameters (e.g.`{context_str}` and `{query_str}`) when replacing a default question answer prompt.You can also define a template from chat messages \n```python\nfrom llama_index.prompts import ChatPromptTemplate, ChatMessage, MessageRole\n\nmessage_templates = [\n    ChatMessage(content=\"You are an expert system.\", role=MessageRole.SYSTEM),\n    ChatMessage(\n        content=\"Generate a short story about {topic}\",\n        role=MessageRole.USER,\n    ),\n]\nchat_template = ChatPromptTemplate(message_templates=message_templates)\n\n# you can create message prompts (for chat API)\nmessages = chat_template.format_messages(topic=...)\n\n# or easily convert to text prompt (for completion API)\nprompt = chat_template.format(topic=...)\n```\n\n### Passing custom prompts into the pipeline\n\nSince LlamaIndex is a multi-step pipeline, it's important to identify the operation that you want to modify and pass in the custom prompt at the right place.At a high-level, prompts are used in 1) index construction, and 2) query engine execution\n\nThe most commonly used prompts will be the `text_qa_template` and the `refine_template`.- `text_qa_template` - used to get an initial answer to a query using retrieved nodes\n- `refine_tempalate` - used when the retrieved text does not fit into a single LLM call with `response_mode=\"compact\"` (the default), or when more than one node is retrieved using `response_mode=\"refine\"`.The answer from the first query is inserted as an `existing_answer`, and the LLM must update or repeat the existing answer based on the new context.#### Modify prompts used in index construction\nDifferent indices use different types of prompts during construction (some don't use prompts at all).For instance, `TreeIndex` uses a summary prompt to hierarchically\nsummarize the nodes, and `KeywordTableIndex` uses a keyword extract prompt to extract keywords.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "05d8bf6c-6183-4751-8a8a-1ae83a70a652": {"__data__": {"id_": "05d8bf6c-6183-4751-8a8a-1ae83a70a652", "embedding": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\prompts.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d078d2a781feb5172a10b102a6b0740cc9dd121", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\prompts.md", "author": "LlamaIndex"}, "hash": "3b837f39e9167f98af17475ba7f2843e27dcc8c244580daf180b9cf27d2be820"}, "2": {"node_id": "ac8b9270-ffc5-4520-b9d2-5b2834a8b691", "node_type": null, "metadata": {"filename": "docs\\core_modules\\model_modules\\prompts.md", "author": "LlamaIndex"}, "hash": "43699fe770c7efe131be1c9a1b7cc16f2f5768b7d5ae0cc7b2c8bc52cbf6d770"}}, "hash": "5de57d308268658a1385fac170764434a652febd6baea8e5174b7992e79d94ad", "text": "There are two equivalent ways to override the prompts:\n\n1. via the default nodes constructor \n\n```python\nindex = TreeIndex(nodes, summary_template=<custom_prompt>)\n```\n2. via the documents constructor.```python\nindex = TreeIndex.from_documents(docs, summary_template=<custom_prompt>)\n```\n\nFor more details on which index uses which prompts, please visit\n[Index class references](/api_reference/indices.rst).#### Modify prompts used in query engine\nMore commonly, prompts are used at query-time (i.e.for executing a query against an index and synthesizing the final response).There are also two equivalent ways to override the prompts:\n\n1. via the high-level API\n```python\nquery_engine = index.as_query_engine(\n    text_qa_template=<custom_qa_prompt>,\n    refine_template=<custom_refine_prompt>\n)\n```\n2. via the low-level composition API\n\n```python\nretriever = index.as_retriever()\nsynth = get_response_synthesizer(\n    text_qa_template=<custom_qa_prompt>,\n    refine_template=<custom_refine_prompt>\n)\nquery_engine = RetrieverQueryEngine(retriever, response_synthesizer)\n```\n\nThe two approaches above are equivalent, where 1 is essentially syntactic sugar for 2 and hides away the underlying complexity.You might want to use 1 to quickly modify some common parameters, and use 2 to have more granular control.\n\n\nFor more details on which classes use which prompts, please visit\n[Query class references](/api_reference/query.rst).\n\nCheck out the [reference documentation](/api_reference/prompts.rst) for a full set of all prompts.\n\n## Modules\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/customization/prompts/completion_prompts.ipynb\n/examples/customization/prompts/chat_prompts.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dccd2d0b-197d-4871-9aaf-f42ea6d4c119": {"__data__": {"id_": "dccd2d0b-197d-4871-9aaf-f42ea6d4c119", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea0eab0ae33fbaf69ee1c99f6b7cc09ae090a277", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\modules.md", "author": "LlamaIndex"}, "hash": "c17d2036e3ce3450f10157911375eeebfea1f875e9f583376b0d65c76122408a"}}, "hash": "05132679a8476ec526232a27f888f40be9880ae0b572f0ee1a31949c1d4e163f", "text": "# Module Guides\n\nWe provide a few simple implementations to start, with more sophisticated modes coming soon!  \n\nMore specifically, the `SimpleChatEngine` does not make use of a knowledge base, \nwhereas all others make use of a query engine over knowledge base.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nReAct Chat Engine </examples/chat_engine/chat_engine_react.ipynb>\nOpenAI Chat Engine </examples/chat_engine/chat_engine_openai.ipynb>\nContext Chat Engine </examples/chat_engine/chat_engine_context.ipynb>\nCondense Question Chat Engine </examples/chat_engine/chat_engine_condense_question.ipynb>\nSimple Chat Engine </examples/chat_engine/chat_engine_repl.ipynb>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "40f8c65b-652a-434b-83d5-d413348c015e": {"__data__": {"id_": "40f8c65b-652a-434b-83d5-d413348c015e", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26371396dff491ae107ecac5d03730c7ddcf78b5", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\root.md", "author": "LlamaIndex"}, "hash": "14d0e9ca16afe5ab47238fff62a01d2b7da1f33757c3a60c8ddf0c079e4f4982"}}, "hash": "b405cdde2e788c3383d9e324d5e1d271ccb117482da6b7bcd6312f2469a2af26", "text": "# Chat Engine\n\n## Concept\nChat engine is a high-level interface for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).\nThink ChatGPT, but augmented with your knowledge base.  \n\nConceptually, it is a **stateful** analogy of a [Query Engine](../query_engine/root.md). \nBy keeping track of the conversation history, it can answer questions with past context in mind.  \n\n\n```{tip}\nIf you want to ask standalone question over your data (i.e. without keeping track of conversation history), use [Query Engine](../query_engine/root.md) instead.  \n```\n\n## Usage Pattern\nGet started with:\n```python\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat(\"Tell me a joke.\")\n```\n\nTo stream response:\n```python\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")\nfor token in streaming_response.response_gen:\n    print(token, end=\"\")\n```\n\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\n\n## Modules\nBelow you can find corresponding tutorials to see the available chat engines in action. \n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "61aeaf9c-7e1b-4638-9dbb-aa23f87aefb4": {"__data__": {"id_": "61aeaf9c-7e1b-4638-9dbb-aa23f87aefb4", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "883a57fc7e0241f919fb6e3213d1ba4571f61fe3", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "88e9202d844611493d09bf17dec741e6e7a32176036d35dfb5ca9e6bbd258af9"}, "3": {"node_id": "10dfb162-591a-4c8a-af2b-ad51ac5635a4", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "47bf355207cbabd0b10d40f16c176c9f559c72c7ebbffe711b8e2983dd6e2740"}}, "hash": "520b182b37b2c1b8eb619a2d295e2956503efde0704f67c9eeae420bcbd2e0db", "text": "# Usage Pattern\n\n## Get Started\n\nBuild a chat engine from index:\n```python\nchat_engine = index.as_chat_engine()\n```\n\n```{tip}\nTo learn how to build an index, see [Index](/core_modules/data_modules/index/root.md)\n```\n\nHave a conversation with your data:\n```python\nresponse = chat_engine.chat(\"Tell me a joke.\")\n```\n\nReset chat history to start a new conversation:\n```python\nchat_engine.reset()\n```\n\nEnter an interactive chat REPL:\n```python\nchat_engine.chat_repl()\n```\n\n\n## Configuring a Chat Engine\nConfiguring a chat engine is very similar to configuring a query engine.### High-Level API\nYou can directly build and configure a chat engine from an index in 1 line of code:\n```python\nchat_engine = index.as_chat_engine(\n    chat_mode='condense_question', \n    verbose=True\n)\n```\n> Note: you can access different chat engines by specifying the `chat_mode` as a kwarg.`condense_question` corresponds to `CondenseQuestionChatEngine`, `react` corresponds to `ReActChatEngine`, `context` corresponds to a `ContextChatEngine`.> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.#### Available Chat Modes\n\n- `best` - Turn the query engine into a tool, for use with a `ReAct` data agent or an `OpenAI` data agent, depending on what your LLM supports.`OpenAI` data agents require `gpt-3.5-turbo` or `gpt-4` as they use the function calling API from OpenAI.- `context` - Retrieve nodes from the index using every user message.The retrieved text is inserted into the system prompt, so that the chat engine can either respond naturally or use the context from the query engine.- `condense_question` - Look at the chat history and re-write the user message to be a query for the index.Return the response after reading the response from the query engine.- `simple` - A simple chat with the LLM directly, no query engine involved.- `react` - Same as `best`, but forces a `ReAct` data agent.- `openai` - Same as `best`, but forces an `OpenAI` data agent.### Low-Level Composition API\n\nYou can use the low-level composition API if you need more granular control.Concretely speaking, you would explicitly construct `ChatEngine` object instead of calling `index.as_chat_engine(...)`.> Note: You may need to look at API references or example notebooks.Here's an example where we configure the following:\n* configure the condense question prompt, \n* initialize the conversation with some existing history,\n* print verbose debug message.```python\nfrom llama_index.prompts  import PromptTemplate\nfrom llama_index.llms import ChatMessage, MessageRole\n\ncustom_prompt = PromptTemplate(\"\"\"\\\nGiven a conversation (between Human and Assistant) and a follow up message from Human, \\\nrewrite the message to be a standalone question that captures all relevant context \\\nfrom the conversation.<Chat History> \n{chat_history}\n\n<Follow Up Message>\n{question}\n\n<Standalone question>\n\"\"\")\n\n# list of `ChatMessage` objects\ncustom_chat_history = [\n    ChatMessage(\n        role=MessageRole.USER, \n        content='Hello assistant, we are having a insightful discussion about Paul Graham today.'), \n    ChatMessage(\n        role=MessageRole.ASSISTANT, \n        content='Okay, sounds good.'", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "10dfb162-591a-4c8a-af2b-ad51ac5635a4": {"__data__": {"id_": "10dfb162-591a-4c8a-af2b-ad51ac5635a4", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "883a57fc7e0241f919fb6e3213d1ba4571f61fe3", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "88e9202d844611493d09bf17dec741e6e7a32176036d35dfb5ca9e6bbd258af9"}, "2": {"node_id": "61aeaf9c-7e1b-4638-9dbb-aa23f87aefb4", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "520b182b37b2c1b8eb619a2d295e2956503efde0704f67c9eeae420bcbd2e0db"}}, "hash": "47bf355207cbabd0b10d40f16c176c9f559c72c7ebbffe711b8e2983dd6e2740", "text": ")\n]\n\nquery_engine = index.as_query_engine()\nchat_engine = CondenseQuestionChatEngine.from_defaults(\n    query_engine=query_engine, \n    condense_question_prompt=custom_prompt,\n    chat_history=custom_chat_history,\n    verbose=True\n)\n```\n\n### Streaming\nTo enable streaming, you simply need to call the `stream_chat` endpoint instead of the `chat` endpoint.```{warning}\nThis somewhat inconsistent with query engine (where you pass in a `streaming=True` flag).We are working on making the behavior more consistent!```\n\n```python\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")for token in streaming_response.response_gen:\n    print(token, end=\"\")\n```\n\nSee an [end-to-end tutorial](/examples/customization/streaming/chat_engine_condense_question_stream_response.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f9b0a12b-11c9-4a58-ae7f-31b9e2a8c3a9": {"__data__": {"id_": "f9b0a12b-11c9-4a58-ae7f-31b9e2a8c3a9", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc633e6e45e519b3fb15f4ac3c40e799d3ae70a2", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "hash": "2a354ed7c62663ac61ca40ebbac856b57a66446882f6e9253867703a599aaacc"}, "3": {"node_id": "330c65bc-2b0e-4d90-89f0-e8c59f1a0b2e", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "hash": "b8cd25d0b990064576c8506a936a379be48892c4a56f70fb0fb6d87aa8622cd0"}}, "hash": "23c6e9d31205941dc11b13b40b9a1c074863d0054037adbf7cfcfd3e5b9c6edf", "text": "# Modules\n\n## SimilarityPostprocessor\n\nUsed to remove nodes that are below a similarity score threshold.```python\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\npostprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n## KeywordNodePostprocessor\n\nUsed to ensure certain keywords are either excluded or included.```python\nfrom llama_index.indices.postprocessor import KeywordNodePostprocessor\n\npostprocessor = KeywordNodePostprocessor(\n  required_keywords=[\"word1\", \"word2\"],\n  exclude_keywords=[\"word3\", \"word4\"]\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n## MetadataReplacementPostProcessor\n\nUsed to replace the node content with a field from the node metadata.If the field is not present in the metadata, then the node text remains unchanged.Most useful when used in combination with the `SentenceWindowNodeParser`.```python\nfrom llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n\npostprocessor = MetadataReplacementPostProcessor(\n  target_metadata_key=\"window\",\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n## SentenceEmbeddingOptimizer\n\nThis postprocessor optimizes token usage by removing sentences that are not relevant to the query (this is done using embeddings).The percentile cutoff is a measure for using the top percentage of relevant sentences.The threshold cutoff can be specified instead, which uses a raw similarity cutoff for picking which sentences to keep.```python\nfrom llama_index.indices.postprocessor import SentenceEmbeddingOptimizer\n\npostprocessor = SentenceEmbeddingOptimizer(\n  embed_model=service_context.embed_model,\n  percentile_cutoff=0.5,\n  # threshold_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full notebook guide can be found [here](/examples/node_postprocessor/OptimizerDemo.ipynb)\n\n## CohereRerank\n\nUses the \"Cohere ReRank\" functionality to re-order nodes, and returns the top N nodes.```python\nfrom llama_index.indices import CohereRerank\n\npostprocessor = CohereRerank(\n  top_n=2\n  model=\"rerank-english-v2.0\",\n  api_key=\"YOUR COHERE API KEY\"\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFull notebook guide is available [here](/examples/node_postprocessor/CohereRerank.ipynb).## SentenceTransformerRerank\n\nUses the cross-encoders from the `sentence-transformer` package to re-order nodes, and returns the top N nodes.```python\nfrom llama_index.indices.postprocessor import SentenceTransformerRerank\n\n# We choose a model with relatively high speed and decent accuracy.postprocessor = SentenceTransformerRerank(\n  model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n  top_n=3\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFull notebook guide is available [here](/examples/node_postprocessor/SentenceTransformerRerank.ipynb).Please also refer to the [`sentence-transformer` docs](https://www.sbert.net/docs/pretrained-models/ce-msmarco.html) for a more complete list of models (and also shows tradeoffs in speed/accuracy).The default model is `cross-encoder/ms-marco-TinyBERT-L-2-v2`, which provides the most speed.## LLM Rerank\n\nUses a LLM to re-order nodes by asking the LLM to return the relevant documents and a score of how relevant they are.Returns the top N ranked nodes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "330c65bc-2b0e-4d90-89f0-e8c59f1a0b2e": {"__data__": {"id_": "330c65bc-2b0e-4d90-89f0-e8c59f1a0b2e", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc633e6e45e519b3fb15f4ac3c40e799d3ae70a2", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "hash": "2a354ed7c62663ac61ca40ebbac856b57a66446882f6e9253867703a599aaacc"}, "2": {"node_id": "f9b0a12b-11c9-4a58-ae7f-31b9e2a8c3a9", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "hash": "23c6e9d31205941dc11b13b40b9a1c074863d0054037adbf7cfcfd3e5b9c6edf"}, "3": {"node_id": "030b0cd8-541b-47e7-befb-69aaeeb69fd0", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "hash": "22eb2ce505e49e6684b5d40b20accf9e76470d018b2755fdfeb3ad8560c3306f"}}, "hash": "b8cd25d0b990064576c8506a936a379be48892c4a56f70fb0fb6d87aa8622cd0", "text": "```python\nfrom llama_index.indices.postprocessor import LLMRerank\n\npostprocessor = LLMRerank(\n  top_n=2\n  service_context=service_context,\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFull notebook guide is available [her for Gatsby](/examples/node_postprocessor/LLMReranker-Gatsby.ipynb) and [here for Lyft 10K documents](/examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb).## FixedRecencyPostprocessor\n\nThis postproccesor returns the top K nodes sorted by date.This assumes there is a `date` field to parse in the metadata of each node.```python\nfrom llama_index.indices.postprocessor import FixedRecencyPostprocessor\n\npostprocessor = FixedRecencyPostprocessor(\n  tok_k=1,\n  date_key=\"date\"  # the key in the metadata to find the date\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n![](/_static/node_postprocessors/recency.png)\n\nA full notebook guide is available [here](/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb).## EmbeddingRecencyPostprocessor\n\nThis postproccesor returns the top K nodes after sorting by date and removing older nodes that are too similar after measuring embedding similarity.```python\nfrom llama_index.indices.postprocessor import EmbeddingRecencyPostprocessor\n\npostprocessor = EmbeddingRecencyPostprocessor(\n  service_context=service_context,\n  date_key=\"date\",\n  similarity_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full notebook guide is available [here](/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb).## TimeWeightedPostprocessor\n\nThis postproccesor returns the top K nodes applying a time-weighted rerank to each node.Each time a node is retrieved, the time it was retrieved is recorded.This biases search to favor information that has not be returned in a query yet.```python\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\npostprocessor = TimeWeightedPostprocessor(\n  time_decay=0.99,\n  top_k=1\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full notebook guide is available [here](/examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb).## (Beta) PIINodePostprocessor\n\nThe PII (Personal Identifiable Information) postprocssor removes information that might be a security risk.It does this by using NER (either with a dedicated NER model, or with a local LLM model).### LLM Version\n\n```python\nfrom llama_index.indices.postprocessor import PIINodePostprocessor\n\npostprocessor = PIINodePostprocessor(\n  service_context=service_context,  # this should be setup with an LLM you trust\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n### NER Version\n\nThis version uses the default local model from Hugging Face that is loaded when you run `pipline(\"ner\")`.```python\nfrom llama_index.indices.postprocessor import NERPIINodePostprocessor\n\npostprocessor = NERPIINodePostprocessor()\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full notebook guide for both can be found [here](/examples/node_postprocessor/PII.ipynb).## (Beta) PrevNextNodePostprocessor\n\nUses pre-defined settings to read the `Node` relationships and fetch either all nodes that come previously, next, or both.This is useful when you know the relationships point to important data (either before, after, or both) that should be sent to the LLM if that node is retrieved.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "030b0cd8-541b-47e7-befb-69aaeeb69fd0": {"__data__": {"id_": "030b0cd8-541b-47e7-befb-69aaeeb69fd0", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc633e6e45e519b3fb15f4ac3c40e799d3ae70a2", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "hash": "2a354ed7c62663ac61ca40ebbac856b57a66446882f6e9253867703a599aaacc"}, "2": {"node_id": "330c65bc-2b0e-4d90-89f0-e8c59f1a0b2e", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}, "hash": "b8cd25d0b990064576c8506a936a379be48892c4a56f70fb0fb6d87aa8622cd0"}}, "hash": "22eb2ce505e49e6684b5d40b20accf9e76470d018b2755fdfeb3ad8560c3306f", "text": "```python\nfrom llama_index.indices.postprocessor import PrevNextNodePostprocessor\n\npostprocessor = PrevNextNodePostprocessor(\n  docstore=index.docstore,\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards\n  mode=\"next\"   # can be either 'next', 'previous', or 'both'\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\n![](/_static/node_postprocessors/prev_next.png)\n\n## (Beta) AutoPrevNextNodePostprocessor\n\nThe same as PrevNextNodePostprocessor, but lets the LLM decide the mode (next, previous, or both).```python\nfrom llama_index.indices.postprocessor import AutoPrevNextNodePostprocessor\n\npostprocessor = AutoPrevNextNodePostprocessor(\n  docstore=index.docstore,\n  service_context=service_context\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nA full example notebook is available [here](/examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb).## All Notebooks\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/node_postprocessor/OptimizerDemo.ipynb\n/examples/node_postprocessor/CohereRerank.ipynb\n/examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb\n/examples/node_postprocessor/LLMReranker-Gatsby.ipynb\n/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb\n/examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb\n/examples/node_postprocessor/PII.ipynb\n/examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb\n/examples/node_postprocessor/MetadataReplacementDemo.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "76b26bd0-4bde-4854-b485-e097abae0a0f": {"__data__": {"id_": "76b26bd0-4bde-4854-b485-e097abae0a0f", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa77fb97241f1e45d8c60e1776ce303765269714", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\root.md", "author": "LlamaIndex"}, "hash": "457739e8a0248448e944117f6ab048136926b726bad7d5577038fd417ae27d1f"}}, "hash": "fbc018df149f0210f37158a5203664398d0207e93b17f7a361a54e624b3eca61", "text": "# Node Postprocessor\n\n## Concept\nNode postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them.\n\nIn LlamaIndex, node postprocessors are most commonly applied within a query engine, after the node retrieval step and before the response synthesis step.\n\nLlamaIndex offers several node postprocessors for immediate use, while also providing a simple API for adding your own custom postprocessors.\n\n```{tip}\nConfused about where node postprocessor fits in the pipeline? Read about [high-level concepts](/getting_started/concepts.md)\n```\n\n## Usage Pattern\n\nAn example of using a node postprocessors is below:\n\n```python\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\n# filter nodes below 0.75 similarity score\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n```\n\nYou can find more details using post processors and how to build your own below.\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\n## Modules\nBelow you can find guides for each node postprocessor.\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2551cd80-a535-4dec-8f2c-50095a3dfa39": {"__data__": {"id_": "2551cd80-a535-4dec-8f2c-50095a3dfa39", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecaaca47b8df50ccde682547956edfdda32b99f2", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "58490979f17689b3c207f3d79cf64d91c0a8618b0fffb96a4e01471c4ce12bf4"}}, "hash": "e22203ff70aca1f74b1f536a1cfd826f87555a6aabf49f04e9c76baaa46bfca0", "text": "# Usage Pattern\n\nMost commonly, node-postprocessors will be used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.\n\n\n## Using with a Query Engine\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(\n  node_postprocessors=[\n    TimeWeightedPostprocessor(\n        time_decay=0.5, time_access_refresh=False, top_k=1\n    )\n  ]\n)\n\n# all node post-processors will be applied during each query\nresponse = query_engine.query(\"query string\")\n```\n\n## Using with Retrieved Nodes\n\nOr used as a standalone object for filtering retrieved nodes:\n\n```python\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\nnodes = index.as_retriever().retrieve(\"test query str\")\n\n# filter nodes below 0.75 similarity score\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n```\n\n## Using with your own nodes\n\nAs you may have noticed, the postprocessors take `NodeWithScore` objects as inputs, which is just a wrapper class with a `Node` and a `score` value.\n\n```python\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\n# filter nodes below 0.75 similarity score\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n```\n\n## Custom Node PostProcessor\n\nThe base class is `BaseNodePostprocessor`, and the API interface is very simple: \n\n```python\nclass BaseNodePostprocessor:\n    \"\"\"Node postprocessor.\"\"\"\n\n    @abstractmethod\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n```\n\nA dummy node-postprocessor can be implemented in just a few lines of code:\n\n```python\nfrom llama_index import QueryBundle\nfrom llama_index.indices.postprocessor.base import BaseNodePostprocessor\nfrom llama_index.schema import NodeWithScore\n\nclass DummyNodePostprocessor:\n\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \n        # subtracts 1 from the score\n        for n in nodes:\n            n.score -= 1\n\n        return nodes\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d8920524-7a92-4572-9c2a-b8a3e37a3970": {"__data__": {"id_": "d8920524-7a92-4572-9c2a-b8a3e37a3970", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c818be1d3a7d8d18fe7cbbdbc93f5eaa92679eb", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md", "author": "LlamaIndex"}, "hash": "8ab4fef40f1b0853ddb25f48dcc79c74f28c9e1a4c5fc4014a47ab242a4a3e8a"}, "3": {"node_id": "2fd9c628-f1f2-4c0c-8f1f-5acc1000985b", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md", "author": "LlamaIndex"}, "hash": "ef0251e9e5acfedee5bf2cece1f7dbb16092f4818a747e841a009df4c3bada18"}}, "hash": "3eb3fb35df81796fa750b925d79e813c4c77216770a220934125d9ef4c6c4a74", "text": "# Query Transformations\n\n\nLlamaIndex allows you to perform *query transformations* over your index structures.\nQuery transformations are modules that will convert a query into another query. They can be **single-step**, as in the transformation is run once before the query is executed against an index. \n\nThey can also be **multi-step**, as in: \n1. The query is transformed, executed against an index, \n2. The response is retrieved.\n3. Subsequent queries are transformed/executed in a sequential fashion.\n\nWe list some of our query transformations in more detail below.\n\n#### Use Cases\nQuery transformations have multiple use cases:\n- Transforming an initial query into a form that can be more easily embedded (e.g. HyDE)\n- Transforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition)\n- Breaking an initial query into multiple subquestions that can be more easily answered on their own. (multi-step query decomposition)\n\n\n### HyDE (Hypothetical Document Embeddings)\n\n[HyDE](http://boston.lti.cs.cmu.edu/luyug/HyDE/HyDE.pdf) is a technique where given a natural language query, a hypothetical document/answer is generated first. This hypothetical document is then used for embedding lookup rather than the raw query.\n\nTo use HyDE, an example code snippet is shown below.\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.query.query_transform.base import HyDEQueryTransform\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\n\n# load documents, build index\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents)\n\n# run query with HyDE query transform\nquery_str = \"what did paul graham do after going to RISD\"\nhyde = HyDEQueryTransform(include_original=True)\nquery_engine = index.as_query_engine()\nquery_engine = TransformQueryEngine(query_engine, query_transform=hyde)\nresponse = query_engine.query(query_str)\nprint(response)\n\n```\n\nCheck out our [example notebook](../../../examples/query_transformations/HyDEQueryTransformDemo.ipynb) for a full walkthrough.\n\n\n### Single-Step Query Decomposition\n\nSome recent approaches (e.g. [self-ask](https://ofir.io/self-ask.pdf), [ReAct](https://arxiv.org/abs/2210.03629)) have suggested that LLM's \nperform better at answering complex questions when they break the question into smaller steps. We have found that this is true for queries that require knowledge augmentation as well.\n\nIf your query is complex, different parts of your knowledge base may answer different \"subqueries\" around the overall query.\n\nOur single-step query decomposition feature transforms a **complicated** question into a simpler one over the data collection to help provide a sub-answer to the original question.\n\nThis is especially helpful over a [composed graph](../../index/composability.md). Within a composed graph, a query can be routed to multiple subindexes, each representing a subset of the overall knowledge corpus. Query decomposition allows us to transform the query into a more suitable question over any given index.\n\nAn example image is shown below.\n\n![](/_static/query_transformations/single_step_diagram.png)\n\n\nHere's a corresponding example code snippet over a composed graph.\n\n```python\n\n# Setting: a summary index composed over multiple vector indices\n# llm_predictor_chatgpt corresponds to the ChatGPT LLM interface\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor_chatgpt, verbose=True\n)\n\n# initialize indexes and graph\n...", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2fd9c628-f1f2-4c0c-8f1f-5acc1000985b": {"__data__": {"id_": "2fd9c628-f1f2-4c0c-8f1f-5acc1000985b", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c818be1d3a7d8d18fe7cbbdbc93f5eaa92679eb", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md", "author": "LlamaIndex"}, "hash": "8ab4fef40f1b0853ddb25f48dcc79c74f28c9e1a4c5fc4014a47ab242a4a3e8a"}, "2": {"node_id": "d8920524-7a92-4572-9c2a-b8a3e37a3970", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md", "author": "LlamaIndex"}, "hash": "3eb3fb35df81796fa750b925d79e813c4c77216770a220934125d9ef4c6c4a74"}}, "hash": "ef0251e9e5acfedee5bf2cece1f7dbb16092f4818a747e841a009df4c3bada18", "text": "# configure retrievers\nvector_query_engine = vector_index.as_query_engine()\nvector_query_engine = TransformQueryEngine(\n    vector_query_engine, \n    query_transform=decompose_transform\n    transform_extra_info={'index_summary': vector_index.index_struct.summary}\n)\ncustom_query_engines = {\n    vector_index.index_id: vector_query_engine\n} \n\n# query\nquery_str = (\n    \"Compare and contrast the airports in Seattle, Houston, and Toronto. \"\n)\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\nresponse = query_engine.query(query_str)\n```\n\nCheck out our [example notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/City_Analysis-Decompose.ipynb) for a full walkthrough.\n\n\n\n### Multi-Step Query Transformations\n\nMulti-step query transformations are a generalization on top of existing single-step query transformation approaches.\n\nGiven an initial, complex query, the query is transformed and executed against an index. The response is retrieved from the query. \nGiven the response (along with prior responses) and the query, followup questions may be asked against the index as well. This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.\n\nAn example image is shown below.\n\n![](/_static/query_transformations/multi_step_diagram.png)\n\n\nHere's a corresponding example code snippet.\n\n```python\nfrom llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform\n# gpt-4\nstep_decompose_transform = StepDecomposeQueryTransform(\n    llm_predictor, verbose=True\n)\n\nquery_engine = index.as_query_engine()\nquery_engine = MultiStepQueryEngine(query_engine, query_transform=step_decompose_transform)\n\nresponse = query_engine.query(\n    \"Who was in the first batch of the accelerator program the author started?\",\n)\nprint(str(response))\n\n```\n\nCheck out our [example notebook](https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-multistep.ipynb) for a full walkthrough.\n\n\n```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n/examples/query_transformations/HyDEQueryTransformDemo.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5be3f2b3-b666-4d89-afe9-9308b99dddfa": {"__data__": {"id_": "5be3f2b3-b666-4d89-afe9-9308b99dddfa", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "533eeabd853e4c0b1ba266ac25fe1f9f3240debe", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\modules.md", "author": "LlamaIndex"}, "hash": "fc8e5732cd159267e57e0ee50bdf2746533f8c855ad4f249a49525999c094d47"}}, "hash": "84407cde45e84690ea192e1ac30d814814d1712be12fdd56b604bd750b398d6e", "text": "# Module Guides\n\n\n## Basic\n```{toctree}\n---\nmaxdepth: 1\n---\nRetriever Query Engine </examples/query_engine/CustomRetrievers.ipynb>\n```\n\n## Structured & Semi-Structured Data\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/json_query_engine.ipynb\n/examples/query_engine/pandas_query_engine.ipynb\n/examples/query_engine/knowledge_graph_query_engine.ipynb\n/examples/query_engine/knowledge_graph_rag_query_engine.ipynb\n```\n\n## Advanced\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/RouterQueryEngine.ipynb\n/examples/query_engine/RetrieverRouterQueryEngine.ipynb\n/examples/query_engine/JointQASummary.ipynb\n/examples/query_engine/sub_question_query_engine.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n/examples/query_engine/SQLRouterQueryEngine.ipynb\n/examples/query_engine/SQLAutoVectorQueryEngine.ipynb\n/examples/query_engine/SQLJoinQueryEngine.ipynb\n/examples/index_structs/struct_indices/duckdb_sql_query.ipynb\nRetry Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Source Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Guideline Query Engine </examples/evaluation/RetryQuery.ipynb>\n/examples/query_engine/citation_query_engine.ipynb\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n/examples/query_engine/recursive_retriever_agents.ipynb\n/examples/query_engine/ensemble_query_engine.ipynb\n```\n\n## Experimental\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/flare_query_engine.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e27449d-52ac-41cb-ba21-9dbf8b23ee0a": {"__data__": {"id_": "8e27449d-52ac-41cb-ba21-9dbf8b23ee0a", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\response_modes.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ebf561d9ceed54572c4415becf9b77af17470a70", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\response_modes.md", "author": "LlamaIndex"}, "hash": "206d3e66507bc96db358a4a77d3c21bfdf507468f9e6e2efe414ccb4d4932047"}}, "hash": "c8af3febefaf135f9c690f56b8f498c38f8a9473a1d49a53cd794c47d74751f5", "text": "# Response Modes\n\nRight now, we support the following options:\n\n- `refine`: ***create and refine*** an answer by sequentially going through each retrieved text chunk. \n    This makes a separate LLM call per Node/retrieved chunk. \n\n    **Details:** the first chunk is used in a query using the \n    `text_qa_template` prompt. Then the answer and the next chunk (as well as the original question) are used \n    in another query with the `refine_template` prompt. And so on until all chunks have been parsed. \n\n    If a chunk is too large to fit within the window (considering the prompt size), it is splitted using a `TokenTextSplitter`\n    (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks\n    of the original chunks collection (and thus queried with the `refine_template` as well).\n\n    Good for more detailed answers.\n- `compact` (default): similar to `refine` but ***compact*** (concatenate) the chunks beforehand, resulting in less LLM calls.\n\n    **Details:** stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window \n    (considering the maximum prompt size between `text_qa_template` and `refine_template`).\n    If the text is too long to fit in one prompt, it is splitted in as many parts as needed \n    (using a `TokenTextSplitter` and thus allowing some overlap between text chunks). \n    \n    Each text part is considered a \"chunk\" and is sent to the `refine` synthesizer. \n    \n    In short, it is like `refine`, but with less LLM calls.\n- `tree_summarize`: Query the LLM using the `summary_template` prompt as many times as needed so that all concatenated chunks\n   have been queried, resulting in as many answers that are themselves recursively used as chunks in a `tree_summarize` LLM call \n   and so on, until there's only one chunk left, and thus only one final answer.\n\n   **Details:** concatenate the chunks as much as possible to fit within the context window using the `summary_template` prompt, \n   and split them if needed (again with a `TokenTextSplitter` and some text overlap). Then, query each resulting chunk/split against \n   `summary_template` (there is no ***refine*** query !) and get as many answers. \n   \n   If there is only one answer (because there was only one chunk), then it's the final answer. \n   \n   If there are more than one answer, these themselves are considered as chunks and sent recursively \n   to the `tree_summarize` process (concatenated/splitted-to-fit/queried).\n   \n   Good for summarization purposes.\n- `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt. Good for quick\n    summarization purposes, but may lose detail due to truncation.\n- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them. Then can be inspected by checking `response.source_nodes`.\n- `accumulate`: Given a set of text chunks and the query, apply the query to each text\n    chunk while accumulating the responses into an array. Returns a concatenated string of all\n    responses. Good for when you need to run the same query separately against each text\n    chunk.\n- `compact_accumulate`: The same as accumulate, but will \"compact\" each LLM prompt similar to\n    `compact`, and run the same query against each text chunk.\n\nSee [Response Synthesizer](/core_modules/query_modules/response_synthesizers/root.md) to learn more.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "53d695ce-fc96-4ce3-9f81-a6f4bc42c7d9": {"__data__": {"id_": "53d695ce-fc96-4ce3-9f81-a6f4bc42c7d9", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60aee9e1672669b7ccdcf974baa3507749fbaf2e", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\root.md", "author": "LlamaIndex"}, "hash": "4e7a82e886f4cc4074d5f282bff8aefd1bd75aa287cbe57a5621127f532334a2"}}, "hash": "26b603751f91854004098523882b3807988e6f0f4b813fb240664f1b301191bf", "text": "# Query Engine\n\n## Concept\nQuery engine is a generic interface that allows you to ask question over your data.\n\nA query engine takes in a natural language query, and returns a rich response.\nIt is most often (but not always) built on one or many [Indices](/core_modules/data_modules/index/root.md) via [Retrievers](/core_modules/query_modules/retriever/root.md).\nYou can compose multiple query engines to achieve more advanced capability.\n\n```{tip}\nIf you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at [Chat Engine](/core_modules/query_modules/chat_engines/root.md)  \n```\n\n## Usage Pattern\nGet started with:\n```python\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Who is Paul Graham.\")\n```\n\nTo stream response:\n```python\nquery_engine = index.as_query_engine(streaming=True)\nstreaming_response = query_engine.query(\"Who is Paul Graham.\")\nstreaming_response.print_response_stream() \n```\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\n\n## Modules\n```{toctree}\n---\nmaxdepth: 3\n---\nmodules.md\n```\n\n\n## Supporting Modules\n```{toctree}\n---\nmaxdepth: 2\n---\nsupporting_modules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "60e2a7b0-3822-4db0-b36f-fe4c5f79749f": {"__data__": {"id_": "60e2a7b0-3822-4db0-b36f-fe4c5f79749f", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\streaming.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0ed385b8ae778071d6879ced4f97763ab2192994", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\streaming.md", "author": "LlamaIndex"}, "hash": "f2e521a192f84ae54ea99f736dfce869609fe763df7c41b83ceec3e4f7d9a371"}}, "hash": "70e3a15099a6741cdda3473920cb507fef242b6abb2d7f13ee4da161dd358d62", "text": "# Streaming\n\nLlamaIndex supports streaming the response as it's being generated.\nThis allows you to start printing or processing the beginning of the response before the full response is finished.\nThis can drastically reduce the perceived latency of queries.\n\n### Setup\nTo enable streaming, you need to use an LLM that supports streaming.\nRight now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).\n\nConfigure query engine to use streaming:\n\nIf you are using the high-level API, set `streaming=True` when building a query engine.\n```python\nquery_engine = index.as_query_engine(\n    streaming=True,\n    similarity_top_k=1\n)\n```\n\nIf you are using the low-level API to compose the query engine,\npass `streaming=True` when constructing the `Response Synthesizer`:\n```python\nfrom llama_index import get_response_synthesizer\nsynth = get_response_synthesizer(streaming=True, ...)\nquery_engine = RetrieverQueryEngine(response_synthesizer=synth, ...)\n```\n\n### Streaming Response\nAfter properly configuring both the LLM and the query engine,\ncalling `query` now returns a `StreamingResponse` object.\n\n```python\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\", \n)\n```\n\nThe response is returned immediately when the LLM call *starts*, without having to wait for the full completion.\n\n> Note: In the case where the query engine makes multiple LLM calls, only the last LLM call will be streamed and the response is returned when the last LLM call starts.\n\nYou can obtain a `Generator` from the streaming response and iterate over the tokens as they arrive:\n```python\nfor text in streaming_response.response_gen:\n    # do something with text as they arrive.\n```\n\nAlternatively, if you just want to print the text as they arrive:\n```\nstreaming_response.print_response_stream() \n```\n\nSee an [end-to-end example](/examples/customization/streaming/SimpleIndexDemo-streaming.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2f005cd4-360b-4e92-8a7a-7638c571428c": {"__data__": {"id_": "2f005cd4-360b-4e92-8a7a-7638c571428c", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\supporting_modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8b2c65800dbf3b0098c8a42d7df7f1bca587259c", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\supporting_modules.md", "author": "LlamaIndex"}, "hash": "397d90477a1fbea9bad72f6c1b5ffadd71d9fe0c81f235f752fbe0acb087ea70"}}, "hash": "0958328ac44134b9f3fc5adaa54b200b2f580a578b646585b0eb9f30888783ad", "text": "# Supporting Modules\n\n```{toctree}\n---\nmaxdepth: 1\n---\nadvanced/query_transformations.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f848b2dc-c545-438a-aac6-2da67a02ad7d": {"__data__": {"id_": "f848b2dc-c545-438a-aac6-2da67a02ad7d", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d66e54f11c8266c84b58d0370639a3e9458b9fb9", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "4f53e70153362d2dabe6675346d35cd18bee2b287f042f0a85aa6ebf00450bd2"}}, "hash": "063d6c2ecd2cdc25635ef0aaa1893fba9b231f609cc4fd7d78bee0c858166c98", "text": "# Usage Pattern\n\n## Get Started\nBuild a query engine from index:\n```python\nquery_engine = index.as_query_engine()\n```\n\n```{tip}\nTo learn how to build an index, see [Index](/core_modules/data_modules/index/root.md)\n```\n\nAsk a question over your data\n```python\nresponse = query_engine.query('Who is Paul Graham?')\n```\n\n## Configuring a Query Engine\n### High-Level API\nYou can directly build and configure a query engine from an index in 1 line of code:\n```python\nquery_engine = index.as_query_engine(\n    response_mode='tree_summarize',\n    verbose=True,\n)\n```\n> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.  \n\nSee [**Response Modes**](./response_modes.md) for a full list of response modes and what they do.\n\n```{toctree}\n---\nmaxdepth: 1\nhidden:\n---\nresponse_modes.md\nstreaming.md\n```\n\n\n\n### Low-Level Composition API\n\nYou can use the low-level composition API if you need more granular control.\nConcretely speaking, you would explicitly construct a `QueryEngine` object instead of calling `index.as_query_engine(...)`.\n> Note: You may need to look at API references or example notebooks.\n\n\n```python\nfrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.retrievers import VectorIndexRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\n\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n\n# configure retriever\nretriever = VectorIndexRetriever(\n    index=index, \n    similarity_top_k=2,\n)\n\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n    response_mode=\"tree_summarize\",\n)\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\n# query\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n### Streaming\nTo enable streaming, you simply need to pass in a `streaming=True` flag\n\n```python\nquery_engine = index.as_query_engine(\n    streaming=True,\n)\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\", \n)\nstreaming_response.print_response_stream() \n```\n\n* Read the full [streaming guide](/core_modules/query_modules/query_engine/streaming.md)\n* See an [end-to-end example](/examples/customization/streaming/SimpleIndexDemo-streaming.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "eb6b3f75-8e8f-4b4d-8ea7-405bad15aa42": {"__data__": {"id_": "eb6b3f75-8e8f-4b4d-8ea7-405bad15aa42", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad7717012c6c9751b3d5eecefd6b0bb5cbb57fe7", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\modules.md", "author": "LlamaIndex"}, "hash": "a9ca82a1971a97a1744f6c1c9fb50d1c7ecbc434b332398d4ddcfcd0f28227c4"}}, "hash": "0eec079d4752edbe16c9106d7ebbd8613b0c1854a9f7bfd4baf999fb5adf8dd1", "text": "# Module Guide\n\nDetailed inputs/outputs for each response synthesizer are found below. \n\n## API Example\n\nThe following shows the setup for utilizing all kwargs.\n\n- `response_mode` specifies which response synthesizer to use\n- `service_context` defines the LLM and related settings for synthesis\n- `text_qa_template` and `refine_template` are the prompts used at various stages\n- `use_async` is used for only the `tree_summarize` response mode right now, to asynchronously build the summary tree\n- `streaming` configures whether to return a streaming response object or not\n- `structured_answer_filtering` enables the active filtering of text chunks that are not relevant to a given question\n\nIn the `synthesize`/`asyntheszie` functions, you can optionally provide additional source nodes, which will be added to the `response.source_nodes` list.\n\n```python\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(\n  response_mode=\"refine\",\n  service_context=service_context,\n  text_qa_template=text_qa_template,\n  refine_template=refine_template,\n  use_async=False,\n  streaming=False\n)\n\n# synchronous\nresponse = response_synthesizer.synthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..], \n)\n\n# asynchronous\nresponse = await response_synthesizer.asynthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..], \n)\n```\n\nYou can also directly return a string, using the lower-level `get_response` and `aget_response` functions\n\n```python\nresponse_str = response_synthesizer.get_response(\n  \"query string\", \n  text_chunks=[\"text1\", \"text2\", ...]\n)\n```\n\n## Example Notebooks\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/response_synthesizers/refine.ipynb\n/examples/response_synthesizers/structured_refine.ipynb\n/examples/response_synthesizers/tree_summarize.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dc78f6aa-4eb3-4885-b984-635e33d81e1a": {"__data__": {"id_": "dc78f6aa-4eb3-4885-b984-635e33d81e1a", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdd5a30d575666616ff440c5be5c77c58aac858a", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\root.md", "author": "LlamaIndex"}, "hash": "1ef116cbc847f6c09716e5dc385eefa26f73c543fcde9cc60596e969c5b57695"}}, "hash": "267021d7e6d90267d607995dfc499d1f2ed22c1af3a6f5b9d6b32c674f287d49", "text": "# Response Synthesizer\n\n## Concept\nA `Response Synthesizer` is what generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a `Response` object.\n\nThe method for doing this can take many forms, from as simple as iterating over text chunks, to as complex as building a tree. The main idea here is to simplify the process of generating a response using an LLM across your data.\n\nWhen used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran.\n\n```{tip}\nConfused about where response synthesizer fits in the pipeline? Read the [high-level concepts](/getting_started/concepts.md)\n```\n\n## Usage Pattern\nUse a response synthesizer on it's own:\n\n```python\nfrom llama_index.schema import Node\nfrom llama_index.response_synthesizers import ResponseMode, get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode=ResponseMode.COMPACT)\n\nresponse = response_synthesizer.synthesize(\"query text\", nodes=[Node(text=\"text\"), ...])\n```\n\nOr in a query engine after you've created an index:\n\n```python\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n```\n\nYou can find more details on all available response synthesizers, modes, and how to build your own below.\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\n## Modules\nBelow you can find detailed API information for each response synthesis module.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nmodules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b35c455a-9d46-4ade-8432-722997229b01": {"__data__": {"id_": "b35c455a-9d46-4ade-8432-722997229b01", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b6b6268a452a67680cd9a4516eaba7ac3e85f46", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "ac6046a52181a831ebc289e0618bb3c455f6639cf320e92eec258e982d99d525"}, "3": {"node_id": "1f0e9a72-a9c4-468b-8f4f-7104f003068e", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "b2824ec8c7523f8634c0e557df3945135b22db073b747264d6e33cd242dbaea2"}}, "hash": "5d3540b49390df54ecfe38ad52474daa71f7b0771cf0fdefbcf894b0a5302a8d", "text": "# Usage Pattern\n\n## Get Started\n\nConfiguring the response synthesizer for a query engine using `response_mode`:\n\n```python\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode='compact')\n\nresponse = response_synthesizer.synthesize(\n  \"query text\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..]\n)\n```\n\nOr, more commonly, in a query engine after you've created an index:\n\n```python\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n```\n\n```{tip}\nTo learn how to build an index, see [Index](/core_modules/data_modules/index/root.md)\n```\n\n## Configuring the Response Mode\nResponse synthesizers are typically specified through a `response_mode` kwarg setting.Several response synthesizers are implemented already in LlamaIndex:\n\n- `refine`: ***create and refine*** an answer by sequentially going through each retrieved text chunk.This makes a separate LLM call per Node/retrieved chunk.**Details:** the first chunk is used in a query using the \n    `text_qa_template` prompt.Then the answer and the next chunk (as well as the original question) are used \n    in another query with the `refine_template` prompt.And so on until all chunks have been parsed.If a chunk is too large to fit within the window (considering the prompt size), it is splitted using a `TokenTextSplitter`\n    (allowing some text overlap between chunks) and the (new) additional chunks are considered as chunks\n    of the original chunks collection (and thus queried with the `refine_template` as well).Good for more detailed answers.- `compact` (default): similar to `refine` but ***compact*** (concatenate) the chunks beforehand, resulting in less LLM calls.**Details:** stuff as many text (concatenated/packed from the retrieved chunks) that can fit within the context window \n    (considering the maximum prompt size between `text_qa_template` and `refine_template`).If the text is too long to fit in one prompt, it is splitted in as many parts as needed \n    (using a `TokenTextSplitter` and thus allowing some overlap between text chunks).Each text part is considered a \"chunk\" and is sent to the `refine` synthesizer.In short, it is like `refine`, but with less LLM calls.- `tree_summarize`: Query the LLM using the `summary_template` prompt as many times as needed so that all concatenated chunks\n   have been queried, resulting in as many answers that are themselves recursively used as chunks in a `tree_summarize` LLM call \n   and so on, until there's only one chunk left, and thus only one final answer.**Details:** concatenate the chunks as much as possible to fit within the context window using the `summary_template` prompt, \n   and split them if needed (again with a `TokenTextSplitter` and some text overlap).Then, query each resulting chunk/split against \n   `summary_template` (there is no ***refine*** query !)and get as many answers.If there is only one answer (because there was only one chunk), then it's the final answer.If there are more than one answer, these themselves are considered as chunks and sent recursively \n   to the `tree_summarize` process (concatenated/splitted-to-fit/queried).Good for summarization purposes.- `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt.Good for quick\n    summarization purposes, but may lose detail due to truncation.- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1f0e9a72-a9c4-468b-8f4f-7104f003068e": {"__data__": {"id_": "1f0e9a72-a9c4-468b-8f4f-7104f003068e", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b6b6268a452a67680cd9a4516eaba7ac3e85f46", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "ac6046a52181a831ebc289e0618bb3c455f6639cf320e92eec258e982d99d525"}, "2": {"node_id": "b35c455a-9d46-4ade-8432-722997229b01", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "5d3540b49390df54ecfe38ad52474daa71f7b0771cf0fdefbcf894b0a5302a8d"}}, "hash": "b2824ec8c7523f8634c0e557df3945135b22db073b747264d6e33cd242dbaea2", "text": "Then can be inspected by checking `response.source_nodes`.- `accumulate`: Given a set of text chunks and the query, apply the query to each text\n    chunk while accumulating the responses into an array.Returns a concatenated string of all\n    responses.Good for when you need to run the same query separately against each text\n    chunk.- `compact_accumulate`: The same as accumulate, but will \"compact\" each LLM prompt similar to\n    `compact`, and run the same query against each text chunk.## Custom Response Synthesizers\n\nEach response synthesizer inherits from `llama_index.response_synthesizers.base.BaseSynthesizer`.The base API is extremely simple, which makes it easy to create your own response synthesizer.Maybe you want to customize which template is used at each step in `tree_summarize`, or maybe a new research paper came out detailing a new way to generate a response to a query, you can create your own response synthesizer and plug it into any query engine or use it on it's own.Below we show the `__init__()` function, as well as the two abstract methods that every response synthesizer must implement.The basic requirements are to process a query and text chunks, and return a string (or string generator) response.```python\nclass BaseSynthesizer(ABC):\n    \"\"\"Response builder class.\"\"\"def __init__(\n        self,\n        service_context: Optional[ServiceContext] = None,\n        streaming: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"self._service_context = service_context or ServiceContext.from_defaults()\n        self._callback_manager = self._service_context.callback_manager\n        self._streaming = streaming\n\n    @abstractmethod\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"...\n\n    @abstractmethod\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"...\n```\n\n## Using Structured Answer Filtering\nWhen using either the `\"refine\"` or `\"compact\"` response synthesis modules, you may find it beneficial to experiment with the `structured_answer_filtering` option.```\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(structured_answer_filtering=True)\n```\n\nWith `structured_answer_filtering` set to `True`, our refine module is able to filter out any input nodes that are not relevant to the question being asked.This is particularly useful for RAG-based Q&A systems that involve retrieving chunks of text from external vector store for a given user query.This option is particularly useful if you're using an [OpenAI model that supports function calling](https://openai.com/blog/function-calling-and-other-api-updates).Other LLM providers or models that don't have native function calling support may be less reliable in producing the structured response this feature relies on.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2169d002-ca5c-476c-8df3-eb9e39c74eaf": {"__data__": {"id_": "2169d002-ca5c-476c-8df3-eb9e39c74eaf", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6324fe6c4228f812b6918760fecbd7d5362c7ad9", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\modules.md", "author": "LlamaIndex"}, "hash": "88389237d37019cabce1aa7020cf539e391ff0f9cb6acd1f72057f72d5402b16"}}, "hash": "00cd478682974e67e1ec7865ddc39cd3682c6f53ae163f3ec48c87820bef3f74", "text": "# Module Guides\nWe are actively adding more tailored retrieval guides.\nIn the meanwhile, please take a look at the [API References](/api_reference/query/retrievers.rst).\n\n## Index Retrievers\n\nPlease see [the retriever modes](/core_modules/query_modules/retriever/retriever_modes.md) for more details on how to get a retriever from any given index.\n\nIf you want to import the corresponding retrievers directly, please check out our [API reference](/api_reference/query/retrievers.rst).\n\n## Advanced Retriever Guides\n\nCheck out our comprehensive guides on various retriever modules, many of which cover advanced concepts (auto-retrieval, routing, ensembling, and more).\n\n## External Retrievers\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/retrievers/bm25_retriever.ipynb \n```\n\n## Knowledge Graph Retrievers\n```{toctree}\n---\nmaxdepth: 1\n---\nCustom Retriever (KG Index and Vector Store Index) </examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb>\nKnowledge Graph RAG Retriever </examples/query_engine/knowledge_graph_rag_query_engine.ipynb>\n```\n\n## Composed Retrievers\n```{toctree}\n---\nmaxdepth: 1\n---\nAuto-Retrieval (with Chroma) </examples/vector_stores/chroma_auto_retriever.ipynb>\nAuto-Retrieval (with BagelDB) </examples/vector_stores/BagelAutoRetriever.ipynb>\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n/examples/retrievers/router_retriever.ipynb\n/examples/retrievers/ensemble_retrieval.ipynb\n/examples/retrievers/auto_merging_retriever.ipynb\n/examples/retrievers/recursive_retriever_nodes.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5ba2a608-17fa-4791-89f8-23f07a77bc3d": {"__data__": {"id_": "5ba2a608-17fa-4791-89f8-23f07a77bc3d", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\retriever_modes.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61fc10afcd33f050116c937e56db5d97add4abfa", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\retriever_modes.md", "author": "LlamaIndex"}, "hash": "711f4e8677c7124bcc880b68073cb82a2d6b41222a0f55309b42b1147a448516"}}, "hash": "28df6210c37f36259c9c7817162e2bf6547306e9cb18c7d0bb89b6c1ebb8dc9e", "text": "# Retriever Modes\nHere we show the mapping from `retriever_mode` configuration to the selected retriever class.\n> Note that `retriever_mode` can mean different thing for different index classes. \n\n## Vector Index\nSpecifying `retriever_mode` has no effect (silently ignored).\n`vector_index.as_retriever(...)` always returns a VectorIndexRetriever.\n\n\n## Summary Index\n* `default`: SummaryIndexRetriever \n* `embedding`: SummaryIndexEmbeddingRetriever \n* `llm`: SummaryIndexLLMRetriever\n\n## Tree Index\n* `select_leaf`: TreeSelectLeafRetriever\n* `select_leaf_embedding`: TreeSelectLeafEmbeddingRetriever\n* `all_leaf`: TreeAllLeafRetriever\n* `root`: TreeRootRetriever\n\n\n## Keyword Table Index\n* `default`: KeywordTableGPTRetriever\n* `simple`: KeywordTableSimpleRetriever\n* `rake`: KeywordTableRAKERetriever\n\n\n## Knowledge Graph Index\n* `keyword`: KGTableRetriever\n* `embedding`: KGTableRetriever\n* `hybrid`: KGTableRetriever\n\n## Document Summary Index\n* `default`: DocumentSummaryIndexRetriever\n* `embedding`: DocumentSummaryIndexEmbeddingRetrievers", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d10b62f5-9f13-484e-8f54-bb8ccaff358a": {"__data__": {"id_": "d10b62f5-9f13-484e-8f54-bb8ccaff358a", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "922638c6d1f38d3de450d8af8edf0e17a51b0c35", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\root.md", "author": "LlamaIndex"}, "hash": "01c4481cd4336f2bbd56a524ef8cabd22bff9c35491a1791fde49cade9892d78"}}, "hash": "ca333b5a81c37fbbf8ae2bea121b5627d166fb181e4d1c3543ca785c7e60bf4c", "text": "# Retriever\n\n## Concept\n\nRetrievers are responsible for fetching the most relevant context given a user query (or chat message).  \n\nIt can be built on top of [Indices](/core_modules/data_modules/index/root.md), but can also be defined independently.\nIt is used as a key building block in [Query Engines](/core_modules/query_modules/query_engine/root.md) (and [Chat Engines](/core_modules/query_modules/chat_engines/root.md)) for retrieving relevant context.\n\n```{tip}\nConfused about where retriever fits in the pipeline? Read about [high-level concepts](/getting_started/concepts.md)\n```\n\n## Usage Pattern\n\nGet started with:\n```python\nretriever = index.as_retriever()\nnodes = retriever.retrieve(\"Who is Paul Graham?\")\n```\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\n\n## Modules\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0c2a9fc8-fcd4-4543-ab26-deee440d6f69": {"__data__": {"id_": "0c2a9fc8-fcd4-4543-ab26-deee440d6f69", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30c07b32076ea2869a8752ed3160786bca681845", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "95aaaeb0096c2eb49539794ec454534184aa8dcf0aeb50cc71f93fcb319068ea"}}, "hash": "35013f307cd7e0571ada923a2422ae7ecc728bedae82973970382160ba9110cc", "text": "# Usage Pattern\n\n## Get Started\nGet a retriever from index:\n```python\nretriever = index.as_retriever()\n```\n\nRetrieve relevant context for a question:\n```python\nnodes = retriever.retrieve('Who is Paul Graham?')\n```\n\n> Note: To learn how to build an index, see [Index](/core_modules/data_modules/index/root.md)\n\n## High-Level API\n\n### Selecting a Retriever\n\nYou can select the index-specific retriever class via `retriever_mode`. \nFor example, with a `SummaryIndex`:\n```python\nretriever = summary_index.as_retriever(\n    retriever_mode='llm',\n)\n```\nThis creates a [SummaryIndexLLMRetriever](/api_reference/query/retrievers/list.rst) on top of the summary index.\n\nSee [**Retriever Modes**](/core_modules/query_modules/retriever/retriever_modes.md) for a full list of (index-specific) retriever modes\nand the retriever classes they map to.\n\n```{toctree}\n---\nmaxdepth: 1\nhidden:\n---\nretriever_modes.md\n```\n\n### Configuring a Retriever\nIn the same way, you can pass kwargs to configure the selected retriever.\n> Note: take a look at the API reference for the selected retriever class' constructor parameters for a list of valid kwargs.\n\nFor example, if we selected the \"llm\" retriever mode, we might do the following:\n```python\nretriever = summary_index.as_retriever(\n    retriever_mode='llm',\n    choice_batch_size=5,\n)\n\n```\n\n## Low-Level Composition API\nYou can use the low-level composition API if you need more granular control.  \n\nTo achieve the same outcome as above, you can directly import and construct the desired retriever class:\n```python\nfrom llama_index.indices.list import SummaryIndexLLMRetriever\n\nretriever = SummaryIndexLLMRetriever(\n    index=summary_index,\n    choice_batch_size=5,\n)\n```\n\n\n## Advanced\n\n```{toctree}\n---\nmaxdepth: 1\n---\nDefine Custom Retriever </examples/query_engine/CustomRetrievers.ipynb>\nBM25 Hybrid Retriever </examples/retrievers/bm25_retriever.ipynb>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8eee734c-38a3-417c-b234-8cbb9df1b1aa": {"__data__": {"id_": "8eee734c-38a3-417c-b234-8cbb9df1b1aa", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a267352c1477cc2f1a3ed8097dfed96bda38964", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\modules.md", "author": "LlamaIndex"}, "hash": "440ae49082060763eff04ed34b21e4bcf281dc407da9f03eb245dc4d0560040f"}}, "hash": "fd06dc89e5747f7393cec6fb03457a40c64acbb3c8cb83238fc547bc147b0772", "text": "# Modules\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/RouterQueryEngine.ipynb\n/examples/query_engine/RetrieverRouterQueryEngine.ipynb\n/examples/query_engine/SQLRouterQueryEngine.ipynb\n/examples/retrievers/router_retriever.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "701411f3-d6cf-472f-9044-21e5e86f5f4a": {"__data__": {"id_": "701411f3-d6cf-472f-9044-21e5e86f5f4a", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2f004252113ef51ed6070e8129422a4ebbbbc4d", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\root.md", "author": "LlamaIndex"}, "hash": "4b9c8c7f0de19ca5baedbfd1dc1eb790c7c4f5ff050e81f2bc19b7631577e8e6"}}, "hash": "b3b775ab87e45d49ff98aa1268c28c66fe8da302e590112a1274ad86d4fdc643", "text": "# Routers\n\n## Concept\nRouters are modules that take in a user query and a set of \"choices\" (defined by metadata), and returns one or more selected choices.\n\nThey can be used on their own (as \"selector modules\"), or used as a query engine or retriever (e.g. on top of other query engines/retrievers).\n\nThey are simple but powerful modules that use LLMs for decision making capabilities. They can be used for the following use cases and more:\n- Selecting the right data source among a diverse range of data sources\n- Deciding whether to do summarization (e.g. using summary index query engine) or semantic search (e.g. using vector index query engine)\n- Deciding whether to \"try\" out a bunch of choices at once and combine the results (using multi-routing capabilities).\n\nThe core router modules exist in the following forms:\n- LLM selectors put the choices as a text dump into a prompt and use LLM text completion endpoint to make decisions\n- Pydantic selectors pass choices as Pydantic schemas into a function calling endpoint, and return Pydantic objects\n\n## Usage Pattern\n\nA simple example of using our router module as part of a query engine is given below.\n\n```python\nfrom llama_index.query_engine.router_query_engine import RouterQueryEngine\nfrom llama_index.selectors.pydantic_selectors import PydanticSingleSelector\nfrom llama_index.tools.query_engine import QueryEngineTool\n\n\nlist_tool = QueryEngineTool.from_defaults(\n    query_engine=list_query_engine,\n    description=\"Useful for summarization questions related to the data source\",\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=vector_query_engine,\n    description=\"Useful for retrieving specific context related to the data source\",\n)\n\nquery_engine = RouterQueryEngine(\n    selector=PydanticSingleSelector.from_defaults(),\n    query_engine_tools=[\n        list_tool,\n        vector_tool,\n    ],\n)\nquery_engine.query(\"<query>\")\n```\n\nYou can find more details using routers as standalone modules, as part of a query engine, and as part of a retriever\nbelow in the usage pattern guide.\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\n## Modules\nBelow you can find extensive guides using routers in different settings.\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c40fb52a-59ce-4300-93ed-e52bc1c0a18f": {"__data__": {"id_": "c40fb52a-59ce-4300-93ed-e52bc1c0a18f", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "679bed9d3c4ad324f657f311e28c0eec3f8cd7fc", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "f1683482ee31f574473b7ee4c2cb42a0bdc14cfd923b057cab0217ea197c6de0"}, "3": {"node_id": "fa87eaf5-9ccf-4d36-bb6e-6619ef22e5e2", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "78abba2e4a2e8d5c49588826a6526a1564fc22834e7add77c43bf0714e91ead4"}}, "hash": "4f9e69a0b6f1781b11740c8b3533093b1a9f5d2602bfe288b14324148322d431", "text": "# Usage Pattern\n\nDefining a \"selector\" is at the core of defining a router.You can easily use our routers as a query engine or a retriever.In these cases, the router will be responsible\nfor \"selecting\" query engine(s) or retriever(s) to route the user query to.We also highlight our `ToolRetrieverRouterQueryEngine` for retrieval-augmented routing - this is the case\nwhere the set of choices themselves may be very big and may need to be indexed.**NOTE**: this is a beta feature.We also highlight using our router as a standalone module.## Defining a selector\n\nSome examples are given below with LLM and Pydantic based single/multi selectors:\n\n```python\nfrom llama_index.selectors.llm_selectors import LLMSingleSelector, LLMMultiSelector\nfrom llama_index.selectors.pydantic_selectors import (\n    PydanticMultiSelector,\n    PydanticSingleSelector,\n)\n\n# pydantic selectors feed in pydantic objects to a function calling API\n# single selector (pydantic)\nselector = PydanticSingleSelector.from_defaults()\n# multi selector (pydantic)\nselector = PydanticMultiSelector.from_defaults()\n\n# LLM selectors use text completion endpoints\n# single selector (LLM)\nselector = LLMSingleSelector.from_defaults()\n# multi selector (LLM)\nselector = LLMMultiSelector.from_defaults()\n\n```\n\n## Using as a Query Engine\n\nA `RouterQueryEngine` is composed on top of other query engines as tools.```python\nfrom llama_index.query_engine.router_query_engine import RouterQueryEngine\nfrom llama_index.selectors.pydantic_selectors import PydanticSingleSelector, Pydantic\nfrom llama_index.tools.query_engine import QueryEngineTool\nfrom llama_index import (\n    VectorStoreIndex,\n    SummaryIndex,\n)\n\n# define query engines\n...\n\n# initialize tools\nlist_tool = QueryEngineTool.from_defaults(\n    query_engine=list_query_engine,\n    description=\"Useful for summarization questions related to the data source\",\n)\nvector_tool = QueryEngineTool.from_defaults(\n    query_engine=vector_query_engine,\n    description=\"Useful for retrieving specific context related to the data source\",\n)\n\n# initialize router query engine (single selection, pydantic)\nquery_engine = RouterQueryEngine(\n    selector=PydanticSingleSelector.from_defaults(),\n    query_engine_tools=[\n        list_tool,\n        vector_tool,\n    ],\n)\nquery_engine.query(\"<query>\")\n\n```\n\n## Using as a Retriever\n\nSimilarly, a `RouterRetriever` is composed on top of other retrievers as tools.An example is given below:\n\n```python\nfrom llama_index.query_engine.router_query_engine import RouterQueryEngine\nfrom llama_index.selectors.pydantic_selectors import PydanticSingleSelector\nfrom llama_index.tools import RetrieverTool\n\n# define indices\n...\n\n# define retrievers\nvector_retriever = vector_index.as_retriever()\nkeyword_retriever = keyword_index.as_retriever()\n\n# initialize tools\nvector_tool = RetrieverTool.from_defaults(\n    retriever=vector_retriever,\n    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fa87eaf5-9ccf-4d36-bb6e-6619ef22e5e2": {"__data__": {"id_": "fa87eaf5-9ccf-4d36-bb6e-6619ef22e5e2", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "679bed9d3c4ad324f657f311e28c0eec3f8cd7fc", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "f1683482ee31f574473b7ee4c2cb42a0bdc14cfd923b057cab0217ea197c6de0"}, "2": {"node_id": "c40fb52a-59ce-4300-93ed-e52bc1c0a18f", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "4f9e69a0b6f1781b11740c8b3533093b1a9f5d2602bfe288b14324148322d431"}}, "hash": "78abba2e4a2e8d5c49588826a6526a1564fc22834e7add77c43bf0714e91ead4", "text": "\",\n)\nkeyword_tool = RetrieverTool.from_defaults(\n    retriever=keyword_retriever,\n    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On (using entities mentioned in query)\",\n)\n\n# define retriever\nretriever = RouterRetriever(\n    selector=PydanticSingleSelector.from_defaults(llm=llm),\n    retriever_tools=[\n        list_tool,\n        vector_tool,\n    ],\n)\n\n```\n\n## Using selector as a standalone module\n\nYou can use the selectors as standalone modules.Define choices as either a list of `ToolMetadata` or as a list of strings.```python\nfrom llama_index.tools import ToolMetadata\nfrom llama_index.selectors.llm_selectors import LLMSingleSelector\n\n\n# choices as a list of tool metadata\nchoices = [\n    ToolMetadata(description=\"description for choice 1\", name=\"choice_1\"),\n    ToolMetadata(description=\"description for choice 2\", name=\"choice_2\"),\n]\n\n# choices as a list of strings\nchoices = [\"choice 1 - description for choice 1\", \"choice 2: description for choice 2\"]\n\nselector = LLMSingleSelector.from_defaults()\nselector_result = selector.select(choices, query=\"What's revenue growth for IBM in 2007?\")\nprint(selector_result.selections)\n\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0912f890-b478-4670-9e15-d994aac2cf44": {"__data__": {"id_": "0912f890-b478-4670-9e15-d994aac2cf44", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f2ba5328b2570353c7b0860760a3b61c100e95b9", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md", "author": "LlamaIndex"}, "hash": "f3d237e004af10f5497b05973ac22b0f03ed28f8c19b50a8cd34bb31fcf2027b"}, "3": {"node_id": "696f5357-db8c-4c4b-81f0-5259c91ce418", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md", "author": "LlamaIndex"}, "hash": "0e35dd7fabc39802f262683a5b9a560bfcd5c972cdde7904d3670bc94b75d6ae"}}, "hash": "3c8842d14f5da72f9cb5592e9b58c279ca7e491e38fb34bd65b2cce30d56dc15", "text": "# Output Parsing\n\nLlamaIndex supports integrations with output parsing modules offered\nby other frameworks. These output parsing modules can be used in the following ways:\n- To provide formatting instructions for any prompt / query (through `output_parser.format`)\n- To provide \"parsing\" for LLM outputs (through `output_parser.parse`)\n\n\n### Guardrails\n\nGuardrails is an open-source Python package for specification/validation/correction of output schemas. See below for a code example.\n\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import GuardrailsOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts import PromptTemplate\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\n\n\n# load documents, build index\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents, chunk_size=512)\nllm_predictor = StructuredLLMPredictor()\n\n\n# specify StructuredLLMPredictor\n# this is a special LLMPredictor that allows for structured outputs\n\n# define query / output spec\nrail_spec = (\"\"\"\n<rail version=\"0.1\">\n\n<output>\n    <list name=\"points\" description=\"Bullet points regarding events in the author's life.\">\n        <object>\n            <string name=\"explanation\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation2\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation3\" format=\"one-line\" on-fail-one-line=\"noop\" />\n        </object>\n    </list>\n</output>\n\n<prompt>\n\nQuery string here.\n\n@xml_prefix_prompt\n\n{output_schema}\n\n@json_suffix_prompt_v2_wo_none\n</prompt>\n</rail>\n\"\"\")\n\n# define output parser\noutput_parser = GuardrailsOutputParser.from_rail_string(rail_spec, llm=llm_predictor.llm)\n\n# format each prompt with output parser instructions\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\n\nqa_prompt = PromptTemplate(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = PromptTemplate(fmt_refine_tmpl, output_parser=output_parser)\n\n# obtain a structured response\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are the three items the author did growing up?\", \n)\nprint(response)\n\n```\n\nOutput:\n```\n{'points': [{'explanation': 'Writing short stories', 'explanation2': 'Programming on an IBM 1401', 'explanation3': 'Using microcomputers'}]}\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "696f5357-db8c-4c4b-81f0-5259c91ce418": {"__data__": {"id_": "696f5357-db8c-4c4b-81f0-5259c91ce418", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f2ba5328b2570353c7b0860760a3b61c100e95b9", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md", "author": "LlamaIndex"}, "hash": "f3d237e004af10f5497b05973ac22b0f03ed28f8c19b50a8cd34bb31fcf2027b"}, "2": {"node_id": "0912f890-b478-4670-9e15-d994aac2cf44", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md", "author": "LlamaIndex"}, "hash": "3c8842d14f5da72f9cb5592e9b58c279ca7e491e38fb34bd65b2cce30d56dc15"}}, "hash": "0e35dd7fabc39802f262683a5b9a560bfcd5c972cdde7904d3670bc94b75d6ae", "text": "### Langchain\n\nLangchain also offers output parsing modules that you can use within LlamaIndex.\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import LangchainOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts import PromptTemplate\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\n\n# load documents, build index\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nllm_predictor = StructuredLLMPredictor()\n\n# define output schema\nresponse_schemas = [\n    ResponseSchema(name=\"Education\", description=\"Describes the author's educational experience/background.\"),\n    ResponseSchema(name=\"Work\", description=\"Describes the author's work experience/background.\")\n]\n\n# define output parser\nlc_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\noutput_parser = LangchainOutputParser(lc_output_parser)\n\n# format each prompt with output parser instructions\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\nqa_prompt = PromptTemplate(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = PromptTemplate(fmt_refine_tmpl, output_parser=output_parser)\n\n# query index\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are a few things the author did growing up?\", \n)\nprint(str(response))\n```\n\nOutput:\n\n```\n{'Education': 'Before college, the author wrote short stories and experimented with programming on an IBM 1401.', 'Work': 'The author worked on writing and programming outside of school.'}\n```\n\n### Guides\n\n```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n\n/examples/output_parsing/GuardrailsDemo.ipynb\n/examples/output_parsing/LangchainOutputParserDemo.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n/examples/output_parsing/openai_pydantic_program.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "96540789-826e-40b9-ba7a-6137b1eae712": {"__data__": {"id_": "96540789-826e-40b9-ba7a-6137b1eae712", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a79ee9daf554d84400c742faa6c48580606f1860", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md", "author": "LlamaIndex"}, "hash": "504b0b7551a7e69a9b11ee33a2c2072c5910aacfb1bb855dacc024aabab6d89d"}}, "hash": "6a3b35c9c60d2f282072516069d12db036524d43887d42f89ef799d1ce7e10cf", "text": "# Pydantic Program\n\nA pydantic program is a generic abstraction that takes in an input string and converts it to a structured Pydantic object type.\n\nBecause this abstraction is so generic, it encompasses a broad range of LLM workflows. The programs are composable and be for more generic or specific use cases.\n\nThere's a few general types of Pydantic Programs:\n- **LLM Text Completion Pydantic Programs**: These convert input text into a user-specified structured object through a text completion API + output parsing.\n- **LLM Function Calling Pydantic Program**: These convert input text into a user-specified structured object through an LLM function calling API.\n- **Prepackaged Pydantic Programs**: These convert input text into prespecified structured objects.\n\n\n## LLM Text Completion Pydantic Programs\nTODO: Coming soon!\n\n\n## LLM Function Calling Pydantic Programs\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/openai_pydantic_program.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n```\n\n\n## Prepackaged Pydantic Programs\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/df_program.ipynb\n/examples/output_parsing/evaporate_program.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6f88b8dc-f3a2-4c1a-bd4b-7b50716f06c6": {"__data__": {"id_": "6f88b8dc-f3a2-4c1a-bd4b-7b50716f06c6", "embedding": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a32026b5e77ab742f2fc15773420557e36820274", "node_type": null, "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\root.md", "author": "LlamaIndex"}, "hash": "72da47c6f2db4246dafc8b1434ebcbf673fa7f59488b483566845e675b4e7b41"}}, "hash": "ae32bb4aa38f57e981f58d1b93979d63fa15502899208c3536a9d0f5fbfbe6aa", "text": "# Structured Outputs\n\nThe ability of LLMs to produce structured outputs are important for downstream applications that rely on reliably parsing output values. \nLlamaIndex itself also relies on structured output in the following ways.\n- **Document retrieval**: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval. For instance, the tree index expects LLM calls to be in the format \"ANSWER: (number)\".\n- **Response synthesis**: Users may expect that the final response contains some degree of structure (e.g. a JSON output, a formatted SQL query, etc.)\n\nLlamaIndex provides a variety of modules enabling LLMs to produce outputs in a structured format. We provide modules at different levels of abstraction:\n- **Output Parsers**: These are modules that operate before and after an LLM text completion endpoint. They are not used with LLM function calling endpoints (since those contain structured outputs out of the box).\n- **Pydantic Programs**: These are generic modules that map an input prompt to a structured output, represented by a Pydantic object. They may use function calling APIs or text completion APIs + output parsers.\n- **Pre-defined Pydantic Program**: We have pre-defined Pydantic programs that map inputs to specific output types (like dataframes).\n\nSee the sections below for an overview of output parsers and Pydantic programs.\n\n## \ud83d\udd2c Anatomy of a Structured Output Function\n\nHere we describe the different components of an LLM-powered structured output function. The pipeline depends on whether you're using a **generic LLM text completion API** or an **LLM function calling API**.\n\n![](/_static/structured_output/diagram1.png)\n\nWith generic completion APIs, the inputs and outputs are handled by text prompts. The output parser plays a role before and after the LLM call in ensuring structured outputs. Before the LLM call, the output parser can\nappend format instructions to the prompt. After the LLM call, the output parser can parse the output to the specified instructions.\n\nWith function calling APIs, the output is inherently in a structured format, and the input can take in the signature of the desired object. The structured output just needs to be cast in the right object format (e.g. Pydantic).\n\n## Output Parser Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\noutput_parser.md\n```\n\n## Pydantic Program Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\npydantic_program.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1bbaca72-5f2b-4df7-9b82-2fbc645f8eb2": {"__data__": {"id_": "1bbaca72-5f2b-4df7-9b82-2fbc645f8eb2", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\callbacks\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d06e4f1fa8264cb1f94541623ab28f70a7224bbb", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\callbacks\\root.md", "author": "LlamaIndex"}, "hash": "e59a3feffa7b2e54b8d9b415cac675b71c6e99e92a5ecaca68c810d9298993e5"}}, "hash": "3fa4f09b007b737cd1a6a809b2457441a3dc3edff65ffc7f3bebf7cce9c0f2da", "text": "# Callbacks\n\n## Concept\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library. \nUsing the callback manager, as many callbacks as needed can be added.\n\nIn addition to logging data related to events, you can also track the duration and number of occurances\nof each event. \n\nFurthermore, a trace map of events is also recorded, and callbacks can use this data\nhowever they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events\nafter most operations.\n\n**Callback Event Types**  \nWhile each callback may not leverage each event type, the following events are available to be tracked:\n\n- `CHUNKING` -> Logs for the before and after of text splitting.\n- `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into.\n- `EMBEDDING` -> Logs for the number of texts embedded.\n- `LLM` -> Logs for the template and response of LLM calls.\n- `QUERY` -> Keeps track of the start and end of each query.\n- `RETRIEVE` -> Logs for the nodes retrieved for a query.\n- `SYNTHESIZE` -> Logs for the result for synthesize calls.\n- `TREE` -> Logs for the summary and level of summaries generated.\n- `SUB_QUESTION` -> Log for a generated sub question and answer.\n\nYou can implement your own callback to track and trace these events, or use an existing callback.\n\n\n## Modules\n\nCurrently supported callbacks are as follows:\n\n- [TokenCountingHandler](/examples/callbacks/TokenCountingHandler.ipynb) -> Flexible token counting for prompt, completion, and embedding token usage. See the migration details [here](/core_modules/model_modules/callbacks/token_counting_migration.md)\n- [LlamaDebugHanlder](/examples/callbacks/LlamaDebugHandler.ipynb) -> Basic tracking and tracing for events. Example usage can be found in the notebook below.\n- [WandbCallbackHandler](/examples/callbacks/WandbCallbackHandler.ipynb) -> Tracking of events and traces using the Wandb Prompts frontend. More details are in the notebook below or at [Wandb](https://docs.wandb.ai/guides/prompts/quickstart)\n- [AimCallback](/examples/callbacks/AimCallback.ipynb) -> Tracking of LLM inputs and outputs. Example usage can be found in the notebook below.\n- [OpenInferenceCallbackHandler](/examples/callbacks/OpenInferenceCallback.ipynb) -> Tracking of AI model inferences. Example usage can be found in the notebook below.\n- [OpenAIFineTuningHandler](https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb) -> Records all LLM inputs and outputs. Then, provides a function `save_finetuning_events()` to save inputs and outputs in a format suitable for fine-tuning with OpenAI.\n\n\n```{toctree}\n---\nmaxdepth: 1\nhidden:\n---\n/examples/callbacks/TokenCountingHandler.ipynb\n/examples/callbacks/LlamaDebugHandler.ipynb\n/examples/callbacks/WandbCallbackHandler.ipynb\n/examples/callbacks/AimCallback.ipynb\n/examples/callbacks/OpenInferenceCallback.ipynb\ntoken_counting_migration.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "20d0b081-42d7-4dfc-890f-a3f22e25a4ca": {"__data__": {"id_": "20d0b081-42d7-4dfc-890f-a3f22e25a4ca", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c92e40ce73179f4aae1b8ed918b805e3a585ffdd", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md", "author": "LlamaIndex"}, "hash": "a84e99ab2ffc1e89cfe59f5b31ecc152279f6d9db8f8ed486f01ca1b0b6b0c1c"}}, "hash": "142ce378bdc07312a2a3352b00e73313f1990a48f55fd420b6a8d97662f53c6b", "text": "# Token Counting - Migration Guide\n\nThe existing token counting implementation has been __deprecated__. \n\nWe know token counting is important to many users, so this guide was created to walkthrough a (hopefully painless) transition. \n\nPreviously, token counting was kept track of on the `llm_predictor` and `embed_model` objects directly, and optionally printed to the console. This implementation used a static tokenizer for token counting (gpt-2), and the `last_token_usage` and `total_token_usage` attributes were not always kept track of properly.\n\nGoing forward, token counting as moved into a callback. Using the `TokenCountingHandler` callback, you now have more options for how tokens are counted, the lifetime of the token counts, and even creating separete token counters for different indexes.\n\nHere is a minimum example of using the new `TokenCountingHandler` with an OpenAI model:\n\n```python\nimport tiktoken\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n\n# you can set a tokenizer directly, or optionally let it default \n# to the same tokenizer that was used previously for token counting\n# NOTE: The tokenizer should be a function that takes in text and returns a list of tokens\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"text-davinci-003\").encode\n    verbose=False  # set to true to see usage printed to the console\n)\n\ncallback_manager = CallbackManager([token_counter])\n\nservice_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n\ndocument = SimpleDirectoryReader(\"./data\").load_data()\n\n# if verbose is turned on, you will see embedding token usage printed\nindex = VectorStoreIndex.from_documents(documents, service_context=service_context)\n\n# otherwise, you can access the count directly\nprint(token_counter.total_embedding_token_count)\n\n# reset the counts at your discretion!\ntoken_counter.reset_counts()\n\n# also track prompt, completion, and total LLM tokens, in addition to embeddings\nresponse = index.as_query_engine().query(\"What did the author do growing up?\")\nprint('Embedding Tokens: ', token_counter.total_embedding_token_count, '\\n',\n      'LLM Prompt Tokens: ', token_counter.prompt_llm_token_count, '\\n',\n      'LLM Completion Tokens: ', token_counter.completion_llm_token_count, '\\n',\n      'Total LLM Token Count: ', token_counter.total_llm_token_count)\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0696e8c1-c965-456c-a011-9ef3e297b71a": {"__data__": {"id_": "0696e8c1-c965-456c-a011-9ef3e297b71a", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f23a92cc6de0d269bc901bee4a5e97b8891a9e5", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\root.md", "author": "LlamaIndex"}, "hash": "13431fae5193249af970f34b8d53ffacb1abeae3e497c244289905e297e9abbd"}, "3": {"node_id": "96252df0-81a0-4037-bb4f-3fdc546ef84d", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\root.md", "author": "LlamaIndex"}, "hash": "5a7c5bd3f1a37a82cc05e2af423ecce6a26b34a9d4dc784c5703d5438a5deefe"}}, "hash": "98ebcefcffdda14107d710ac3b028441d05a811adf6ecaee84d26176b2f09000", "text": "# Cost Analysis\n\n## Concept\nEach call to an LLM will cost some amount of money - for instance, OpenAI's gpt-3.5-turbo costs $0.002 / 1k tokens.The cost of building an index and querying depends on \n\n- the type of LLM used\n- the type of data structure used\n- parameters used during building \n- parameters used during querying\n\nThe cost of building and querying each index is a TODO in the reference documentation.In the meantime, we provide the following information:\n\n1.A high-level overview of the cost structure of the indices.2.A token predictor that you can use directly within LlamaIndex!### Overview of Cost Structure\n\n#### Indices with no LLM calls\nThe following indices don't require LLM calls at all during building (0 cost):\n- `SummaryIndex`\n- `SimpleKeywordTableIndex` - uses a regex keyword extractor to extract keywords from each document\n- `RAKEKeywordTableIndex` - uses a RAKE keyword extractor to extract keywords from each document\n\n#### Indices with LLM calls\nThe following indices do require LLM calls during build time:\n- `TreeIndex` - use LLM to hierarchically summarize the text to build the tree\n- `KeywordTableIndex` - use LLM to extract keywords from each document\n\n### Query Time\n\nThere will always be >= 1 LLM call during query time, in order to synthesize the final answer.Some indices contain cost tradeoffs between index building and querying.`SummaryIndex`, for instance,\nis free to build, but running a query over a summary index (without filtering or embedding lookups), will\ncall the LLM {math}`N` times.Here are some notes regarding each of the indices:\n- `SummaryIndex`: by default requires {math}`N` LLM calls, where N is the number of nodes.- `TreeIndex`: by default requires {math}`\\log (N)` LLM calls, where N is the number of leaf nodes.- Setting `child_branch_factor=2` will be more expensive than the default `child_branch_factor=1` (polynomial vs logarithmic), because we traverse 2 children instead of just 1 for each parent node.- `KeywordTableIndex`: by default requires an LLM call to extract query keywords.- Can do `index.as_retriever(retriever_mode=\"simple\")` or `index.as_retriever(retriever_mode=\"rake\")` to also use regex/RAKE keyword extractors on your query text.-  `VectorStoreIndex`: by default, requires one LLM call per query.If you increase the `similarity_top_k` or `chunk_size`, or change the `response_mode`, then this number will increase.## Usage Pattern\n\nLlamaIndex offers token **predictors** to predict token usage of LLM and embedding calls.This allows you to estimate your costs during 1) index construction, and 2) index querying, before\nany respective LLM calls are made.Tokens are counted using the `TokenCountingHandler` callback.See the [example notebook](../../../examples/callbacks/TokenCountingHandler.ipynb) for details on the setup.### Using MockLLM\n\nTo predict token usage of LLM calls, import and instantiate the MockLLM as shown below.The `max_tokens` parameter is used as a \"worst case\" prediction, where each LLM response will contain exactly that number of tokens.If `max_tokens` is not specified, then it will simply predict back the prompt.```python\nfrom llama_index import ServiceContext, set_global_service_context\nfrom llama_index.llms import MockLLM\n\nllm = MockLLM(max_tokens=256)\n\nservice_context = ServiceContext.from_defaults(llm=llm)\n\n# optionally set a global service context\nset_global_service_context(service_context)\n```\n\nYou can then use this predictor during both index construction and querying.### Using MockEmbedding\n\nYou may also predict the token usage of embedding calls with `MockEmbedding`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "96252df0-81a0-4037-bb4f-3fdc546ef84d": {"__data__": {"id_": "96252df0-81a0-4037-bb4f-3fdc546ef84d", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f23a92cc6de0d269bc901bee4a5e97b8891a9e5", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\root.md", "author": "LlamaIndex"}, "hash": "13431fae5193249af970f34b8d53ffacb1abeae3e497c244289905e297e9abbd"}, "2": {"node_id": "0696e8c1-c965-456c-a011-9ef3e297b71a", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\root.md", "author": "LlamaIndex"}, "hash": "98ebcefcffdda14107d710ac3b028441d05a811adf6ecaee84d26176b2f09000"}}, "hash": "5a7c5bd3f1a37a82cc05e2af423ecce6a26b34a9d4dc784c5703d5438a5deefe", "text": "```python\nfrom llama_index import ServiceContext, set_global_service_context\nfrom llama_index import MockEmbedding\n\n# specify a MockLLMPredictor\nembed_model = MockEmbedding(embed_dim=1536)\n\nservice_context = ServiceContext.from_defaults(embed_model=embed_model)\n\n# optionally set a global service context\nset_global_service_context(service_context)\n```\n\n## Usage Pattern\n\nRead about the full usage pattern below!```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\nusage_pattern.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c3288a8a-9ee9-4db9-b48b-1d5a7d091913": {"__data__": {"id_": "c3288a8a-9ee9-4db9-b48b-1d5a7d091913", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "145c0d35c448a994b56de44800293d21f344c849", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "f36caea2f3ffb530b897742a114cb42a08eb1d5e0ba4be2e2d2f77fc2d11d3a9"}}, "hash": "404a31719092827e17badea9673445eb2a29662ef5f6daafee8d5b4b23c5722c", "text": "# Usage Pattern\n\n## Estimating LLM and Embedding Token Counts\n\nIn order to measure LLM and Embedding token counts, you'll need to\n\n1. Setup `MockLLM` and `MockEmbedding` objects\n\n```python\nfrom llama_index.llms import MockLLM\nfrom llama_index import MockEmbedding\n\nllm = MockLLM(max_tokens=256)\nembed_model = MockEmbedding(embed_dim=1536)\n```\n\n2. Setup the `TokenCountingCallback` handler\n\n```python\nimport tiktoken\nfrom llama_index.callbacks import CallbackManager, TokenCountingHandler\n\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n)\n\ncallback_manager = CallbackManager([token_counter])\n```\n\n3. Add them to the global `ServiceContext`\n\n```python\nfrom llama_index import ServiceContext, set_global_service_context\n\nset_global_service_context(\n    ServiceContext.from_defaults(\n        llm=llm, \n        embed_model=embed_model, \n        callback_manager=callback_manager\n    )\n)\n```\n\n4. Construct an Index \n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./docs/examples/data/paul_graham\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n5. Measure the counts!\n\n```python\nprint(\n    \"Embedding Tokens: \",\n    token_counter.total_embedding_token_count,\n    \"\\n\",\n    \"LLM Prompt Tokens: \",\n    token_counter.prompt_llm_token_count,\n    \"\\n\",\n    \"LLM Completion Tokens: \",\n    token_counter.completion_llm_token_count,\n    \"\\n\",\n    \"Total LLM Token Count: \",\n    token_counter.total_llm_token_count,\n    \"\\n\",\n)\n\n# reset counts\ntoken_counter.reset_counts()\n```\n\n6. Run a query, mesaure again\n\n```python\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"query\")\n\nprint(\n    \"Embedding Tokens: \",\n    token_counter.total_embedding_token_count,\n    \"\\n\",\n    \"LLM Prompt Tokens: \",\n    token_counter.prompt_llm_token_count,\n    \"\\n\",\n    \"LLM Completion Tokens: \",\n    token_counter.completion_llm_token_count,\n    \"\\n\",\n    \"Total LLM Token Count: \",\n    token_counter.total_llm_token_count,\n    \"\\n\",\n)\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "948fa402-50f5-47b4-bbf9-cfb44fb9b649": {"__data__": {"id_": "948fa402-50f5-47b4-bbf9-cfb44fb9b649", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\modules.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7fe5150420891da3c4bef00958f002d61217ae1e", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\modules.md", "author": "LlamaIndex"}, "hash": "e959c9737e4502d5150b9b75d38c5a2aaea1197d40c20d591ff2a064e1f4e6a7"}}, "hash": "2e7c203423eb24339587efba0b603233f6014cbfc4e527be04e201929d7f3460", "text": "# Modules\n\nNotebooks with usage of these components can be found below.\n\n```{toctree}\n---\nmaxdepth: 1\n---\n\n../../../examples/evaluation/TestNYC-Evaluation.ipynb\n../../../examples/evaluation/TestNYC-Evaluation-Query.ipynb\n../../../examples/evaluation/QuestionGeneration.ipynb\n../../../examples/evaluation/Deepeval.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0dbc1fbc-a574-4dd3-b0a0-df32bd0218b0": {"__data__": {"id_": "0dbc1fbc-a574-4dd3-b0a0-df32bd0218b0", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a196c68d6f3e5add99049b21f34ff4c0314eba73", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\root.md", "author": "LlamaIndex"}, "hash": "6f0483ca1afdf0c07b516e47712aecf8c51fee99b8153a74e9279a68da2b5b9c"}}, "hash": "e57b66a64e44aefdff1d96e1740807653dc898feb4aea3369e048a59d796796f", "text": "(evaluation)=\n# Evaluation\n\n## Concept\nEvaluation in generative AI and retrieval is a difficult task. Due to the unpredictable nature of text, and a general lack of \"expected\" outcomes to compare against, there are many blockers to getting started with evaluation.\n\nHowever, LlamaIndex offers a few key modules for evaluating the quality of both Document retrieval and response synthesis.\nHere are some key questions for each component:\n\n- **Document retrieval**: Are the sources relevant to the query?\n- **Response synthesis**: Does the response match the retrieved context? Does it also match the query? \n\nThis guide describes how the evaluation components within LlamaIndex work. Note that our current evaluation modules\ndo *not* require ground-truth labels. Evaluation can be done with some combination of the query, context, response,\nand combine these with LLM calls.\n\n### Evaluation of the Response + Context\n\nEach response from a `query_engine.query` calls returns both the synthesized response as well as source documents.\n\nWe can evaluate the response against the retrieved sources - without taking into account the query!\n\nThis allows you to measure hallucination - if the response does not match the retrieved sources, this means that the model may be \"hallucinating\" an answer since it is not rooting the answer in the context provided to it in the prompt.\n\nThere are two sub-modes of evaluation here. We can either get a binary response \"YES\"/\"NO\" on whether response matches *any* source context,\nand also get a response list across sources to see which sources match.\n\nThe `ResponseEvaluator` handles both modes for evaluating in this context.\n\n### Evaluation of the Query + Response + Source Context\n\nThis is similar to the above section, except now we also take into account the query. The goal is to determine if\nthe response + source context answers the query.\n\nAs with the above, there are two submodes of evaluation. \n- We can either get a binary response \"YES\"/\"NO\" on whether\nthe response matches the query, and whether any source node also matches the query.\n- We can also ignore the synthesized response, and check every source node to see\nif it matches the query.\n\n### Question Generation\n\nIn addition to evaluating queries, LlamaIndex can also use your data to generate questions to evaluate on. This means that you can automatically generate questions, and then run an evaluation pipeline to test if the LLM can actually answer questions accurately using your data.\n\n## Integrations\n\nWe also integrate with community evaluation tools.\n\n- [DeepEval](../../../community/integrations/deepeval.md)\n- [Ragas](https://github.com/explodinggradients/ragas/blob/main/docs/integrations/llamaindex.ipynb)\n\n## Usage Pattern\n\nFor full usage details, see the usage pattern below.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nusage_pattern.md\n```\n\n## Modules\n\nNotebooks with usage of these components can be found below.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nmodules.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "127950f3-614b-4ab1-aa62-17af686c4039": {"__data__": {"id_": "127950f3-614b-4ab1-aa62-17af686c4039", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a6836444066c29794155c38d9b80271c4f86a6c", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "2add21964577494cbcf91e44015d00a04e7c0642ee08bf0483d57f87108f9ba7"}, "3": {"node_id": "a495af7c-c87a-496b-a868-5b8f30953f42", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "13e9c0ec3f7cb21beb852f3ab1dac572bf9524ad655eaf4dea5e44339dc89da1"}}, "hash": "135d25875613bd046e01e8d809b5c8506c7301f0ccd6ebea2a23af7e02c16c1c", "text": "# Usage Pattern\n\n## Evaluating Response for Hallucination\n\n### Binary Evaluation\n\nThis mode of evaluation will return \"YES\"/\"NO\" if the synthesized response matches any source context.```python\nfrom llama_index import VectorStoreIndex, ServiceContext\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import ResponseEvaluator\n\n# build service context\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\n# build index\n...\n\n# define evaluator\nevaluator = ResponseEvaluator(service_context=service_context)\n\n# query index\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")eval_result = evaluator.evaluate(response)\nprint(str(eval_result))\n\n```\n\nYou'll get back either a `YES` or `NO` response.![](/_static/evaluation/eval_response_context.png)\n\n### Sources Evaluation\n\nThis mode of evaluation will return \"YES\"/\"NO\" for every source node.```python\nfrom llama_index import VectorStoreIndex, ServiceContext\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import ResponseEvaluator\n\n# build service context\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\n# build index\n...\n\n# define evaluator\nevaluator = ResponseEvaluator(service_context=service_context)\n\n# query index\nquery_engine = vector_index.as_query_engine()\nresponse = query_engine.query(\"What battles took place in New York City in the American Revolution?\")eval_result = evaluator.evaluate_source_nodes(response)\nprint(str(eval_result))\n\n```\n\nYou'll get back a list of \"YES\"/\"NO\", corresponding to each source node in `response.source_nodes`.## Evaluting Query + Response for Answer Quality\n\n### Binary Evaluation\n\nThis mode of evaluation will return \"YES\"/\"NO\" if the synthesized response matches the query + any source context.```python\nfrom llama_index import VectorStoreIndex, ServiceContext\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import QueryResponseEvaluator\n\n# build service context\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\n# build index\n...\n\n# define evaluator\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\n# query index\nquery_engine = vector_index.as_query_engine()\nquery = \"What battles took place in New York City in the American Revolution?\"response = query_engine.query(query)\neval_result = evaluator.evaluate(query, response)\nprint(str(eval_result))\n\n```\n\n![](/_static/evaluation/eval_query_response_context.png)\n\n### Sources Evaluation\n\nThis mode of evaluation will look at each source node, and see if each source node contains an answer to the query.```python\nfrom llama_index import VectorStoreIndex, ServiceContext\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import QueryResponseEvaluator\n\n# build service context\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\n# build index\n...\n\n# define evaluator\nevaluator = QueryResponseEvaluator(service_context=service_context)\n\n# query index\nquery_engine = vector_index.as_query_engine()\nquery = \"What battles took place in New York City in the American Revolution?\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a495af7c-c87a-496b-a868-5b8f30953f42": {"__data__": {"id_": "a495af7c-c87a-496b-a868-5b8f30953f42", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a6836444066c29794155c38d9b80271c4f86a6c", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "2add21964577494cbcf91e44015d00a04e7c0642ee08bf0483d57f87108f9ba7"}, "2": {"node_id": "127950f3-614b-4ab1-aa62-17af686c4039", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "135d25875613bd046e01e8d809b5c8506c7301f0ccd6ebea2a23af7e02c16c1c"}}, "hash": "13e9c0ec3f7cb21beb852f3ab1dac572bf9524ad655eaf4dea5e44339dc89da1", "text": "response = query_engine.query(query)\neval_result = evaluator.evaluate_source_nodes(query, response)\nprint(str(eval_result))\n```\n\n![](/_static/evaluation/eval_query_sources.png)\n\n## Question Generation\n\nLlamaIndex can also generate questions to answer using your data.Using in combination with the above evaluators, you can create a fully automated evaluation pipeline over your data.```python\nfrom llama_index import SimpleDirectoryReader, ServiceContext\nfrom llama_index.llms import OpenAI\nfrom llama_index.evaluation import ResponseEvaluator\n\n# build service context\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\nservice_context = ServiceContext.from_defaults(llm=llm)\n\n# build documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# define genertor, generate questions\ndata_generator = DatasetGenerator.from_documents(documents)\n\neval_questions = data_generator.generate_questions_from_nodes()\n```\n\n## Integrations\n\nWe also integrate with community evaluation tools.- [DeepEval](../../../community/integrations/deepeval.md)\n- [Ragas](https://github.com/explodinggradients/ragas/blob/main/docs/integrations/llamaindex.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d75a6a73-65df-4a57-96bf-41bf1d77ab17": {"__data__": {"id_": "d75a6a73-65df-4a57-96bf-41bf1d77ab17", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\playground\\root.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a13b8e777811d7031e201f14669e7caf0f6bb66f", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\playground\\root.md", "author": "LlamaIndex"}, "hash": "272f77687988f40b715b3d493c43c27ba3f2aea744f363380d829141212b04df"}}, "hash": "c960715ccd733b8c265467ee42251f72d5a4211c6200367c97ea8f0720686f33", "text": "# Playground\n\n## Concept\n\nThe Playground module in LlamaIndex is a way to automatically test your data (i.e. documents) across a diverse combination of indices, models, embeddings, modes, etc. to decide which ones are best for your purposes. More options will continue to be added.\n\nFor each combination, you'll be able to compare the results for any query and compare the answers, latency, tokens used, and so on.\n\nYou may initialize a Playground with a list of pre-built indices, or initialize one from a list of Documents using the preset indices.\n\n## Usage Pattern\n\nA sample usage is given below.\n\n```python\nfrom llama_index import download_loader\nfrom llama_index.indices.vector_store import VectorStoreIndex\nfrom llama_index.indices.tree.base import TreeIndex\nfrom llama_index.playground import Playground\n\n# load data \nWikipediaReader = download_loader(\"WikipediaReader\")\nloader = WikipediaReader()\ndocuments = loader.load_data(pages=['Berlin'])\n\n# define multiple index data structures (vector index, summary index)\nindices = [VectorStoreIndex(documents), TreeIndex(documents)]\n\n# initialize playground\nplayground = Playground(indices=indices)\n\n# playground compare\nplayground.compare(\"What is the population of Berlin?\")\n\n```\n\n## Modules\n\n```{toctree}\n---\nmaxdepth: 1\n---\n../../../examples/analysis/PlaygroundDemo.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8c21c0d0-cf6b-4a03-80c6-66f9bb5c153b": {"__data__": {"id_": "8c21c0d0-cf6b-4a03-80c6-66f9bb5c153b", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\service_context.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44dba472d4169443c3401d14c5631465eab0725f", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\service_context.md", "author": "LlamaIndex"}, "hash": "a97d880f18116a3b6b2a368493d55767724a59aa586921405eff94d1e1749c5d"}, "3": {"node_id": "3e553d2d-28b3-4576-bd6a-e4833517ee4b", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\service_context.md", "author": "LlamaIndex"}, "hash": "cea98ec4625ad2ec5438ecbca5948aebb90d33480d61d0ce892ff8e93cbebaaf"}}, "hash": "0f061e4c66b6baea9909eb681e9c49e60021fffdcbb1a9ae6f1908a7e56b644d", "text": "# ServiceContext\n\n## Concept\nThe `ServiceContext` is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application.You can use it to set the [global configuration](#setting-global-configuration), as well as [local configurations](#setting-local-configuration) at specific parts of the pipeline.## Usage Pattern\n\n### Configuring the service context\nThe `ServiceContext` is a simple python dataclass that you can directly construct by passing in the desired components.```python\n@dataclass\nclass ServiceContext:\n    # The LLM used to generate natural language responses to queries.# If not provided, defaults to gpt-3.5-turbo from OpenAI\n    # If your OpenAI key is not set, defaults to llama2-chat-13B from Llama.cpp\n    llm: LLM\n\n    # The PromptHelper object that helps with truncating and repacking text chunks to fit in the LLM's context window.prompt_helper: PromptHelper\n\n    # The embedding model used to generate vector representations of text.# If not provided, defaults to text-embedding-ada-002\n    # If your OpenAI key is not set, defaults to BAAI/bge-small-en\n    embed_model: BaseEmbedding\n\n    # The parser that converts documents into nodes.node_parser: NodeParser\n\n    # The callback manager object that calls it's handlers on events.Provides basic logging and tracing capabilities.callback_manager: CallbackManager\n\n    @classmethod\n    def from_defaults(cls, ...) -> \"ServiceContext\":\n      ... \n```\n\n```{tip}\nLearn how to configure specific modules:\n- [LLM](/core_modules/model_modules/llms/usage_custom.md)\n- [Embedding Model](/core_modules/model_modules/embeddings/usage_pattern.md)\n- [Node Parser](/core_modules/data_modules/node_parsers/usage_pattern.md)\n\n```\n\nWe also expose some common kwargs (of the above components) via the `ServiceContext.from_defaults` method\nfor convenience (so you don't have to manually construct them).**Kwargs for node parser**:\n- `chunk_size`: The size of the text chunk for a node .Is used for the node parser when they aren't provided.- `chunk_overlap`: The amount of overlap between nodes (i.e.text chunks).**Kwargs for prompt helper**:\n- `context_window`: The size of the context window of the LLM.Typically we set this \n  automatically with the model metadata.But we also allow explicit override via this parameter\n  for additional control (or in case the default is not available for certain latest\n  models)\n- `num_output`: The number of maximum output from the LLM.Typically we set this\n  automatically given the model metadata.This parameter does not actually limit the model\n  output, it affects the amount of \"space\" we save for the output, when computing \n  available context window size for packing text from retrieved Nodes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3e553d2d-28b3-4576-bd6a-e4833517ee4b": {"__data__": {"id_": "3e553d2d-28b3-4576-bd6a-e4833517ee4b", "embedding": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\service_context.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44dba472d4169443c3401d14c5631465eab0725f", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\service_context.md", "author": "LlamaIndex"}, "hash": "a97d880f18116a3b6b2a368493d55767724a59aa586921405eff94d1e1749c5d"}, "2": {"node_id": "8c21c0d0-cf6b-4a03-80c6-66f9bb5c153b", "node_type": null, "metadata": {"filename": "docs\\core_modules\\supporting_modules\\service_context.md", "author": "LlamaIndex"}, "hash": "0f061e4c66b6baea9909eb681e9c49e60021fffdcbb1a9ae6f1908a7e56b644d"}}, "hash": "cea98ec4625ad2ec5438ecbca5948aebb90d33480d61d0ce892ff8e93cbebaaf", "text": "Here's a complete example that sets up all objects using their default settings:\n\n```python\nfrom llama_index import ServiceContext, LLMPredictor, OpenAIEmbedding, PromptHelper\nfrom llama_index.llms import OpenAI\nfrom llama_index.text_splitter import TokenTextSplitter\nfrom llama_index.node_parser import SimpleNodeParser\n\nllm = OpenAI(model='text-davinci-003', temperature=0, max_tokens=256)\nembed_model = OpenAIEmbedding()\nnode_parser = SimpleNodeParser.from_defaults(\n  text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n)\nprompt_helper = PromptHelper(\n  context_window=4096, \n  num_output=256, \n  chunk_overlap_ratio=0.1, \n  chunk_size_limit=None\n)\n\nservice_context = ServiceContext.from_defaults(\n  llm=llm,\n  embed_model=embed_model,\n  node_parser=node_parser,\n  prompt_helper=prompt_helper\n)\n```\n\n### Setting global configuration\nYou can set a service context as the global default that applies to the entire LlamaIndex pipeline:\n\n```python\nfrom llama_index import set_global_service_context\nset_global_service_context(service_context)\n```\n\n### Setting local configuration\nYou can pass in a service context to specific part of the pipeline to override the default configuration: \n\n```python\nquery_engine = index.as_query_engine(service_context=service_context)\nresponse = query_engine.query(\"What did the author do growing up?\")print(response)\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "290972af-dd34-49b7-8c81-43d512b3d500": {"__data__": {"id_": "290972af-dd34-49b7-8c81-43d512b3d500", "embedding": null, "metadata": {"filename": "docs\\deprecated_terms.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46bf7a7099a0456a4f30cb6bcb5ac0ce59bac8aa", "node_type": null, "metadata": {"filename": "docs\\deprecated_terms.md", "author": "LlamaIndex"}, "hash": "c26cf4ddf143eb03924517820ca92683cf8783a2fe7471cc8603534950990e60"}}, "hash": "8d3f13c03446e27b399655d112b6bf5e24203303e81d5d96433904f38c4b0f27", "text": "# Deprecated Terms\n\nAs LlamaIndex continues to evolve, many class names and APIs have been adjusted, improved, and deprecated.\n\nThe following is a list of previously popular terms that have been deprecated, with links to their replacements.\n\n## GPTSimpleVectorIndex\n\nThis has been renamed to `VectorStoreIndex`, as well as unifying all vector indexes to a single unified interface. You can integrate with various vector databases by modifying the underlying `vector_store`. \n\nPlease see the following links for more details on usage.\n\n- [Index Usage Pattern](/core_modules/data_modules/index/usage_pattern.md)\n- [Vector Store Guide](/core_modules/data_modules/index/vector_store_guide.ipynb)\n- [Vector Store Integrations](/community/integrations/vector_stores.md)\n\n## GPTVectorStoreIndex\n\nThis has been renamed to `VectorStoreIndex`, but it is only a cosmetic change. Please see the following links for more details on usage.\n\n- [Index Usage Pattern](/core_modules/data_modules/index/usage_pattern.md)\n- [Vector Store Guide](/core_modules/data_modules/index/vector_store_guide.ipynb)\n- [Vector Store Integrations](/community/integrations/vector_stores.md)\n\n## LLMPredictor\n\nThe `LLMPredictor` object is no longer intended to be used by users. Instead, you can setup an LLM directly and pass it into the `ServiceContext`.\n\n- [LLMs in LlamaIndex](/core_modules/model_modules/llms/root.md)\n- [Setting LLMs in the ServiceContext](/core_modules/supporting_modules/service_context.md)\n\n## PromptHelper and max_input_size/\n\nThe `max_input_size` parameter for the prompt helper has since been replaced with `context_window`.\n\nThe `PromptHelper` in general has been deprecated in favour of specifying parameters directly in the `service_context` and `node_parser`.\n\nSee the following links for more details.\n\n- [Configuring settings in the Service Context](/core_modules/supporting_modules/service_context.md)\n- [Parsing Documents into Nodes](/core_modules/data_modules/node_parsers/root.md)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d9f52f29-8ac7-40e5-81e6-b4e1cf135561": {"__data__": {"id_": "d9f52f29-8ac7-40e5-81e6-b4e1cf135561", "embedding": null, "metadata": {"filename": "docs\\development\\privacy.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5777d9a1f01e0f708202499624b34da6c33300a5", "node_type": null, "metadata": {"filename": "docs\\development\\privacy.md", "author": "LlamaIndex"}, "hash": "d372cbab0c5a5f055f189288407e7fd426707f2e8337de739c90fffe71080319"}}, "hash": "b7b64ad1aead9e30066ded87c32b28a0b7968900a12c314f05422512e856cb72", "text": "# Privacy and Security  \nBy default, LLamaIndex sends your data to OpenAI for generating embeddings and natural language responses. However, it is important to note that this can be configured according to your preferences. LLamaIndex provides the flexibility to use your own embedding model or run a large language model locally if desired.\n\n## Data Privacy\nRegarding data privacy, when using LLamaIndex with OpenAI, the privacy details and handling of your data are subject to OpenAI's policies. And each custom service other than OpenAI have their own policies as well.\n\n## Vector stores\nLLamaIndex offers modules to connect with other vector stores within indexes to store embeddings. It is worth noting that each vector store has its own privacy policies and practices, and LLamaIndex does not assume responsibility for how they handle or use your data. Also by default LLamaIndex have a default option to store your embeddings locally.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6558c03c-1f31-416b-9f95-f9a8e51c90f8": {"__data__": {"id_": "6558c03c-1f31-416b-9f95-f9a8e51c90f8", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\agents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "728ccebf2fce50b57bf06af7f576fcf7aaf61e19", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\agents.md", "author": "LlamaIndex"}, "hash": "2747bee09fb69dab1d6dee14888c35cd128b5c06c95bdd29bd4cd8cffdfa6ffb"}, "3": {"node_id": "30cc6acc-df1d-4249-be11-3c1e90cbc9a9", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\agents.md", "author": "LlamaIndex"}, "hash": "e2df54bcff160038c82fbe1f7a6f925a7aeead515ec48fdeeb6027b6d910dfe0"}}, "hash": "0072dae9f054e195140bf3f1fff2c58b811cd87b56d47d08fcfab4f9d019791f", "text": "# Agents\n\n## Context\nAn \"agent\" is an automated reasoning and decision engine. It takes in a user input/query and can make internal decisions for executing\nthat query in order to return the correct result. The key agent components can include, but are not limited to:\n- Breaking down a complex question into smaller ones\n- Choosing an external Tool to use + coming up with parameters for calling the Tool\n- Planning out a set of tasks\n- Storing previously completed tasks in a memory module\n\nResearch developments in LLMs (e.g. [ChatGPT Plugins](https://openai.com/blog/chatgpt-plugins)), LLM research ([ReAct](https://arxiv.org/abs/2210.03629), [Toolformer](https://arxiv.org/abs/2302.04761)) and LLM tooling ([LangChain](https://python.langchain.com/en/latest/modules/agents.html), [Semantic Kernel](https://github.com/microsoft/semantic-kernel)) have popularized the concept of agents.\n\n\n\n## Agents + LlamaIndex\n\nLlamaIndex provides some amazing tools to manage and interact with your data within your LLM application. And it can be a core tool that you use while building an agent-based app.\n- On one hand, some components within LlamaIndex are \"agent-like\" - these make automated decisions to help a particular use case over your data.\n- On the other hand, LlamaIndex can be used as a core Tool within another agent framework.\n\nIn general, LlamaIndex components offer more explicit, constrained behavior for more specific use cases. Agent frameworks such as ReAct (implemented in LangChain) offer agents that are more unconstrained + \ncapable of general reasoning. \n\nThere are tradeoffs for using both - less-capable LLMs typically do better with more constraints. Take a look at [our blog post on this](https://medium.com/llamaindex-blog/dumber-llm-agents-need-more-constraints-and-better-tools-17a524c59e12) for \na more information + a detailed analysis.\n\n\n### \"Agent-like\" Components within LlamaIndex \n\nLlamaIndex provides core modules capable of automated reasoning for different use cases over your data. Please check out our [use cases doc](/end_to_end_tutorials/use_cases.md) for more details on high-level use cases that LlamaIndex can help fulfill.\n\nSome of these core modules are shown below along with example tutorials (not comprehensive, please click into the guides/how-tos for more details).\n\n**SubQuestionQueryEngine for Multi-Document Analysis**\n- [Usage](queries.md#multi-document-queries)\n- [Sub Question Query Engine (Intro)](/examples/query_engine/sub_question_query_engine.ipynb)\n- [10Q Analysis (Uber)](/examples/usecases/10q_sub_question.ipynb)\n- [10K Analysis (Uber and Lyft)](/examples/usecases/10k_sub_question.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "30cc6acc-df1d-4249-be11-3c1e90cbc9a9": {"__data__": {"id_": "30cc6acc-df1d-4249-be11-3c1e90cbc9a9", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\agents.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "728ccebf2fce50b57bf06af7f576fcf7aaf61e19", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\agents.md", "author": "LlamaIndex"}, "hash": "2747bee09fb69dab1d6dee14888c35cd128b5c06c95bdd29bd4cd8cffdfa6ffb"}, "2": {"node_id": "6558c03c-1f31-416b-9f95-f9a8e51c90f8", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\agents.md", "author": "LlamaIndex"}, "hash": "0072dae9f054e195140bf3f1fff2c58b811cd87b56d47d08fcfab4f9d019791f"}}, "hash": "e2df54bcff160038c82fbe1f7a6f925a7aeead515ec48fdeeb6027b6d910dfe0", "text": "**Query Transformations**\n- [How-To](/core_modules/query_modules/query_engine/advanced/query_transformations.md)\n- [Multi-Step Query Decomposition](/examples/query_transformations/HyDEQueryTransformDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb))\n\n**Routing**\n- [Usage](queries.md#routing-over-heterogeneous-data)\n- [Router Query Engine Guide](/examples/query_engine/RouterQueryEngine.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_engine/RouterQueryEngine.ipynb))\n\n**LLM Reranking**\n- [Second Stage Processing How-To](/core_modules/query_modules/node_postprocessors/root.md)\n- [LLM Reranking Guide (Great Gatsby)](/examples/node_postprocessor/LLMReranker-Gatsby.ipynb)\n\n**Chat Engines**\n- [Chat Engines How-To](/core_modules/query_modules/chat_engines/root.md)\n\n\n### Using LlamaIndex as as Tool within an Agent Framework\n\nLlamaIndex can be used as as Tool within an agent framework - including LangChain, ChatGPT. These integrations are described below.\n\n#### LangChain\n\nWe have deep integrations with LangChain. \nLlamaIndex query engines can be easily packaged as Tools to be used within a LangChain agent, and LlamaIndex can also be used as a memory module / retriever. Check out our guides/tutorials below!\n\n**Resources**\n- [LangChain integration guide](/community/integrations/using_with_langchain.md)\n- [Building a Chatbot Tutorial (LangChain + LlamaIndex)](/guides/tutorials/building_a_chatbot.md)\n- [OnDemandLoaderTool Tutorial](/examples/tools/OnDemandLoaderTool.ipynb)\n\n#### ChatGPT\n\nLlamaIndex can be used as a ChatGPT retrieval plugin (we have a TODO to develop a more general plugin as well).\n\n**Resources**\n- [LlamaIndex ChatGPT Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin#llamaindex)\n\n\n### Native OpenAIAgent\n\nWith the [new OpenAI API](https://openai.com/blog/function-calling-and-other-api-updates) that supports function calling, it\u2019s never been easier to build your own agent!\n\nLearn how to write your own OpenAI agent in **under 50 lines of code**, or directly use our super simple\n`OpenAIAgent` implementation.\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/agent/openai_agent.ipynb\n/examples/agent/openai_agent_with_query_engine.ipynb\n/examples/agent/openai_agent_retrieval.ipynb\n/examples/agent/openai_agent_query_cookbook.ipynb\n/examples/agent/openai_agent_query_plan.ipynb\n/examples/agent/openai_agent_context_retrieval.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d0afa067-1613-42a3-a67d-d440ad8901b2": {"__data__": {"id_": "d0afa067-1613-42a3-a67d-d440ad8901b2", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61b1a0d2bcdf1ffef5187fde6c0e0ae4450e6db4", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps.md", "author": "LlamaIndex"}, "hash": "36a4b61b89f7ef71c87299c0e624a7eacce32ec148f5200d2cfe997b6941d86b"}}, "hash": "77ee8d4a95928507632d3b2e6fbdb25942b52b308f15153264f5b9b1eff64b8a", "text": "# Full-Stack Web Application\n\nLlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit.\n\nWe provide tutorials and resources to help you get started in this area.\n\nRelevant Resources:\n- [Fullstack Application Guide](/end_to_end_tutorials/apps/fullstack_app_guide.md)\n- [Fullstack Application with Delphic](/end_to_end_tutorials/apps/fullstack_with_delphic.md)\n- [A Guide to Extracting Terms and Definitions](/end_to_end_tutorials/question_and_answer/terms_definitions_tutorial.md)\n- [LlamaIndex Starter Pack](https://github.com/logan-markewich/llama_index_starter_pack)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e319874d-fc82-4b5a-a288-52af9c990a82": {"__data__": {"id_": "e319874d-fc82-4b5a-a288-52af9c990a82", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "818302b819cd3e0ea2b753e807a2c0c67cb01ed6433cec7ceb016e398e602399"}, "3": {"node_id": "81d03ce7-6627-45a7-8a9f-66374ec48eb0", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "35d69d0ac8405fae54303f8d65863e65be1d6ac2d2486785623e2391363bf2f6"}}, "hash": "bae9d00b3c1da0b24a201c7443a371ce99dbd6b07b75538992b59cf17c4e787b", "text": "# A Guide to Building a Full-Stack Web App with LLamaIndex\r\n\r\nLlamaIndex is a python library, which means that integrating it with a full-stack web application will be a little different than what you might be used to.This guide seeks to walk through the steps needed to create a basic API service written in python, and how this interacts with a TypeScript+React frontend.All code examples here are available from the [llama_index_starter_pack](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react) in the flask_react folder.The main technologies used in this guide are as follows:\r\n\r\n- python3.11\r\n- llama_index\r\n- flask\r\n- typescript\r\n- react\r\n\r\n## Flask Backend\r\n\r\nFor this guide, our backend will use a [Flask](https://flask.palletsprojects.com/en/2.2.x/) API server to communicate with our frontend code.If you prefer, you can also easily translate this to a [FastAPI](https://fastapi.tiangolo.com/) server, or any other python server library of your choice.Setting up a server using Flask is easy.You import the package, create the app object, and then create your endpoints.Let's create a basic skeleton for the server first:\r\n\r\n```python\r\nfrom flask import Flask\r\n\r\napp = Flask(__name__)\r\n\r\n@app.route(\"/\")\r\ndef home():\r\n    return \"Hello World!\"if __name__ == \"__main__\":\r\n    app.run(host=\"0.0.0.0\", port=5601)\r\n```\r\n\r\n_flask_demo.py_\r\n\r\nIf you run this file (`python flask_demo.py`), it will launch a server on port 5601.If you visit `http://localhost:5601/`, you will see the \"Hello World!\"text rendered in your browser.Nice!The next step is deciding what functions we want to include in our server, and to start using LlamaIndex.To keep things simple, the most basic operation we can provide is querying an existing index.Using the [paul graham essay](https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/data/paul_graham_essay.txt) from LlamaIndex, create a documents folder and download+place the essay text file inside of it.### Basic Flask - Handling User Index Queries\r\n\r\nNow, let's write some code to initialize our index:\r\n\r\n```python\r\nimport os\r\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, StorageContext\r\n\r\n# NOTE: for local testing only, do NOT deploy with your key hardcoded\r\nos.environ['OPENAI_API_KEY'] = \"your key here\"\r\n\r\nindex = None\r\n\r\ndef initialize_index():\r\n    global index\r\n    storage_context = StorageContext.from_defaults()\r\n    if os.path.exists(index_dir):\r\n        index = load_index_from_storage(storage_context)\r\n    else:\r\n        documents = SimpleDirectoryReader(\"./documents\").load_data()\r\n        index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\r\n        storage_context.persist(index_dir)\r\n```\r\n\r\nThis function will initialize our index.If we call this just before starting the flask server in the `main` function, then our index will be ready for user queries!Our query endpoint will accept `GET` requests with the query text as a parameter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "81d03ce7-6627-45a7-8a9f-66374ec48eb0": {"__data__": {"id_": "81d03ce7-6627-45a7-8a9f-66374ec48eb0", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "818302b819cd3e0ea2b753e807a2c0c67cb01ed6433cec7ceb016e398e602399"}, "2": {"node_id": "e319874d-fc82-4b5a-a288-52af9c990a82", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "bae9d00b3c1da0b24a201c7443a371ce99dbd6b07b75538992b59cf17c4e787b"}, "3": {"node_id": "c27b9bc8-c421-4da3-8b0b-2a72343235a9", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "9f80e5cc8dbfe224cab229fe9316d4043a20eb5f69540f0593c71e294802b363"}}, "hash": "35d69d0ac8405fae54303f8d65863e65be1d6ac2d2486785623e2391363bf2f6", "text": "Here's what the full endpoint function will look like:\r\n\r\n```python\r\nfrom flask import request\r\n\r\n@app.route(\"/query\", methods=[\"GET\"])\r\ndef query_index():\r\n  global index\r\n  query_text = request.args.get(\"text\", None)\r\n  if query_text is None:\r\n    return \"No text found, please include a ?text=blah parameter in the URL\", 400\r\n  query_engine = index.as_query_engine()\r\n  response = query_engine.query(query_text)\r\n  return str(response), 200\r\n```\r\n\r\nNow, we've introduced a few new concepts to our server:\r\n\r\n- a new `/query` endpoint, defined by the function decorator\r\n- a new import from flask, `request`, which is used to get parameters from the request\r\n- if the `text` parameter is missing, then we return an error message and an appropriate HTML response code\r\n- otherwise, we query the index, and return the response as a string\r\n\r\nA full query example that you can test in your browser might look something like this: `http://localhost:5601/query?text=what did the author do growing up` (once you press enter, the browser will convert the spaces into \"%20\" characters).Things are looking pretty good!We now have a functional API.Using your own documents, you can easily provide an interface for any application to call the flask API and get answers to queries.### Advanced Flask - Handling User Document Uploads\r\n\r\nThings are looking pretty cool, but how can we take this a step further?What if we want to allow users to build their own indexes by uploading their own documents?Have no fear, Flask can handle it all :muscle:.To let users upload documents, we have to take some extra precautions.Instead of querying an existing index, the index will become **mutable**.If you have many users adding to the same index, we need to think about how to handle concurrency.Our Flask server is threaded, which means multiple users can ping the server with requests which will be handled at the same time.One option might be to create an index for each user or group, and store and fetch things from S3.But for this example, we will assume there is one locally stored index that users are interacting with.To handle concurrent uploads and ensure sequential inserts into the index, we can use the `BaseManager` python package to provide sequential access to the index using a separate server and locks.This sounds scary, but it's not so bad!We will just move all our index operations (initializing, querying, inserting) into the `BaseManager` \"index_server\", which will be called from our Flask server.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c27b9bc8-c421-4da3-8b0b-2a72343235a9": {"__data__": {"id_": "c27b9bc8-c421-4da3-8b0b-2a72343235a9", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "818302b819cd3e0ea2b753e807a2c0c67cb01ed6433cec7ceb016e398e602399"}, "2": {"node_id": "81d03ce7-6627-45a7-8a9f-66374ec48eb0", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "35d69d0ac8405fae54303f8d65863e65be1d6ac2d2486785623e2391363bf2f6"}, "3": {"node_id": "4b7ca0e6-132d-4928-a648-c728ad3171a1", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "dc58d4b355c7021e9ed7a4192b8dec7aa23f11f3f08c6e07219dbdcdf6e0b3c8"}}, "hash": "9f80e5cc8dbfe224cab229fe9316d4043a20eb5f69540f0593c71e294802b363", "text": "Here's a basic example of what our `index_server.py` will look like after we've moved our code:\r\n\r\n```python\r\nimport os\r\nfrom multiprocessing import Lock\r\nfrom multiprocessing.managers import BaseManager\r\nfrom llama_index import SimpleDirectoryReader, VectorStoreIndex, Document\r\n\r\n# NOTE: for local testing only, do NOT deploy with your key hardcoded\r\nos.environ['OPENAI_API_KEY'] = \"your key here\"\r\n\r\nindex = None\r\nlock = Lock()\r\n\r\ndef initialize_index():\r\n  global index\r\n\r\n  with lock:\r\n    # same as before ...\r\n  ...\r\n\r\ndef query_index(query_text):\r\n  global index\r\n  query_engine = index.as_query_engine()\r\n  response = query_engine.query(query_text)\r\n  return str(response)\r\n\r\nif __name__ == \"__main__\":\r\n    # init the global index\r\n    print(\"initializing index...\")\r\n    initialize_index()\r\n\r\n    # setup server\r\n    # NOTE: you might want to handle the password in a less hardcoded way\r\n    manager = BaseManager(('', 5602), b'password')\r\n    manager.register('query_index', query_index)\r\n    server = manager.get_server()\r\n\r\n    print(\"starting server...\")\r\n    server.serve_forever()\r\n```\r\n\r\n_index_server.py_\r\n\r\nSo, we've moved our functions, introduced the `Lock` object which ensures sequential access to the global index, registered our single function in the server, and started the server on port 5602 with the password `password`.Then, we can adjust our flask code as follows:\r\n\r\n```python\r\nfrom multiprocessing.managers import BaseManager\r\nfrom flask import Flask, request\r\n\r\n# initialize manager connection\r\n# NOTE: you might want to handle the password in a less hardcoded way\r\nmanager = BaseManager(('', 5602), b'password')\r\nmanager.register('query_index')\r\nmanager.connect()\r\n\r\n@app.route(\"/query\", methods=[\"GET\"])\r\ndef query_index():\r\n  global index\r\n  query_text = request.args.get(\"text\", None)\r\n  if query_text is None:\r\n    return \"No text found, please include a ?text=blah parameter in the URL\", 400\r\n  response = manager.query_index(query_text)._getvalue()\r\n  return str(response), 200\r\n\r\n@app.route(\"/\")\r\ndef home():\r\n    return \"Hello World!\"if __name__ == \"__main__\":\r\n    app.run(host=\"0.0.0.0\", port=5601)\r\n\r\n```\r\n\r\n_flask_demo.py_\r\n\r\nThe two main changes are connecting to our existing `BaseManager` server and registering the functions, as well as calling the function through the manager in the `/query` endpoint.One special thing to note is that `BaseManager` servers don't return objects quite as we expect.To resolve the return value into it's original object, we call the `_getvalue()` function.If we allow users to upload their own documents, we should probably remove the Paul Graham essay from the documents folder, so let's do that first.Then, let's add an endpoint to upload files!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4b7ca0e6-132d-4928-a648-c728ad3171a1": {"__data__": {"id_": "4b7ca0e6-132d-4928-a648-c728ad3171a1", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "818302b819cd3e0ea2b753e807a2c0c67cb01ed6433cec7ceb016e398e602399"}, "2": {"node_id": "c27b9bc8-c421-4da3-8b0b-2a72343235a9", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "9f80e5cc8dbfe224cab229fe9316d4043a20eb5f69540f0593c71e294802b363"}, "3": {"node_id": "afdaba8b-1eea-488d-b4da-ebab174cec70", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "fe72dbe3608876c21b26f2b4b34f70f669f1ae4053261f2a695c36822523cf8d"}}, "hash": "dc58d4b355c7021e9ed7a4192b8dec7aa23f11f3f08c6e07219dbdcdf6e0b3c8", "text": "First, let's define our Flask endpoint function:\r\n\r\n```python\r\n...\r\nmanager.register('insert_into_index')\r\n...\r\n\r\n@app.route(\"/uploadFile\", methods=[\"POST\"])\r\ndef upload_file():\r\n    global manager\r\n    if 'file' not in request.files:\r\n        return \"Please send a POST request with a file\", 400\r\n\r\n    filepath = None\r\n    try:\r\n        uploaded_file = request.files[\"file\"]\r\n        filename = secure_filename(uploaded_file.filename)\r\n        filepath = os.path.join('documents', os.path.basename(filename))\r\n        uploaded_file.save(filepath)\r\n\r\n        if request.form.get(\"filename_as_doc_id\", None) is not None:\r\n            manager.insert_into_index(filepath, doc_id=filename)\r\n        else:\r\n            manager.insert_into_index(filepath)\r\n    except Exception as e:\r\n        # cleanup temp file\r\n        if filepath is not None and os.path.exists(filepath):\r\n            os.remove(filepath)\r\n        return \"Error: {}\".format(str(e)), 500\r\n\r\n    # cleanup temp file\r\n    if filepath is not None and os.path.exists(filepath):\r\n        os.remove(filepath)\r\n\r\n    return \"File inserted!\", 200\r\n```\r\n\r\nNot too bad!You will notice that we write the file to disk.We could skip this if we only accept basic file formats like `txt` files, but written to disk we can take advantage of LlamaIndex's `SimpleDirectoryReader` to take care of a bunch of more complex file formats.Optionally, we also use a second `POST` argument to either use the filename as a doc_id or let LlamaIndex generate one for us.This will make more sense once we implement the frontend.With these more complicated requests, I also suggest using a tool like [Postman](https://www.postman.com/downloads/?utm_source=postman-home).Examples of using postman to test our endpoints are in the [repository for this project](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react/postman_examples).Lastly, you'll notice we added a new function to the manager.Let's implement that inside `index_server.py`:\r\n\r\n```python\r\ndef insert_into_index(doc_text, doc_id=None):\r\n    global index\r\n    document = SimpleDirectoryReader(input_files=[doc_text]).load_data()[0]\r\n    if doc_id is not None:\r\n        document.doc_id = doc_id\r\n\r\n    with lock:\r\n        index.insert(document)\r\n        index.storage_context.persist()\r\n\r\n...\r\nmanager.register('insert_into_index', insert_into_index)\r\n...\r\n```\r\n\r\nEasy!If we launch both the `index_server.py` and then the `flask_demo.py` python files, we have a Flask API server that can handle multiple requests to insert documents into a vector index and respond to user queries!To support some functionality in the frontend, I've adjusted what some responses look like from the Flask API, as well as added some functionality to keep track of which documents are stored in the index (LlamaIndex doesn't currently support this in a user-friendly way, but we can augment it ourselves!).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "afdaba8b-1eea-488d-b4da-ebab174cec70": {"__data__": {"id_": "afdaba8b-1eea-488d-b4da-ebab174cec70", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "818302b819cd3e0ea2b753e807a2c0c67cb01ed6433cec7ceb016e398e602399"}, "2": {"node_id": "4b7ca0e6-132d-4928-a648-c728ad3171a1", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "dc58d4b355c7021e9ed7a4192b8dec7aa23f11f3f08c6e07219dbdcdf6e0b3c8"}, "3": {"node_id": "8c626d5e-a15a-43a8-be3f-18a0a556655a", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "502f03d9715a3df01b8ede451909869b88e383a20c216c0d5a04b75cffc65c41"}}, "hash": "fe72dbe3608876c21b26f2b4b34f70f669f1ae4053261f2a695c36822523cf8d", "text": "Lastly, I had to add CORS support to the server using the `Flask-cors` python package.Check out the complete `flask_demo.py` and `index_server.py` scripts in the [repository](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react) for the final minor changes, the`requirements.txt` file, and a sample `Dockerfile` to help with deployment.## React Frontend\r\n\r\nGenerally, React and Typescript are one of the most popular libraries and languages for writing webapps today.This guide will assume you are familiar with how these tools work, because otherwise this guide will triple in length :smile:.In the [repository](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react), the frontend code is organized inside of the `react_frontend` folder.The most relevant part of the frontend will be the `src/apis` folder.This is where we make calls to the Flask server, supporting the following queries:\r\n\r\n- `/query` -- make a query to the existing index\r\n- `/uploadFile` -- upload a file to the flask server for insertion into the index\r\n- `/getDocuments` -- list the current document titles and a portion of their texts\r\n\r\nUsing these three queries, we can build a robust frontend that allows users to upload and keep track of their files, query the index, and view the query response and information about which text nodes were used to form the response.### fetchDocuments.tsx\r\n\r\nThis file contains the function to, you guessed it, fetch the list of current documents in the index.The code is as follows:\r\n\r\n```typescript\r\nexport type Document = {\r\n  id: string;\r\n  text: string;\r\n};\r\n\r\nconst fetchDocuments = async (): Promise<Document[]> => {\r\n  const response = await fetch(\"http://localhost:5601/getDocuments\", {\r\n    mode: \"cors\",\r\n  });\r\n\r\n  if (!response.ok) {\r\n    return [];\r\n  }\r\n\r\n  const documentList = (await response.json()) as Document[];\r\n  return documentList;\r\n};\r\n```\r\n\r\nAs you can see, we make a query to the Flask server (here, it assumes running on localhost).Notice that we need to include the `mode: 'cors'` option, as we are making an external request.Then, we check if the response was ok, and if so, get the response json and return it.Here, the response json is a list of `Document` objects that are defined in the same file.### queryIndex.tsx\r\n\r\nThis file sends the user query to the flask server, and gets the response back, as well as details about which nodes in our index provided the response.```typescript\r\nexport type ResponseSources = {\r\n  text: string;\r\n  doc_id: string;\r\n  start: number;\r\n  end: number;\r\n  similarity: number;\r\n};\r\n\r\nexport type QueryResponse = {\r\n  text: string;\r\n  sources: ResponseSources[];\r\n};\r\n\r\nconst queryIndex = async (query: string): Promise<QueryResponse> => {\r\n  const queryURL = new URL(\"http://localhost:5601/query?text=1\");\r\n  queryURL.searchParams.append(\"text\", query);\r\n\r\n  const response = await fetch(queryURL, { mode: \"cors\" });\r\n  if (!response.ok) {\r\n    return { text: \"Error in query\", sources: [] };\r\n  }\r\n\r\n  const queryResponse = (await response.json()) as QueryResponse;\r\n\r\n  return queryResponse;\r\n};\r\n\r\nexport default queryIndex;\r\n```\r\n\r\nThis is similar to the `fetchDocuments.tsx` file, with the main difference being we include the query text as a parameter in the URL.Then, we check if the response is ok and return it with the appropriate typescript type.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8c626d5e-a15a-43a8-be3f-18a0a556655a": {"__data__": {"id_": "8c626d5e-a15a-43a8-be3f-18a0a556655a", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "818302b819cd3e0ea2b753e807a2c0c67cb01ed6433cec7ceb016e398e602399"}, "2": {"node_id": "afdaba8b-1eea-488d-b4da-ebab174cec70", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}, "hash": "fe72dbe3608876c21b26f2b4b34f70f669f1ae4053261f2a695c36822523cf8d"}}, "hash": "502f03d9715a3df01b8ede451909869b88e383a20c216c0d5a04b75cffc65c41", "text": "### insertDocument.tsx\r\n\r\nProbably the most complex API call is uploading a document.The function here accepts a file object and constructs a `POST` request using `FormData`.The actual response text is not used in the app but could be utilized to provide some user feedback on if the file failed to upload or not.```typescript\r\nconst insertDocument = async (file: File) => {\r\n  const formData = new FormData();\r\n  formData.append(\"file\", file);\r\n  formData.append(\"filename_as_doc_id\", \"true\");\r\n\r\n  const response = await fetch(\"http://localhost:5601/uploadFile\", {\r\n    mode: \"cors\",\r\n    method: \"POST\",\r\n    body: formData,\r\n  });\r\n\r\n  const responseText = response.text();\r\n  return responseText;\r\n};\r\n\r\nexport default insertDocument;\r\n```\r\n\r\n### All the Other Frontend Good-ness\r\n\r\nAnd that pretty much wraps up the frontend portion!The rest of the react frontend code is some pretty basic react components, and my best attempt to make it look at least a little nice :smile:.I encourage to read the rest of the [codebase](https://github.com/logan-markewich/llama_index_starter_pack/tree/main/flask_react/react_frontend) and submit any PRs for improvements!## Conclusion\r\n\r\nThis guide has covered a ton of information.We went from a basic \"Hello World\" Flask server written in python, to a fully functioning LlamaIndex powered backend and how to connect that to a frontend application.As you can see, we can easily augment and wrap the services provided by LlamaIndex (like the little external document tracker) to help provide a good user experience on the frontend.You could take this and add many features (multi-index/user support, saving objects into S3, adding a Pinecone vector server, etc.).And when you build an app after reading this, be sure to share the final result in the Discord!Good Luck!:muscle:", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "98988af5-7ed4-42ce-a24e-b2eae1327a55": {"__data__": {"id_": "98988af5-7ed4-42ce-a24e-b2eae1327a55", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "3": {"node_id": "bfa861a8-eead-4df4-b4b9-0987c129ad3e", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "440e975e87495e13306fe28be45a617e134ed9b87248bcd34d77fa8b6f7b352a"}}, "hash": "2bd538d38672f5ef397d089a423503ace67dd04f5b5d863dcc47d78b75629718", "text": "# A Guide to Building a Full-Stack LlamaIndex Web App with Delphic\n\nThis guide seeks to walk you through using LlamaIndex with a production-ready web app starter template\ncalled [Delphic](https://github.com/JSv4/Delphic).All code examples here are available from\nthe [Delphic](https://github.com/JSv4/Delphic) repo\n\n## What We're Building\n\nHere's a quick demo of the out-of-the-box functionality of Delphic:\n\nhttps://user-images.githubusercontent.com/5049984/233236432-aa4980b6-a510-42f3-887a-81485c9644e6.mp4\n\n## Architectural Overview\n\nDelphic leverages the LlamaIndex python library to let users to create their own document collections they can then\nquery in a responsive frontend.We chose a stack that provides a responsive, robust mix of technologies that can (1) orchestrate complex python\nprocessing tasks while providing (2) a modern, responsive frontend and (3) a secure backend to build additional\nfunctionality upon.The core libraries are:\n\n1.[Django](https://www.djangoproject.com/)\n2.[Django Channels](https://channels.readthedocs.io/en/stable/)\n3.[Django Ninja](https://django-ninja.rest-framework.com/)\n4.[Redis](https://redis.io/)\n5.[Celery](https://docs.celeryq.dev/en/stable/getting-started/introduction.html)\n6.[LlamaIndex](https://gpt-index.readthedocs.io/en/latest/)\n7.[Langchain](https://python.langchain.com/en/latest/index.html)\n8.[React](https://github.com/facebook/react)\n9.Docker & Docker Compose\n\nThanks to this modern stack built on the super stable Django web framework, the starter Delphic app boasts a streamlined\ndeveloper experience, built-in authentication and user management, asynchronous vector store processing, and\nweb-socket-based query connections for a responsive UI.In addition, our frontend is built with TypeScript and is based\non MUI React for a responsive and modern user interface.## System Requirements\n\nCelery doesn't work on Windows.It may be deployable with Windows Subsystem for Linux, but configuring that is beyond\nthe scope of this tutorial.For this reason, we recommend you only follow this tutorial if you're running Linux or OSX.You will need Docker and Docker Compose installed to deploy the application.Local development will require node version\nmanager (nvm).## Django Backend\n\n### Project Directory Overview\n\nThe Delphic application has a structured backend directory organization that follows common Django project conventions.From the repo root, in the `./delphic` subfolder, the main folders are:\n\n1.`contrib`: This directory contains custom modifications or additions to Django's built-in `contrib` apps.2.`indexes`: This directory contains the core functionality related to document indexing and LLM integration.It\n   includes:\n\n- `admin.py`: Django admin configuration for the app\n- `apps.py`: Application configuration\n- `models.py`: Contains the app's database models\n- `migrations`: Directory containing database schema migrations for the app\n- `signals.py`: Defines any signals for the app\n- `tests.py`: Unit tests for the app\n\n3.`tasks`: This directory contains tasks for asynchronous processing using Celery.The `index_tasks.py` file includes\n   the tasks for creating vector indexes.4.`users`: This directory is dedicated to user management, including:\n5.`utils`: This directory contains utility modules and functions that are used across the application, such as custom\n   storage backends, path helpers, and collection-related utilities.### Database Models\n\nThe Delphic application has two core models: `Document` and `Collection`.These models represent the central entities\nthe application deals with when indexing and querying documents using LLMs.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bfa861a8-eead-4df4-b4b9-0987c129ad3e": {"__data__": {"id_": "bfa861a8-eead-4df4-b4b9-0987c129ad3e", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "98988af5-7ed4-42ce-a24e-b2eae1327a55", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "2bd538d38672f5ef397d089a423503ace67dd04f5b5d863dcc47d78b75629718"}, "3": {"node_id": "31d74118-6633-45bd-939d-20eb4f24a622", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "f54660b0ac1aecb6ae88f111bbca94ca78f7e28f145a8ae35955d2637c0b7417"}}, "hash": "440e975e87495e13306fe28be45a617e134ed9b87248bcd34d77fa8b6f7b352a", "text": "They're defined in\n[`./delphic/indexes/models.py`](https://github.com/JSv4/Delphic/blob/main/delphic/indexes/models.py).1.`Collection`:\n\n- `api_key`: A foreign key that links a collection to an API key.This helps associate jobs with the source API key.- `title`: A character field that provides a title for the collection.- `description`: A text field that provides a description of the collection.- `status`: A character field that stores the processing status of the collection, utilizing the `CollectionStatus`\n  enumeration.- `created`: A datetime field that records when the collection was created.- `modified`: A datetime field that records the last modification time of the collection.- `model`: A file field that stores the model associated with the collection.- `processing`: A boolean field that indicates if the collection is currently being processed.2.`Document`:\n\n- `collection`: A foreign key that links a document to a collection.This represents the relationship between documents\n  and collections.- `file`: A file field that stores the uploaded document file.- `description`: A text field that provides a description of the document.- `created`: A datetime field that records when the document was created.- `modified`: A datetime field that records the last modification time of the document.These models provide a solid foundation for collections of documents and the indexes created from them with LlamaIndex.### Django Ninja API\n\nDjango Ninja is a web framework for building APIs with Django and Python 3.7+ type hints.It provides a simple,\nintuitive, and expressive way of defining API endpoints, leveraging Python\u2019s type hints to automatically generate input\nvalidation, serialization, and documentation.In the Delphic repo,\nthe [`./config/api/endpoints.py`](https://github.com/JSv4/Delphic/blob/main/config/api/endpoints.py)\nfile contains the API routes and logic for the API endpoints.Now, let\u2019s briefly address the purpose of each endpoint\nin the `endpoints.py` file:\n\n1.`/heartbeat`: A simple GET endpoint to check if the API is up and running.Returns `True` if the API is accessible.This is helpful for Kubernetes setups that expect to be able to query your container to ensure it's up and running.2.`/collections/create`: A POST endpoint to create a new `Collection`.Accepts form parameters such\n   as `title`, `description`, and a list of `files`.Creates a new `Collection` and `Document` instances for each file,\n   and schedules a Celery task to create an index.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "31d74118-6633-45bd-939d-20eb4f24a622": {"__data__": {"id_": "31d74118-6633-45bd-939d-20eb4f24a622", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "bfa861a8-eead-4df4-b4b9-0987c129ad3e", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "440e975e87495e13306fe28be45a617e134ed9b87248bcd34d77fa8b6f7b352a"}, "3": {"node_id": "97ad579d-fce9-484d-8665-0c18c2151d28", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "bf3e4fc756f86d78388550d1837e700a75bdf2ec0261c5c6b9512be5b97f02cf"}}, "hash": "f54660b0ac1aecb6ae88f111bbca94ca78f7e28f145a8ae35955d2637c0b7417", "text": "```python\n@collections_router.post(\"/create\")\nasync def create_collection(request,\n                            title: str = Form(...),\n                            description: str = Form(...),\n                            files: list[UploadedFile] = File(...), ):\n    key = None if getattr(request, \"auth\", None) is None else request.auth\n    if key is not None:\n        key = await key\n\n    collection_instance = Collection(\n        api_key=key,\n        title=title,\n        description=description,\n        status=CollectionStatusEnum.QUEUED,\n    )\n\n    await sync_to_async(collection_instance.save)()\n\n    for uploaded_file in files:\n        doc_data = uploaded_file.file.read()\n        doc_file = ContentFile(doc_data, uploaded_file.name)\n        document = Document(collection=collection_instance, file=doc_file)\n        await sync_to_async(document.save)()\n\n    create_index.si(collection_instance.id).apply_async()\n\n    return await sync_to_async(CollectionModelSchema)(\n        ...\n    )\n```\n\n3.`/collections/query` \u2014 a POST endpoint to query a document collection using the LLM.Accepts a JSON payload\n   containing `collection_id` and `query_str`, and returns a response generated by querying the collection.We don't\n   actually use this endpoint in our chat GUI (We use a websocket - see below), but you could build an app to integrate\n   to this REST endpoint to query a specific collection.```python\n@collections_router.post(\"/query\",\n                         response=CollectionQueryOutput,\n                         summary=\"Ask a question of a document collection\", )\ndef query_collection_view(request: HttpRequest, query_input: CollectionQueryInput):\n    collection_id = query_input.collection_id\n    query_str = query_input.query_str\n    response = query_collection(collection_id, query_str)\n    return {\"response\": response}\n```\n\n4.`/collections/available`: A GET endpoint that returns a list of all collections created with the user's API key.The\n   output is serialized using the `CollectionModelSchema`.```python\n@collections_router.get(\"/available\",\n                        response=list[CollectionModelSchema],\n                        summary=\"Get a list of all of the collections created with my api_key\", )\nasync def get_my_collections_view(request: HttpRequest):\n    key = None if getattr(request, \"auth\", None) is None else request.auth\n    if key is not None:\n        key = await key\n\n    collections = Collection.objects.filter(api_key=key)\n\n    return [\n        {\n            ...\n        }\n        async for collection in collections\n    ]\n```\n\n5.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "97ad579d-fce9-484d-8665-0c18c2151d28": {"__data__": {"id_": "97ad579d-fce9-484d-8665-0c18c2151d28", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "31d74118-6633-45bd-939d-20eb4f24a622", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "f54660b0ac1aecb6ae88f111bbca94ca78f7e28f145a8ae35955d2637c0b7417"}, "3": {"node_id": "2bc3ad40-0112-49aa-8bf1-7aa42d89642f", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "c9ca6cc8937b9f169648e0fec41940f0222705ae0eb53f11b1da90ed743bcc94"}}, "hash": "bf3e4fc756f86d78388550d1837e700a75bdf2ec0261c5c6b9512be5b97f02cf", "text": "`/collections/{collection_id}/add_file`: A POST endpoint to add a file to an existing collection.Accepts\n   a `collection_id` path parameter, and form parameters such as `file` and `description`.Adds the file as a `Document`\n   instance associated with the specified collection.```python\n@collections_router.post(\"/{collection_id}/add_file\", summary=\"Add a file to a collection\")\nasync def add_file_to_collection(request,\n                                 collection_id: int,\n                                 file: UploadedFile = File(...),\n                                 description: str = Form(...), ):\n    collection = await sync_to_async(Collection.objects.get)(id=collection_id\n```\n\n### Intro to Websockets\n\nWebSockets are a communication protocol that enables bidirectional and full-duplex communication between a client and a\nserver over a single, long-lived connection.The WebSocket protocol is designed to work over the same ports as HTTP and\nHTTPS (ports 80 and 443, respectively) and uses a similar handshake process to establish a connection.Once the\nconnection is established, data can be sent in both directions as \u201cframes\u201d without the need to reestablish the\nconnection each time, unlike traditional HTTP requests.There are several reasons to use WebSockets, particularly when working with code that takes a long time to load into\nmemory but is quick to run once loaded:\n\n1.**Performance**: WebSockets eliminate the overhead associated with opening and closing multiple connections for each\n   request, reducing latency.2.**Efficiency**: WebSockets allow for real-time communication without the need for polling, resulting in more\n   efficient use of resources and better responsiveness.3.**Scalability**: WebSockets can handle a large number of simultaneous connections, making it ideal for applications\n   that require high concurrency.In the case of the Delphic application, using WebSockets makes sense as the LLMs can be expensive to load into memory.By establishing a WebSocket connection, the LLM can remain loaded in memory, allowing subsequent requests to be\nprocessed quickly without the need to reload the model each time.The ASGI configuration file [`./config/asgi.py`](https://github.com/JSv4/Delphic/blob/main/config/asgi.py) defines how\nthe application should handle incoming connections, using the Django Channels `ProtocolTypeRouter` to route connections\nbased on their protocol type.In this case, we have two protocol types: \"http\" and \"websocket\".The \u201chttp\u201d protocol type uses the standard Django ASGI application to handle HTTP requests, while the \u201cwebsocket\u201d\nprotocol type uses a custom `TokenAuthMiddleware` to authenticate WebSocket connections.The `URLRouter` within\nthe `TokenAuthMiddleware` defines a URL pattern for the `CollectionQueryConsumer`, which is responsible for handling\nWebSocket connections related to querying document collections.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2bc3ad40-0112-49aa-8bf1-7aa42d89642f": {"__data__": {"id_": "2bc3ad40-0112-49aa-8bf1-7aa42d89642f", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "97ad579d-fce9-484d-8665-0c18c2151d28", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "bf3e4fc756f86d78388550d1837e700a75bdf2ec0261c5c6b9512be5b97f02cf"}, "3": {"node_id": "cd444059-4829-454b-84a7-5890a6c92a80", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "a1eef3694e9fe6fd2f21d88e7783423b95e297cc0af67ed896147cf63d8d84a4"}}, "hash": "c9ca6cc8937b9f169648e0fec41940f0222705ae0eb53f11b1da90ed743bcc94", "text": "```python\napplication = ProtocolTypeRouter(\n    {\n        \"http\": get_asgi_application(),\n        \"websocket\": TokenAuthMiddleware(\n            URLRouter(\n                [\n                    re_path(\n                        r\"ws/collections/(?P<collection_id>\\w+)/query/$\",\n                        CollectionQueryConsumer.as_asgi(),\n                    ),\n                ]\n            )\n        ),\n    }\n)\n```\n\nThis configuration allows clients to establish WebSocket connections with the Delphic application to efficiently query\ndocument collections using the LLMs, without the need to reload the models for each request.### Websocket Handler\n\nThe `CollectionQueryConsumer` class\nin [`config/api/websockets/queries.py`](https://github.com/JSv4/Delphic/blob/main/config/api/websockets/queries.py) is\nresponsible for handling WebSocket connections related to querying document collections.It inherits from\nthe `AsyncWebsocketConsumer` class provided by Django Channels.The `CollectionQueryConsumer` class has three main methods:\n\n1.`connect`: Called when a WebSocket is handshaking as part of the connection process.2.`disconnect`: Called when a WebSocket closes for any reason.3.`receive`: Called when the server receives a message from the WebSocket.#### Websocket connect listener\n\nThe `connect` method is responsible for establishing the connection, extracting the collection ID from the connection\npath, loading the collection model, and accepting the connection.```python\nasync def connect(self):\n    try:\n        self.collection_id = extract_connection_id(self.scope[\"path\"])\n        self.index = await load_collection_model(self.collection_id)\n        await self.accept()\n\nexcept ValueError as e:\nawait self.accept()\nawait self.close(code=4000)\nexcept Exception as e:\npass\n```\n\n#### Websocket disconnect listener\n\nThe `disconnect` method is empty in this case, as there are no additional actions to be taken when the WebSocket is\nclosed.#### Websocket receive listener\n\nThe `receive` method is responsible for processing incoming messages from the WebSocket.It takes the incoming message,\ndecodes it, and then queries the loaded collection model using the provided query.The response is then formatted as a\nmarkdown string and sent back to the client over the WebSocket connection.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cd444059-4829-454b-84a7-5890a6c92a80": {"__data__": {"id_": "cd444059-4829-454b-84a7-5890a6c92a80", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "2bc3ad40-0112-49aa-8bf1-7aa42d89642f", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "c9ca6cc8937b9f169648e0fec41940f0222705ae0eb53f11b1da90ed743bcc94"}, "3": {"node_id": "b5736017-4998-4aea-ba88-7fcd6b544808", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "37a1dbcf312086c692ff2d844da4123f3a0e0753622166dd1419c95b54488044"}}, "hash": "a1eef3694e9fe6fd2f21d88e7783423b95e297cc0af67ed896147cf63d8d84a4", "text": "```python\nasync def receive(self, text_data):\n    text_data_json = json.loads(text_data)\n\n    if self.index is not None:\n        query_str = text_data_json[\"query\"]\n        modified_query_str = f\"Please return a nicely formatted markdown string to this request:\\n\\n{query_str}\"\n        query_engine = self.index.as_query_engine()\n        response = query_engine.query(modified_query_str)\n\n        markdown_response = f\"## Response\\n\\n{response}\\n\\n\"\n        if response.source_nodes:\n            markdown_sources = f\"## Sources\\n\\n{response.get_formatted_sources()}\"\n        else:\n            markdown_sources = \"\"\n\n        formatted_response = f\"{markdown_response}{markdown_sources}\"\n\n        await self.send(json.dumps({\"response\": formatted_response}, indent=4))\n    else:\n        await self.send(json.dumps({\"error\": \"No index loaded for this connection.\"}, indent=4))\n```\n\nTo load the collection model, the `load_collection_model` function is used, which can be found\nin [`delphic/utils/collections.py`](https://github.com/JSv4/Delphic/blob/main/delphic/utils/collections.py).This\nfunction retrieves the collection object with the given collection ID, checks if a JSON file for the collection model\nexists, and if not, creates one.Then, it sets up the `LLMPredictor` and `ServiceContext` before loading\nthe `VectorStoreIndex` using the cache file.```python\nasync def load_collection_model(collection_id: str | int) -> VectorStoreIndex:\n    \"\"\"\n    Load the Collection model from cache or the database, and return the index.Args:\n        collection_id (Union[str, int]): The ID of the Collection model instance.Returns:\n        VectorStoreIndex: The loaded index.This function performs the following steps:\n    1.Retrieve the Collection object with the given collection_id.2.Check if a JSON file with the name '/cache/model_{collection_id}.json' exists.3.If the JSON file doesn't exist, load the JSON from the Collection.model FileField and save it to\n       '/cache/model_{collection_id}.json'.4.Call VectorStoreIndex.load_from_disk with the cache_file_path.\n    \"\"\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b5736017-4998-4aea-ba88-7fcd6b544808": {"__data__": {"id_": "b5736017-4998-4aea-ba88-7fcd6b544808", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "cd444059-4829-454b-84a7-5890a6c92a80", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "a1eef3694e9fe6fd2f21d88e7783423b95e297cc0af67ed896147cf63d8d84a4"}, "3": {"node_id": "cc232f0b-69fb-45af-bbc9-77e97ff5e84c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "d486dcca18b6179017b988caba2c46e5e56c082c8a8908d663d42c4e297e0d8c"}}, "hash": "37a1dbcf312086c692ff2d844da4123f3a0e0753622166dd1419c95b54488044", "text": "# Retrieve the Collection object\n    collection = await Collection.objects.aget(id=collection_id)\n    logger.info(f\"load_collection_model() - loaded collection {collection_id}\")\n\n    # Make sure there's a model\n    if collection.model.name:\n        logger.info(\"load_collection_model() - Setup local json index file\")\n\n        # Check if the JSON file exists\n        cache_dir = Path(settings.BASE_DIR) / \"cache\"\n        cache_file_path = cache_dir / f\"model_{collection_id}.json\"\n        if not cache_file_path.exists():\n            cache_dir.mkdir(parents=True, exist_ok=True)\n            with collection.model.open(\"rb\") as model_file:\n                with cache_file_path.open(\"w+\", encoding=\"utf-8\") as cache_file:\n                    cache_file.write(model_file.read().decode(\"utf-8\"))\n\n        # define LLM\n        logger.info(\n            f\"load_collection_model() - Setup service context with tokens {settings.MAX_TOKENS} and \"\n            f\"model {settings.MODEL_NAME}\"\n        )\n        llm = OpenAI(temperature=0, model=\"text-davinci-003\", max_tokens=512)\n        service_context = ServiceContext.from_defaults(llm=llm)\n\n        # Call VectorStoreIndex.load_from_disk\n        logger.info(\"load_collection_model() - Load llama index\")\n        index = VectorStoreIndex.load_from_disk(\n            cache_file_path, service_context=service_context\n        )\n        logger.info(\n            \"load_collection_model() - Llamaindex loaded and ready for query...\"\n        )\n\n    else:\n        logger.error(\n            f\"load_collection_model() - collection {collection_id} has no model!\")\n        raise ValueError(\"No model exists for this collection!\")return index\n```\n\n## React Frontend\n\n### Overview\n\nWe chose to use TypeScript, React and Material-UI (MUI) for the Delphic project\u2019s frontend for a couple reasons.First,\nas the most popular component library (MUI) for the most popular frontend framework (React), this choice makes this\nproject accessible to a huge community of developers.Second, React is, at this point, a stable and generally well-liked\nframework that delivers valuable abstractions in the form of its virtual DOM while still being relatively stable and, in\nour opinion, pretty easy to learn, again making it accessible.### Frontend Project Structure\n\nThe frontend can be found in the [`/frontend`](https://github.com/JSv4/Delphic/tree/main/frontend) directory of the\nrepo, with the React-related components being in `/frontend/src` .You\u2019ll notice there is a DockerFile in the `frontend`\ndirectory and several folders and files related to configuring our frontend web\nserver \u2014 [nginx](https://www.nginx.com/).The `/frontend/src/App.tsx` file serves as the entry point of the application.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cc232f0b-69fb-45af-bbc9-77e97ff5e84c": {"__data__": {"id_": "cc232f0b-69fb-45af-bbc9-77e97ff5e84c", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "b5736017-4998-4aea-ba88-7fcd6b544808", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "37a1dbcf312086c692ff2d844da4123f3a0e0753622166dd1419c95b54488044"}, "3": {"node_id": "45e5b4de-d494-4270-880b-1fa853e039ab", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "efccb38ecc7fa4bedf62dddfe4f1817307ed26fb2adfe2861b6ffa97ccd30245"}}, "hash": "d486dcca18b6179017b988caba2c46e5e56c082c8a8908d663d42c4e297e0d8c", "text": "It defines the main components, such as\nthe login form, the drawer layout, and the collection create modal.The main components are conditionally rendered based\non whether the user is logged in and has an authentication token.The DrawerLayout2 component is defined in the`DrawerLayour2.tsx` file.This component manages the layout of the\napplication and provides the navigation and main content areas.Since the application is relatively simple, we can get away with not using a complex state management solution like\nRedux and just use React\u2019s useState hooks.### Grabbing Collections from the Backend\n\nThe collections available to the logged-in user are retrieved and displayed in the DrawerLayout2 component.The process\ncan be broken down into the following steps:\n\n1.Initializing state variables:\n\n```tsx\nconst[collections, setCollections] = useState < CollectionModelSchema[] > ([]);\nconst[loading, setLoading] = useState(true);\n```\n\nHere, we initialize two state variables: `collections` to store the list of collections and `loading` to track whether\nthe collections are being fetched.2.Collections are fetched for the logged-in user with the `fetchCollections()` function:\n\n```tsx\nconst\nfetchCollections = async () = > {\ntry {\nconst accessToken = localStorage.getItem(\"accessToken\");\nif (accessToken) {\nconst response = await getMyCollections(accessToken);\nsetCollections(response.data);\n}\n} catch (error) {\nconsole.error(error);\n} finally {\nsetLoading(false);\n}\n};\n```\n\nThe `fetchCollections` function retrieves the collections for the logged-in user by calling the `getMyCollections` API\nfunction with the user's access token.It then updates the `collections` state with the retrieved data and sets\nthe `loading` state to `false` to indicate that fetching is complete.### Displaying Collections\n\nThe latest collectios are displayed in the drawer like this:\n\n```tsx\n< List >\n{collections.map((collection) = > (\n    < div key={collection.id} >\n    < ListItem disablePadding >\n    < ListItemButton\n    disabled={\n    collection.status != = CollectionStatus.COMPLETE | |\n    !collection.has_model\n    }\n    onClick={() = > handleCollectionClick(collection)}\nselected = {\n    selectedCollection & &\n    selectedCollection.id == = collection.id\n}\n>\n< ListItemText\nprimary = {collection.title} / >\n          {collection.status == = CollectionStatus.RUNNING ?(\n    < CircularProgress\n    size={24}\n    style={{position: \"absolute\", right: 16}}\n    / >\n): null}\n< / ListItemButton >\n    < / ListItem >\n        < / div >\n))}\n< / List >\n```\n\nYou\u2019ll notice that the `disabled` property of a collection\u2019s `ListItemButton` is set based on whether the collection's\nstatus is not `CollectionStatus.COMPLETE` or the collection does not have a model (`!collection.has_model`).If either\nof these conditions is true, the button is disabled, preventing users from selecting an incomplete or model-less\ncollection.Where the CollectionStatus is RUNNING, we also show a loading wheel over the button.In a separate `useEffect` hook, we check if any collection in the `collections` state has a status\nof `CollectionStatus.RUNNING` or `CollectionStatus.QUEUED`.If so, we set up an interval to repeatedly call\nthe `fetchCollections` function every 15 seconds (15,000 milliseconds) to update the collection statuses.This way, the\napplication periodically checks for completed collections, and the UI is updated accordingly when the processing is\ndone.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "45e5b4de-d494-4270-880b-1fa853e039ab": {"__data__": {"id_": "45e5b4de-d494-4270-880b-1fa853e039ab", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "cc232f0b-69fb-45af-bbc9-77e97ff5e84c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "d486dcca18b6179017b988caba2c46e5e56c082c8a8908d663d42c4e297e0d8c"}, "3": {"node_id": "49ce9594-5f43-407c-9e20-4366e340ff72", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "0289a609a73837ab3d038af9bb6c727dfaa3e6c4a5073ac05d152186102b8c72"}}, "hash": "efccb38ecc7fa4bedf62dddfe4f1817307ed26fb2adfe2861b6ffa97ccd30245", "text": "```tsx\nuseEffect(() = > {\n    let\ninterval: NodeJS.Timeout;\nif (\n    collections.some(\n        (collection) = >\ncollection.status == = CollectionStatus.RUNNING | |\ncollection.status == = CollectionStatus.QUEUED\n)\n) {\n    interval = setInterval(() = > {\n    fetchCollections();\n}, 15000);\n}\nreturn () = > clearInterval(interval);\n}, [collections]);\n```\n\n### Chat View Component\n\nThe `ChatView` component in `frontend/src/chat/ChatView.tsx` is responsible for handling and displaying a chat interface\nfor a user to interact with a collection.The component establishes a WebSocket connection to communicate in real-time\nwith the server, sending and receiving messages.Key features of the `ChatView` component include:\n\n1.Establishing and managing the WebSocket connection with the server.2.Displaying messages from the user and the server in a chat-like format.3.Handling user input to send messages to the server.4.Updating the messages state and UI based on received messages from the server.5.Displaying connection status and errors, such as loading messages, connecting to the server, or encountering errors\n   while loading a collection.Together, all of this allows users to interact with their selected collection with a very smooth, low-latency\nexperience.#### Chat Websocket Client\n\nThe WebSocket connection in the `ChatView` component is used to establish real-time communication between the client and\nthe server.The WebSocket connection is set up and managed in the `ChatView` component as follows:\n\nFirst, we want to initialize the the WebSocket reference:\n\nconst websocket = useRef<WebSocket | null>(null);\n\nA `websocket` reference is created using `useRef`, which holds the WebSocket object that will be used for\ncommunication.`useRef` is a hook in React that allows you to create a mutable reference object that persists across\nrenders.It is particularly useful when you need to hold a reference to a mutable object, such as a WebSocket\nconnection, without causing unnecessary re-renders.In the `ChatView` component, the WebSocket connection needs to be established and maintained throughout the lifetime of\nthe component, and it should not trigger a re-render when the connection state changes.By using `useRef`, you ensure\nthat the WebSocket connection is kept as a reference, and the component only re-renders when there are actual state\nchanges, such as updating messages or displaying errors.The `setupWebsocket` function is responsible for establishing the WebSocket connection and setting up event handlers to\nhandle different WebSocket events.Overall, the setupWebsocket function looks like this:\n\n```tsx\nconst setupWebsocket = () => {  \n  setConnecting(true);  \n  // Here, a new WebSocket object is created using the specified URL, which includes the   \n  // selected collection's ID and the user's authentication token.websocket.current = new WebSocket(  \n    `ws://localhost:8000/ws/collections/${selectedCollection.id}/query/?token=${authToken}`  \n  );  \n  \n  websocket.current.onopen = (event) => {  \n    //...  \n  };  \n  \n  websocket.current.onmessage = (event) => {  \n    //...  \n  };  \n  \n  websocket.current.onclose = (event) => {  \n    //...  \n  };  \n  \n  websocket.current.onerror = (event) => {  \n    //...  \n  };  \n  \n  return () => {  \n    websocket.current?.close();  \n  };  \n};\n```\n\nNotice in a bunch of places we trigger updates to the GUI based on the information from the web socket client.When the component first opens and we try to establish a connection, the `onopen` listener is triggered.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "49ce9594-5f43-407c-9e20-4366e340ff72": {"__data__": {"id_": "49ce9594-5f43-407c-9e20-4366e340ff72", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "45e5b4de-d494-4270-880b-1fa853e039ab", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "efccb38ecc7fa4bedf62dddfe4f1817307ed26fb2adfe2861b6ffa97ccd30245"}, "3": {"node_id": "6868a6e6-abd1-4a18-a7d4-0a8bc78aaa29", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "6247c2b7bec0659953bee83111a106e1d5a56b9baf191f9fc90ab09a309c247c"}}, "hash": "0289a609a73837ab3d038af9bb6c727dfaa3e6c4a5073ac05d152186102b8c72", "text": "In the\ncallback, the component updates the states to reflect that the connection is established, any previous errors are\ncleared, and no messages are awaiting responses:\n\n```tsx\nwebsocket.current.onopen = (event) => {  \n  setError(false);  \n  setConnecting(false);  \n  setAwaitingMessage(false);  \n  \n  console.log(\"WebSocket connected:\", event);  \n};\n```\n\n`onmessage`is triggered when a new message is received from the server through the WebSocket connection.In the\ncallback, the received data is parsed and the `messages` state is updated with the new message from the server:\n\n```\nwebsocket.current.onmessage = (event) => {  \n  const data = JSON.parse(event.data);  \n  console.log(\"WebSocket message received:\", data);  \n  setAwaitingMessage(false);  \n  \n  if (data.response) {  \n    // Update the messages state with the new message from the server  \n    setMessages((prevMessages) => [  \n      ...prevMessages,  \n      {  \n        sender_id: \"server\",  \n        message: data.response,  \n        timestamp: new Date().toLocaleTimeString(),  \n      },  \n    ]);  \n  }  \n};\n```\n\n`onclose`is triggered when the WebSocket connection is closed.In the callback, the component checks for a specific\nclose code (`4000`) to display a warning toast and update the component states accordingly.It also logs the close\nevent:\n\n```tsx\nwebsocket.current.onclose = (event) => {  \n  if (event.code === 4000) {  \n    toast.warning(  \n      \"Selected collection's model is unavailable.Was it created properly?\");  \n    setError(true);  \n    setConnecting(false);  \n    setAwaitingMessage(false);  \n  }  \n  console.log(\"WebSocket closed:\", event);  \n};\n```\n\nFinally, `onerror` is triggered when an error occurs with the WebSocket connection.In the callback, the component\nupdates the states to reflect the error and logs the error event:\n\n```tsx\n    websocket.current.onerror = (event) => {\n      setError(true);\n      setConnecting(false);\n      setAwaitingMessage(false);\n\n      console.error(\"WebSocket error:\", event);\n    };\n  ```\n\n#### Rendering our Chat Messages\n\nIn the `ChatView` component, the layout is determined using CSS styling and Material-UI components.The main layout\nconsists of a container with a `flex` display and a column-oriented `flexDirection`.This ensures that the content\nwithin the container is arranged vertically.There are three primary sections within the layout:\n\n1.The chat messages area: This section takes up most of the available space and displays a list of messages exchanged\n   between the user and the server.It has an overflow-y set to \u2018auto\u2019, which allows scrolling when the content\n   overflows the available space.The messages are rendered using the `ChatMessage` component for each message and\n   a `ChatMessageLoading` component to show the loading state while waiting for a server response.2.The divider: A Material-UI `Divider` component is used to separate the chat messages area from the input area,\n   creating a clear visual distinction between the two sections.3.The input area: This section is located at the bottom and allows the user to type and send messages.It contains\n   a `TextField` component from Material-UI, which is set to accept multiline input with a maximum of 2 rows.The input\n   area also includes a `Button` component to send the message.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6868a6e6-abd1-4a18-a7d4-0a8bc78aaa29": {"__data__": {"id_": "6868a6e6-abd1-4a18-a7d4-0a8bc78aaa29", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "49ce9594-5f43-407c-9e20-4366e340ff72", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "0289a609a73837ab3d038af9bb6c727dfaa3e6c4a5073ac05d152186102b8c72"}, "3": {"node_id": "824f712c-18a8-49e1-b746-94906a75383d", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "72ac9c6a226d10c2ddeeb809525e78d51f4b700cf362e760653e10b38da3dc5e"}}, "hash": "6247c2b7bec0659953bee83111a106e1d5a56b9baf191f9fc90ab09a309c247c", "text": "The user can either click the \"Send\" button or press \"\n   Enter\" on their keyboard to send the message.The user inputs accepted in the `ChatView` component are text messages that the user types in the `TextField`.The\ncomponent processes these text inputs and sends them to the server through the WebSocket connection.## Deployment\n\n### Prerequisites\n\nTo deploy the app, you're going to need Docker and Docker Compose installed.If you're on Ubuntu or another, common\nLinux distribution, DigitalOcean has\na [great Docker tutorial](https://www.digitalocean.com/community/tutorial_collections/how-to-install-and-use-docker) and\nanother great tutorial\nfor [Docker Compose](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-20-04)\nyou can follow.If those don't work for you, try\nthe [official docker documentation.](https://docs.docker.com/engine/install/)\n\n### Build and Deploy\n\nThe project is based on django-cookiecutter, and it\u2019s pretty easy to get it deployed on a VM and configured to serve\nHTTPs traffic for a specific domain.The configuration is somewhat involved, however \u2014 not because of this project, but\nit\u2019s just a fairly involved topic to configure your certificates, DNS, etc.For the purposes of this guide, let\u2019s just get running locally.Perhaps we\u2019ll release a guide on production deployment.In the meantime, check out\nthe [Django Cookiecutter project docs](https://cookiecutter-django.readthedocs.io/en/latest/deployment-with-docker.html)\nfor starters.This guide assumes your goal is to get the application up and running for use.If you want to develop, most likely you\nwon\u2019t want to launch the compose stack with the \u2014 profiles fullstack flag and will instead want to launch the react\nfrontend using the node development server.To deploy, first clone the repo:\n\n```commandline\ngit clone https://github.com/yourusername/delphic.git\n```\n\nChange into the project directory:\n\n```commandline\ncd delphic\n```\n\nCopy the sample environment files:\n\n```commandline\nmkdir -p ./.envs/.local/  \ncp -a ./docs/sample_envs/local/.frontend ./frontend  \ncp -a ./docs/sample_envs/local/.django ./.envs/.local  \ncp -a ./docs/sample_envs/local/.postgres ./.envs/.local\n```\n\nEdit the `.django` and `.postgres` configuration files to include your OpenAI API key and set a unique password for your\ndatabase user.You can also set the response token limit in the .django file or switch which OpenAI model you want to\nuse.GPT4 is supported, assuming you\u2019re authorized to access it.Build the docker compose stack with the `--profiles fullstack` flag:\n\n```commandline\nsudo docker-compose --profiles fullstack -f local.yml build\n```\n\nThe fullstack flag instructs compose to build a docker container from the frontend folder and this will be launched\nalong with all of the needed, backend containers.It takes a long time to build a production React container, however,\nso we don\u2019t recommend you develop this way.Follow\nthe [instructions in the project readme.md](https://github.com/JSv4/Delphic#development) for development environment\nsetup instructions.Finally, bring up the application:\n\n```commandline\nsudo docker-compose -f local.yml up\n```\n\nNow, visit `localhost:3000` in your browser to see the frontend, and use the Delphic application locally.## Using the Application\n\n### Setup Users\n\nIn order to actually use the application (at the moment, we intend to make it possible to share certain models with\nunauthenticated users), you need a login.You can use either a superuser or non-superuser.In either case, someone needs\nto first create a superuser using the console:\n\n**Why set up a Django superuser?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "824f712c-18a8-49e1-b746-94906a75383d": {"__data__": {"id_": "824f712c-18a8-49e1-b746-94906a75383d", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "555f3c16c59f113cc8610556d708419d4f9e4a0e83239e5316352b4d5fcfffe5"}, "2": {"node_id": "6868a6e6-abd1-4a18-a7d4-0a8bc78aaa29", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}, "hash": "6247c2b7bec0659953bee83111a106e1d5a56b9baf191f9fc90ab09a309c247c"}}, "hash": "72ac9c6a226d10c2ddeeb809525e78d51f4b700cf362e760653e10b38da3dc5e", "text": "** A Django superuser has all the permissions in the application and can manage all\naspects of the system, including creating, modifying, and deleting users, collections, and other data.Setting up a\nsuperuser allows you to fully control and manage the application.**How to create a Django superuser:**\n\n1 Run the following command to create a superuser:\n\nsudo docker-compose -f local.yml run django python manage.py createsuperuser\n\n2 You will be prompted to provide a username, email address, and password for the superuser.Enter the required\ninformation.**How to create additional users using Django admin:**\n\n1.Start your Delphic application locally following the deployment instructions.2.Visit the Django admin interface by navigating to `http://localhost:8000/admin` in your browser.3.Log in with the superuser credentials you created earlier.4.Click on \u201cUsers\u201d under the \u201cAuthentication and Authorization\u201d section.5.Click on the \u201cAdd user +\u201d button in the top right corner.6.Enter the required information for the new user, such as username and password.Click \u201cSave\u201d to create the user.7.To grant the new user additional permissions or make them a superuser, click on their username in the user list,\n   scroll down to the \u201cPermissions\u201d section, and configure their permissions accordingly.Save your changes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d35927e0-0092-4e38-b844-9f1d050d2862": {"__data__": {"id_": "d35927e0-0092-4e38-b844-9f1d050d2862", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3756835c7d1eaf6d708e55b30ffc2cb879bb429c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots.md", "author": "LlamaIndex"}, "hash": "810a23329f2b6e42edb8c2123659711ebb56d288c067170818d643dad4ed6afe"}}, "hash": "59b1cf51c868749231e0562d8c99a5a354e00bfc7b8bc2b3ce721a5a657c3a47", "text": "# Chatbots\n\nChatbots are an incredibly popular use case for LLM's. LlamaIndex gives you the tools to build Knowledge-augmented chatbots and agents.\n\nRelevant Resources:\n- [Building a Chatbot](/end_to_end_tutorials/chatbots/building_a_chatbot.md)\n- [Using with a LangChain Agent](/community/integrations/using_with_langchain.md)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "efa12611-2447-4d58-8a08-26a3704535b8": {"__data__": {"id_": "efa12611-2447-4d58-8a08-26a3704535b8", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13037c457040479ca229000a12561167ba21e892", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "8138abfbb52da1559f77580a1a67aa7604adab9e80a56cb9944b46e36a57096d"}, "3": {"node_id": "8a8f7393-61ee-48f4-a416-8165f53882ac", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "e5c2a8f710cfafd24fd608163df09005bc4f022c25ae602440fdc75492963bdb"}}, "hash": "a943d7c12a844c9fa57217796c5838b8dfd46ca375b22353e602189f12232204", "text": "# \ud83d\udcac\ud83e\udd16 How to Build a Chatbot\n\nLlamaIndex is an interface between your data and LLM's; it offers the toolkit for you to setup a query interface around your data for any downstream task, whether it's question-answering, summarization, or more.In this tutorial, we show you how to build a context augmented chatbot.We use Langchain for the underlying Agent/Chatbot abstractions, and we use LlamaIndex for the data retrieval/lookup/querying!The result is a chatbot agent that has access to a rich set of \"data interface\" Tools that LlamaIndex provides to answer queries over your data.**Note**: This is a continuation of some initial work building a query interface over SEC 10-K filings - [check it out here](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d).### Context\n\nIn this tutorial, we build an \"10-K Chatbot\" by downloading the raw UBER 10-K HTML filings from Dropbox.The user can choose to ask questions regarding the 10-K filings.### Ingest Data\n\nLet's first download the raw 10-k files, from 2019-2022.```python\n# NOTE: the code examples assume you're operating within a Jupyter notebook.# download files\n!mkdir data\n!wget \"https://www.dropbox.com/s/948jr9cfs7fgj99/UBER.zip?dl=1\" -O data/UBER.zip\n!unzip data/UBER.zip -d data\n\n```\n\nWe use the [Unstructured](https://github.com/Unstructured-IO/unstructured) library to parse the HTML files into formatted text.We have a direct integration with Unstructured through [LlamaHub](https://llamahub.ai/) - this allows us to convert any text into a Document format that LlamaIndex can ingest.```python\n\nfrom llama_index import download_loader, VectorStoreIndex, ServiceContext, StorageContext, load_index_from_storage\nfrom pathlib import Path\n\nyears = [2022, 2021, 2020, 2019]\nUnstructuredReader = download_loader(\"UnstructuredReader\", refresh_cache=True)\n\nloader = UnstructuredReader()\ndoc_set = {}\nall_docs = []\nfor year in years:\n    year_docs = loader.load_data(file=Path(f'./data/UBER/UBER_{year}.html'), split_documents=False)\n    # insert year metadata into each year\n    for d in year_docs:\n        d.metadata = {\"year\": year}\n    doc_set[year] = year_docs\n    all_docs.extend(year_docs)\n```\n\n### Setting up Vector Indices for each year\n\nWe first setup a vector index for each year.Each vector index allows us \nto ask questions about the 10-K filing of a given year.We build each index and save it to disk.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a8f7393-61ee-48f4-a416-8165f53882ac": {"__data__": {"id_": "8a8f7393-61ee-48f4-a416-8165f53882ac", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13037c457040479ca229000a12561167ba21e892", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "8138abfbb52da1559f77580a1a67aa7604adab9e80a56cb9944b46e36a57096d"}, "2": {"node_id": "efa12611-2447-4d58-8a08-26a3704535b8", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "a943d7c12a844c9fa57217796c5838b8dfd46ca375b22353e602189f12232204"}, "3": {"node_id": "657efc36-df83-4e56-933f-89039a0395a5", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "b73161384c4eb326e2ce3f0b40aa73b59494723a401cf5bd2de5d8cb7ce1a0f4"}}, "hash": "e5c2a8f710cfafd24fd608163df09005bc4f022c25ae602440fdc75492963bdb", "text": "```python\n# initialize simple vector indices + global vector index\nservice_context = ServiceContext.from_defaults(chunk_size=512)\nindex_set = {}\nfor year in years:\n    storage_context = StorageContext.from_defaults()\n    cur_index = VectorStoreIndex.from_documents(\n        doc_set[year], \n        service_context=service_context,\n        storage_context=storage_context,\n    )\n    index_set[year] = cur_index\n    storage_context.persist(persist_dir=f'./storage/{year}')\n\n```\n\nTo load an index from disk, do the following\n```python\n# Load indices from disk\nindex_set = {}\nfor year in years:\n    storage_context = StorageContext.from_defaults(persist_dir=f'./storage/{year}')\n    cur_index = load_index_from_storage(storage_context=storage_context)\n    index_set[year] = cur_index\n```\n\n\n### Composing a Graph to Synthesize Answers Across 10-K Filings\n\nSince we have access to documents of 4 years, we may not only want to ask questions regarding the 10-K document of a given year, but ask questions that require analysis over all 10-K filings.To address this, we compose a \"graph\" which consists of a summary index defined over the 4 vector indices.Querying this graph would first retrieve information from each vector index, and combine information together via the summary index.```python\nfrom llama_index import SummaryIndex, LLMPredictor, ServiceContext, load_graph_from_storage\nfrom llama_index.llms import OpenAI\nfrom llama_index.indices.composability import ComposableGraph\n\n# describe each index to help traversal of composed graph\nindex_summaries = [f\"UBER 10-k Filing for {year} fiscal year\" for year in years]\n\n# define an LLMPredictor set number of output tokens\nllm = OpenAI(temperature=0, max_tokens=512, model=\"gpt-4\")\nservice_context = ServiceContext.from_defaults(llm=llm)\nstorage_context = StorageContext.from_defaults()\n\n# define a summary index over the vector indices\n# allows us to synthesize information across each index\ngraph = ComposableGraph.from_indices(\n    SummaryIndex,\n    [index_set[y] for y in years], \n    index_summaries=index_summaries,\n    service_context=service_context,\n    storage_context = storage_context,\n)\nroot_id = graph.root_id\n\n# [optional] save to disk\nstorage_context.persist(persist_dir=f'./storage/root')\n\n# [optional] load from disk, so you don't need to build graph from scratch\ngraph = load_graph_from_storage(\n    root_id=root_id, \n    service_context=service_context,\n    storage_context=storage_context,\n)\n\n```\n\n### Setting up the Tools + Langchain Chatbot Agent\n\nWe use Langchain to setup the outer chatbot agent, which has access to a set of Tools.LlamaIndex provides some wrappers around indices and graphs so that they can be easily used within a Tool interface.```python\n# do imports\nfrom langchain.chains.conversation.memory import ConversationBufferMemory\nfrom langchain.agents import initialize_agent\n\nfrom llama_index import LLMPredictor\nfrom llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig\n```\n\nWe want to define a separate Tool for each index (corresponding to a given year), as well \nas the graph.We can define all tools under a central `LlamaToolkit` interface.Below, we define a `IndexToolConfig` for our graph.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "657efc36-df83-4e56-933f-89039a0395a5": {"__data__": {"id_": "657efc36-df83-4e56-933f-89039a0395a5", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13037c457040479ca229000a12561167ba21e892", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "8138abfbb52da1559f77580a1a67aa7604adab9e80a56cb9944b46e36a57096d"}, "2": {"node_id": "8a8f7393-61ee-48f4-a416-8165f53882ac", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "e5c2a8f710cfafd24fd608163df09005bc4f022c25ae602440fdc75492963bdb"}, "3": {"node_id": "72a3bcd2-7945-444e-965b-c18a766f7a5d", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "b40caad1fa14017a011e7e81edd35fed396a4e8b76e3d7541790870a3ff99b96"}}, "hash": "b73161384c4eb326e2ce3f0b40aa73b59494723a401cf5bd2de5d8cb7ce1a0f4", "text": "Note that we also import a `DecomposeQueryTransform` module for use within each vector index within the graph - this allows us to \"decompose\" the overall query into a query that can be answered from each subindex.(see example below).```python\n# define a decompose transform\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    LLMPredictor(llm=llm), verbose=True\n)\n\n# define custom retrievers\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\n\ncustom_query_engines = {}\nfor index in index_set.values():\n    query_engine = index.as_query_engine()\n    query_engine = TransformQueryEngine(\n        query_engine,\n        query_transform=decompose_transform,\n        transform_extra_info={'index_summary': index.index_struct.summary},\n    )\n    custom_query_engines[index.index_id] = query_engine\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n    response_mode='tree_summarize',\n    verbose=True,\n)\n\n# construct query engine\ngraph_query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\n\n# tool config\ngraph_config = IndexToolConfig(\n    query_engine=graph_query_engine,\n    name=f\"Graph Index\",\n    description=\"useful for when you want to answer queries that require analyzing multiple SEC 10-K documents for Uber.\",\n    tool_kwargs={\"return_direct\": True}\n)\n```\n\nBesides the `IndexToolConfig` object for the graph, we also define an `IndexToolConfig` corresponding to each index:\n\n```python\n# define toolkit\nindex_configs = []\nfor y in range(2019, 2023):\n    query_engine = index_set[y].as_query_engine(\n        similarity_top_k=3,\n    )\n    tool_config = IndexToolConfig(\n        query_engine=query_engine, \n        name=f\"Vector Index {y}\",\n        description=f\"useful for when you want to answer queries about the {y} SEC 10-K for Uber\",\n        tool_kwargs={\"return_direct\": True}\n    )\n    index_configs.append(tool_config)\n```\n\nFinally, we combine these configs with our `LlamaToolkit`: \n\n```python\ntoolkit = LlamaToolkit(\n    index_configs=index_configs + [graph_config],\n)\n```\n\n\nFinally, we call `create_llama_chat_agent` to create our Langchain chatbot agent, which\nhas access to the 5 Tools we defined above:\n\n```python\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\nllm=OpenAI(temperature=0)\nagent_chain = create_llama_chat_agent(\n    toolkit,\n    llm,\n    memory=memory,\n    verbose=True\n)\n```\n\n### Testing the Agent\n\nWe can now test the agent with various queries.If we test it with a simple \"hello\" query, the agent does not use any Tools.```python\nagent_chain.run(input=\"hi, i am bob\")\n```\n\n```\n> Entering new AgentExecutor chain...Thought: Do I need to use a tool?No\nAI: Hi Bob, nice to meet you!How can I help you today?> Finished chain.'Hi Bob, nice to meet you!How can I help you today?'", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "72a3bcd2-7945-444e-965b-c18a766f7a5d": {"__data__": {"id_": "72a3bcd2-7945-444e-965b-c18a766f7a5d", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13037c457040479ca229000a12561167ba21e892", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "8138abfbb52da1559f77580a1a67aa7604adab9e80a56cb9944b46e36a57096d"}, "2": {"node_id": "657efc36-df83-4e56-933f-89039a0395a5", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "b73161384c4eb326e2ce3f0b40aa73b59494723a401cf5bd2de5d8cb7ce1a0f4"}, "3": {"node_id": "1d32a6ac-f244-4992-bd34-440a2182a532", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "37724116fd79334c8471fc4693766e1d5bcfdc1820b6cf608447eca8fe921b0e"}}, "hash": "b40caad1fa14017a011e7e81edd35fed396a4e8b76e3d7541790870a3ff99b96", "text": "```\n\nIf we test it with a query regarding the 10-k of a given year, the agent will use\nthe relevant vector index Tool.```python\nagent_chain.run(input=\"What were some of the biggest risk factors in 2020 for Uber?\")```\n\n```\n> Entering new AgentExecutor chain...Thought: Do I need to use a tool?Yes\nAction: Vector Index 2020\nAction Input: Risk Factors\n...Observation: \n\nRisk Factors\n\nThe COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business, financial condition, and results of operations....\n'\\n\\nRisk Factors\\n\\nThe COVID-19 pandemic and the impact of actions to mitigate the pandemic has adversely affected and continues to adversely affect our business,\n\n```\n\nFinally, if we test it with a query to compare/contrast risk factors across years,\nthe agent will use the graph index Tool.```python\ncross_query_str = (\n    \"Compare/contrast the risk factors described in the Uber 10-K across years.Give answer in bullet points.\")\nagent_chain.run(input=cross_query_str)\n```\n\n```\n> Entering new AgentExecutor chain...Thought: Do I need to use a tool?Yes\nAction: Graph Index\nAction Input: Compare/contrast the risk factors described in the Uber 10-K across years.> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2022 fiscal year?INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 964 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n> Got response: \nThe risk factors described in the Uber 10-K for the 2022 fiscal year include: the potential for changes in the classification of Drivers, the potential for increased competition, the potential for...\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2021 fiscal year?INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 590 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n> Got response: \n1.The COVID-19 pandemic and the impact of actions to mitigate the pandemic have adversely affected and may continue to adversely affect parts of our business.2.Our business would be adversely ...\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2020 fiscal year?INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 516 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\n> Got response: \nThe risk factors described in the Uber 10-K for the 2020 fiscal year include: the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental ...\n> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?> Current query: Compare/contrast the risk factors described in the Uber 10-K across years.> New query:  What are the risk factors described in the Uber 10-K for the 2019 fiscal year?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1d32a6ac-f244-4992-bd34-440a2182a532": {"__data__": {"id_": "1d32a6ac-f244-4992-bd34-440a2182a532", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13037c457040479ca229000a12561167ba21e892", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "8138abfbb52da1559f77580a1a67aa7604adab9e80a56cb9944b46e36a57096d"}, "2": {"node_id": "72a3bcd2-7945-444e-965b-c18a766f7a5d", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}, "hash": "b40caad1fa14017a011e7e81edd35fed396a4e8b76e3d7541790870a3ff99b96"}}, "hash": "37724116fd79334c8471fc4693766e1d5bcfdc1820b6cf608447eca8fe921b0e", "text": "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 1020 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 18 tokens\nINFO:llama_index.indices.common.tree.base:> Building index from nodes: 0 chunks\n> Got response: \nRisk factors described in the Uber 10-K for the 2019 fiscal year include: competition from other transportation providers; the impact of government regulations; the impact of litigation; the impac...\nINFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 7039 tokens\nINFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 72 tokens\n\nObservation: \nIn 2020, the risk factors included the timing of widespread adoption of vaccines against the virus, additional actions that may be taken by governmental authorities, the further impact on the business of Drivers\n\n...\n\n```\n\n\n### Setting up the Chatbot Loop\n\nNow that we have the chatbot setup, it only takes a few more steps to setup a basic interactive loop to converse with our SEC-augmented chatbot! \n\n```python\nwhile True:\n    text_input = input(\"User: \")\n    response = agent_chain.run(input=text_input)\n    print(f'Agent: {response}')\n    \n```\n\nHere's an example of the loop in action:\n```\nUser:  What were some of the legal proceedings against Uber in 2022?\nAgent: \n\nIn 2022, legal proceedings against Uber include a motion to compel arbitration, an appeal of a ruling that Proposition 22 is unconstitutional, a complaint alleging that drivers are employees and entitled to protections under the wage and labor laws, a summary judgment motion, allegations of misclassification of drivers and related employment violations in New York, fraud related to certain deductions, class actions in Australia alleging that Uber entities conspired to injure the group members during the period 2014 to 2017 by either directly breaching transport legislation or commissioning offenses against transport legislation by UberX Drivers in Australia, and claims of lost income and decreased value of certain taxi. Additionally, Uber is facing a challenge in California Superior Court alleging that Proposition 22 is unconstitutional, and a preliminary injunction order prohibiting Uber from classifying Drivers as independent contractors and from violating various wage and hour laws.\n\nUser: \n\n```\n\n### Notebook\n\nTake a look at our [corresponding notebook](https://github.com/jerryjliu/llama_index/blob/main/examples/chatbot/Chatbot_SEC.ipynb).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "01a3a473-0083-40fa-8a11-75f394861e2d": {"__data__": {"id_": "01a3a473-0083-40fa-8a11-75f394861e2d", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\component_wise_evaluation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd8f1ca277aa1815e930b0daa1b13b2f678cfe96", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\component_wise_evaluation.md", "author": "LlamaIndex"}, "hash": "7bd4eb3e432f8494d791f8076e7fb0ce7b7639a5ff5dddc197eac8c6eebce112"}}, "hash": "4dbb499390887428a0c8568a3f46a5a2d56d14c8c82dfceaea91bb40378f981e", "text": "# Component Wise Evaluation\nTo do more in-depth evaluation of your pipeline, it helps to break it down into an evaluation of individual components. \n\nFor instance, a particular failure case may be due to a combination of not retrieving the right documents and also the LLM misunderstanding the context and hallucinating an incorrect result. Being able to isolate and deal with these issues separately can help reduce complexity and guide you in a step-by-step manner to a more satisfactory overall result.\n\n## Utilizing public benchmarks\nWhen doing initial model selection, it helps to look at how well the model is performing on a standardized, diverse set of domains or tasks.\n\nA useful benchmark for embeddings is the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n\n## Evaluating Retrieval\n### BEIR dataset\n\nBEIR is useful for benchmarking if a particular retrieval model generalize well to niche domains in a zero-shot setting.\n\nSince most publically-available embedding and retrieval models are already benchmarked against BEIR (e.g. through the MTEB benchmark), utilizing BEIR is more helpful when you have a unique model that you want to evaluate. \n\nFor instance, after fine-tuning an embedding model on your dataset, it may be helpful to view whether and by how much its performance degrades on a diverse set of domains. This can be an indication of how much data drift may affect your retrieval accuracy, such as if you add documents to your RAG system outside of your fine-tuning training distribution.\n\nHere is a notebook showing how the BEIR dataset can be used with your retrieval pipeline.\n\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/evaluation/BeirEvaluation.ipynb\n```\n\nWe will be adding more methods to evaluate retrieval soon. This includes evaluating retrieval on your own dataset.\n\n## Evaluating the Query Engine Components (e.g. Without Retrieval)\n\nIn this case, we may want to evaluate how specific components of a query engine (one which may generate sub-questions or follow-up questions) may perform on a standard benchmark. It can help give an indication of how far behind or ahead your retrieval pipeline is compared to alternate pipelines or models.\n\n### HotpotQA Dataset\n\nThe HotpotQA dataset is useful for evaluating queries that require multiple retrieval steps.\n\nExample:\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/evaluation/HotpotQADistractor.ipynb\n```\n\nLimitations:\n\n1. HotpotQA is evaluated on a Wikipedia corpus. LLMs, especially GPT4, tend to have memorized information from Wikipedia relatively well. Hence, the benchmark is not particularly good for evaluating retrieval + rerank systems with knowledgeable models like GPT4.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "15707a2c-3f1e-4723-86fd-67091dcf67ba": {"__data__": {"id_": "15707a2c-3f1e-4723-86fd-67091dcf67ba", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\development_pathway.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "21b8dd44a82617ea07b8f295ca1d583d2541b8f1", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\development_pathway.md", "author": "LlamaIndex"}, "hash": "eac956911fca435af5ec4939f2a8b81b5969f3d394a5fdb74245e807103f751a"}, "3": {"node_id": "8c778fa6-168d-4457-a731-fcefc050a6d3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\development_pathway.md", "author": "LlamaIndex"}, "hash": "db85ad406ce9957c2cebb636256c0fc57a5c15997213b97f2c104dc1469f4e19"}}, "hash": "29219bc57f7adc0b338dab601f016d1b84d51537a7cde9102a4489f0955fe3ab", "text": "# The Development Pathway\n\nIn your journey to developing an LLM application, it helps to start with a discovery phase of understanding your data and doing some identification of issues and corner cases as you interact with the system.Over time, you would try to formalize processes and evaluation methodology, setting up tools for observability, debugging and experiment tracking, and eventually production monitoring.Below, we provide some additional guidance on considerations and hurdles you may face when developing your application.## The Challenges of Building a Production-Ready LLM Application\nMany who are interested in the LLM application space are not machine learning engineers but rather software developers or  even non-technical folk.One of the biggest strides forward that LLMs and foundation models have made to the AI/ML application landscape is that it makes it really easy to go from idea to prototype without facing all of the hurdles and uncertainty of a traditional machine learning project.This would have involved collecting, exploring and cleaning data, keeping up with latest research and exploring different methods, training models, adjusting hyperparameters, and dealing with unexpected issues in model quality.The huge infrastructure burden, long development cycle, and high risk to reward ratio have been blockers to successful applications.At the same time, despite the fact that getting a prototype working quickly through frameworks like LlamaIndex has become a lot more accessible, deploying a machine learning product in the real world is still rife with uncertainty and challenges.### Quality and User Interaction\nOn the tamer side, one may face quality issues, and in the worse case, one may be liable to losing user trust if the application proves itself to be unreliable.We've already seen a bit of this with ChatGPT - despite its life-likeness and seeming ability to understand our conversations and requests, it often makes things up (\"hallucinates\").It's not connected to the real world, data, or other digital applications.It is important to be able to monitor, track and improve against quality issues.## Tradeoffs in LLM Application Development\nThere are a few tradeoffs in LLM application development:\n1.**Cost** - more powerful models may be more expensive\n2.**Latency** - more powerful models may be slower\n3.**Simplicity** (one size fits all) - how powerful and flexible is the model / pipeline?4.**Reliability / Useability** - is my application working at least in the general case?Is it ready for unstructured user interaction?Have I covered the major usage patterns?LLM infra improvements are progressing quickly and we expect cost and latency to go down over time.Here are some additional concerns:\n1.**Evaluation** - Once I start diving deeper into improving quality, how can I evaluate individual components?How can I keep track of issues and track whether / how they are being improved over time as I change my application?2.**Data-Driven** - How can I automate more of my evaluation and iteration process?How do I start small and add useful data points over time?How can I organize different datasets and metrics which serving different purposes?How can I manage the complexity while keeping track of my guiding light of providing the best user experience?3.**Customization / Complexity Tradeoff** - How do I improve each stage of the pipeline - preprocessing and feature extraction, retrieval, generation?Does this involve adding additional structure or processing?How can I break down this goal into more measurable and trackable sub-goals?Differences between **Evaluation** and being **Data-Driven**:\n1.**Evaluation** does not necessarily have to be rigorous or fully data-driven process - especially at the initial stages.It is more concerned with the initial *development* phase of the application - validating that the overall pipeline works in the general case and starting to define possible signals and metrics which may be carried forward into production.2.Being **Data-Driven** is closely tied to *automation*.After we've chosen our basic application structure, how can we improve the system over time?How can we ensure quality in a systematic way?How can we reduce the cost of monitoring, and what are the pathways to adding and curating data points?How can we leverage ML systems (including but not limited to LLMs) to make this process easier?Additional considerations:\n1.**Privacy** - how can I ensure that my data is not leaked if I am feeding it into these models?What infrastructure am I using and what is the security guarantee / how is the access control structured?## Development Hurdles\n\nHere are some potential problems you may encounter when developing your LLM application which may lead to unsatisfactory results.### Retrieval\n\n1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8c778fa6-168d-4457-a731-fcefc050a6d3": {"__data__": {"id_": "8c778fa6-168d-4457-a731-fcefc050a6d3", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\development_pathway.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "21b8dd44a82617ea07b8f295ca1d583d2541b8f1", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\development_pathway.md", "author": "LlamaIndex"}, "hash": "eac956911fca435af5ec4939f2a8b81b5969f3d394a5fdb74245e807103f751a"}, "2": {"node_id": "15707a2c-3f1e-4723-86fd-67091dcf67ba", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\development_pathway.md", "author": "LlamaIndex"}, "hash": "29219bc57f7adc0b338dab601f016d1b84d51537a7cde9102a4489f0955fe3ab"}}, "hash": "db85ad406ce9957c2cebb636256c0fc57a5c15997213b97f2c104dc1469f4e19", "text": "**Out of Domain:**\nIf your data is extremely specific (medical, legal, scientific, financial, or other documents with technical lingo), it may be worth:\n    - trying out alternate embeddings \n      - Check the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n      - You may configure a local embedding model [with the steps here](local-embedding-models)\n    - testing out fine-tuning of embeddings\n        - Tools: [setfit](https://github.com/huggingface/setfit)\n        - Anecdotally, we have seen retrieval accuracy improve by ~12% by curating a small annotated dataset from production data\n        - Even synthetic data generation without human labels has been shown to improve retrieval metrics across similar documents in train / val sets.- More detailed guides and case studies will come soon.- testing out sparse retrieval methods (see ColBERT, SPLADE)\n        - these methods have been shown to generalize well to out of domain data\n        - that are starting to be available in some enterprise systems (e.g.[Elastic Search's ELSeR](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html))\n    - checking out our [evaluation principles guide](Evaluation) on how you might evaluate the above changes", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "52e7f52d-519c-48fd-85e3-73a0d296b135": {"__data__": {"id_": "52e7f52d-519c-48fd-85e3-73a0d296b135", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\e2e_evaluation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ebf0bea9e9892ed101bd9eb7114fe44a4d4679cb", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\e2e_evaluation.md", "author": "LlamaIndex"}, "hash": "ae645c554e34845bf057f39425a8f5d8f30fed97927939f851d0b4aa88635bd5"}}, "hash": "0dd2ac184799a0582127610a808432fd6215443a12567ee83cf1207b19bf5d11", "text": "# End-to-End Evaluation\nEnd-to-End evaluation should be the guiding signal for your RAG application - will my pipeline generate the right responses given the data sources and a set of queries?\n\nWhile it helps initially to individually inspect queries and responses, as you deal with more failure and corner cases, it may stop being feasible to look at each query individually, and rather it may help instead to define a set of summary metrics or automated evaluation, and gain an intuition for what they might be telling you and where you might dive deeper.\n\n## Setting up an Evaluation Set\n\nIt is helpful to start off with a small but diverse set of queries, and build up more examples as one discovers problematic queries or interactions.\n\nWe've created some tools that automatically generate a dataset for you given a set of documents to query. (See example below).\n\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/evaluation/QuestionGeneration.ipynb\n```\n\nIn the future, we will also be able to create datasets automatically against tools.\n\n## The Spectrum of Evaluation Options\n\nQuantitative eval is more useful when evaluating applications where there is a correct answer - for instance, validating that the choice of tools and their inputs are correct given the plan, or retrieving specific pieces of information, or attempting to produce intermediate output of a certain schema (e.g. JSON fields).\n\nQualitative eval is more useful when generating long-form responses that are meant to be *helpful* but not necessarily completely accurate.\n\nThere is a spectrum of evaluation options ranging from metrics, cheaper models, more expensive models (GPT4), and human evaluation.\n\nBelow is some example usage of the [evaluation modules](evaluation):\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/evaluation/TestNYC-Evaluation-Query.ipynb\n/examples/evaluation/TestNYC-Evaluation.ipynb\n```\n\n\n## Discovery - Sensitivity Testing\n\nWith a complex pipeline, it may be unclear which parts of the pipeline are affecting your results.\n\nSensitivity testing can be a good inroad into choosing which components to individually test or tweak more thoroughly, or which parts of your dataset (e.g. queries) may be producing problematic results.\n\nMore details on how to discover issues automatically with methods such as sensitivity testing will come soon.\n\nExamples of this in the more traditional ML domain include [Giskard](https://docs.giskard.ai/en/latest/getting-started/quickstart.html).\n\n## Metrics Ensembling\n\nIt may be expensive to use GPT-4 to carry out evaluation especially as your dev set grows large.\n\nMetrics ensembling uses an ensemble of weaker signals (exact match, F1, ROUGE, BLEU, BERT-NLI and BERT-similarity) to predict the output of a more expensive evaluation methods that are closer to the gold labels (human-labelled/GPT-4).\n\nIt is intenteded for two purposes:\n\n1. Evaluating changes cheaply and quickly across a large dataset during the development stage.\n2. Flagging outliers for further evaluation (GPT-4 / human alerting) during the production monitoring stage.\n\nWe also want the metrics ensembling to be interpretable - the correlation and weighting scores should give an indication of which metrics best capture the evaluation criteria.\n\nWe will discuss more about the methodology in future updates.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2fb6d4f0-f671-424e-8640-8accc6adab3b": {"__data__": {"id_": "2fb6d4f0-f671-424e-8640-8accc6adab3b", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\evaluation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d29918512506aca78286a2e0b139a9fc88580df", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\evaluation.md", "author": "LlamaIndex"}, "hash": "2338363c86b7f669ff4d137527f0297e021b0bb37b1782ed33366dcec2e7d4f6"}}, "hash": "c9ec9783a0edc9c950b3ef3442f152cd6a98ea0283b9c3505548f79efa707f9f", "text": "(Evaluation)=\n# Evaluation\n\n## Setting the Stage\n\nLlamaIndex is meant to connect your data to your LLM applications.\n\nSometimes, even after diagnosing and fixing bugs by looking at traces, more fine-grained evaluation is required to systematically diagnose issues.\n\nLlamaIndex aims to provide those tools to make identifying issues and receiving useful diagnostic signals easy.\n\nClosely tied to evaluation are the concepts of experimentation and experiment tracking.\n\n## General Strategy\n\nWhen developing your LLM application, it could help to first define an end-to-end evaluation workflow, and then once you've started collecting failure or corner cases and getting an intuition for what is or isn't going well, you may dive deeper into evaluating and improving specific components. \n\nThe analogy with software testing is integration tests and unit tests. You should probably start writing unit tests once you start fiddling with individual components. Equally, your gold standard on whether things are working will together are integration tests. Both are equally important.\n\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/end_to_end_tutorials/dev_practices/e2e_evaluation.md\n/end_to_end_tutorials/dev_practices/component_wise_evaluation.md\n```\n\nHere is an overview of the existing modules for evaluation. We will be adding more modules and support over time.\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/core_modules/supporting_modules/evaluation/root.md\n```\n\n### E2E or Component-Wise - Which Do I Start With?\nIf you want to get an overall idea of how your system is doing as you iterate upon it, it makes sense to start with centering your core development loop around the e2e eval - as an overall sanity/vibe check.\n\nIf you have an idea of what you're doing and want to iterate step by step on each component, building it up as things go - you may want to start with a component-wise eval. However this may run the risk of premature optimization - making model selection or parameter choices without assessing the overall application needs. You may have to revisit these choices when creating your final application.\n\n\n\n## Diving Deeper into Evaluation\nEvaluation is a controversial topic, and as the field of NLP has evolved, so have the methods of evaluation.\n\nIn a world where powerful foundation models are now performing annotation tasks better than human annotators, the best practices around evaluation are constantly changing. Previous methods of evaluation which were used to bootstrap and evaluate today's models such as BLEU or F1 have been shown to have poor correlation with human judgements, and need to be applied prudently.\n\nTypically, generation-heavy, open-ended tasks and requiring judgement or opinion and harder to evaluate automatically than factual questions due to their subjective nature. We will aim to provide more guides and case-studies for which methods are appropriate in a given scenario.\n\n### Standard Metrics\n\nAgainst annotated datasets, whether your own data or an academic benchmark, there are a number of standard metrics that it helps to be aware of:\n\n1. **Exact Match (EM):** The percentage of queries that are answered exactly correctly.\n2. **F1:** The percentage of queries that are answered exactly correctly or with a small edit distance (e.g. 1-2 words).\n3. **Recall:** The percentage of queries that are answered correctly, regardless of the number of answers returned.\n4. **Precision:** The percentage of queries that are answered correctly, divided by the number of answers returned.\n\nThis [towardsdatascience article](https://towardsdatascience.com/ranking-evaluation-metrics-for-recommender-systems-263d0a66ef54) covers more technical metrics like NDCG, MAP and MRR in greater depth.\n\n\n## Case Studies and Resources\n1. (Course) [Data-Centric AI (MIT), 2023](https://www.youtube.com/playlist?list=PLnSYPjg2dHQKdig0vVbN-ZnEU0yNJ1mo5)\n2. [Scale's Approach to LLM Testing and Evaluation](https://scale.com/llm-test-evaluation)\n3. [LLM Patterns by Eugene Yan](https://eugeneyan.com/writing/llm-patterns/)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "96dca026-efdc-4ac7-90bf-906b643f1573": {"__data__": {"id_": "96dca026-efdc-4ac7-90bf-906b643f1573", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\monitoring.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfafd62b0e9f5cdb3eb121d03a96198ae64feff9", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\monitoring.md", "author": "LlamaIndex"}, "hash": "eabe956d8a12f35d8a74c89ce23144c3da32e767bbd73a9ddf50496d598427f3"}}, "hash": "8bb44d40620f4354ebe12d9a8832cd965ac0ab7ac33713fc20cb0575f012c9c4", "text": "# Monitoring\n## Why Monitoring?\nWhen developing your LLM application, it can be helpful to keep track of production data such as:\n-   Pipeline performance (latency/token count/throughput of various stages)\n-   Resource usage (LLM/Embedding Inference Cost, CPU/GPU utilization)\n-   Evaluation metrics (accuracy, precision, recall, qualitative eval and drift)\n-   Pipeline versioning (which versions of sub-components e.g. LLM/embedding &  artifacts e.g. prompts were used in the pipeline at a given time)\n\nWe will share more on how to set up monitoring in the future.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a27331a1-1bad-4ccd-9bd0-429465d80172": {"__data__": {"id_": "a27331a1-1bad-4ccd-9bd0-429465d80172", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\observability.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3554ad15f7802465c8587c648b3494e572ab167", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\observability.md", "author": "LlamaIndex"}, "hash": "745adf34206a0dd72fbb5b7a6b0a5fc53f57a7a310b9f2c17e14febceaf2ac19"}}, "hash": "f3ee53f6ae3a67f583202e210b25902cd5dd562c3be54ceaf56b149725644a22", "text": "# Observability\n## Why Observability?\n\nIn a complex LLM application with many moving parts, as with traditional software engineering, it helps to be able to inspect the artifacts and execution traces of the application.\n\n## How to Set Up Observability\n\nYou may refer to our [\"One-Click Observability\" guide](one-click-observability) to set up observability with your preferred observability provider.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6dcacd63-f1ce-4905-a2b3-a367a12576da": {"__data__": {"id_": "6dcacd63-f1ce-4905-a2b3-a367a12576da", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\production_rag.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d53d8b47de2c7a38bcb395a4c7e80275684dff56", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\production_rag.md", "author": "LlamaIndex"}, "hash": "436a5bdad83d762d174d2d321307650b30c7c1a75024c4ca5dd9fe734359a51d"}, "3": {"node_id": "a0d2eca7-f20c-49eb-9cd7-6a80d576903c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\production_rag.md", "author": "LlamaIndex"}, "hash": "45079b1b4fe5356c59fb205d2b7f4846d1bfd15ddd5bb4dedff3c5a90ab81bfd"}}, "hash": "30fcc40a65b4e0ddfbcb679197b3d3a85469b7ad7d0f833920306039095a0999", "text": "# Building Performant RAG Applications for Production\n\nPrototyping a RAG application is easy, but making it performant, robust, and scalable to a large knowledge corpus is hard.\n\nThis guide contains a variety of tips and tricks to improve the performance of your RAG pipeline. We first outline \nsome general techniques - they are loosely ordered in terms of most straightforward to most challenging.\nWe then dive a bit more deeply into each technique, the use cases that it solves,\nand how to implement it with LlamaIndex! \n\nThe end goal is to optimize your retrieval and generation performance to answer more \nqueries over more complex datasets accurately and without hallucinations.\n\n## General Techniques for Building Production-Grade RAG\n\nHere are some top Considerations for Building Production-Grade RAG\n\n- Decoupling chunks used for retrieval vs. chunks used for synthesis\n- Structured Retrieval for Larger Document Sets\n- Dynamically Retrieve Chunks Depending on your Task\n- Optimize context embeddings\n\nWe discussed this and more during our [Production RAG Webinar](https://www.youtube.com/watch?v=Zj5RCweUHIk). \nCheck out [this Tweet thread](https://twitter.com/jerryjliu0/status/1692931028963221929?s=20) for more synthesized details.\n\n\n## Decoupling Chunks Used for Retrieval vs. Chunks Used for Synthesis\n\nA key technique for better retrieval is to decouple chunks used for retrieval with those that are used for synthesis. \n\n![](/_static/production_rag/decouple_chunks.png)\n\n#### Motivation\nThe optimal chunk representation for retrieval might be different than the optimal consideration used for synthesis. \nFor instance, a raw text chunk may contain needed details for the LLM to synthesize a more detailed answer given a query. However, it \nmay contain filler words/info that may bias the embedding representation, or it may lack global context and not be retrieved at all\nwhen a relevant query comes in.\n\n#### Key Techniques\nThere\u2019s two main ways to take advantage of this idea:\n\n**1. Embed a document summary, which links to chunks associated with the document.**\n\nThis can help retrieve relevant documents at a high-level before retrieving chunks vs. retrieving chunks directly (that might be in irrelevant documents).\n\nResources:\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n/examples/index_structs/doc_summary/DocSummary.ipynb\n```\n\n**2. Embed a sentence, which then links to a window around the sentence.**\n\nThis allows for finer-grained retrieval of relevant context (embedding giant chunks leads to \u201clost in the middle\u201d problems), but also ensures enough context for LLM synthesis.\n\nResources:\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/node_postprocessor/MetadataReplacementDemo.ipynb\n\n```\n\n\n## Structured Retrieval for Larger Document Sets\n\n![](/_static/production_rag/structured_retrieval.png)\n\n#### Motivation\nA big issue with the standard RAG stack (top-k retrieval + basic text splitting) is that it doesn\u2019t do well as the number of documents scales up - e.g.if you have 100 different PDFs.In this setting, given a query you may want to use structured information to help with more precise retrieval; for instance, if you ask a question that's only relevant to two PDFs,\nusing structured information to ensure those two PDFs get returned beyond raw embedding similarity with chunks.#### Key Techniques\nThere\u2019s a few ways of performing more structured tagging/retrieval for production-quality RAG systems, each with their own pros/cons.**1.Metadata Filters + Auto Retrieval** \nTag each document with metadata and then store in a vector database.During inference time, use the LLM to infer the right metadata filters to query the vector db in addition to the semantic query string.- Pros \u2705: Supported in major vector dbs.Can filter document via multiple dimensions.- Cons \ud83d\udeab: Can be hard to define the right tags.Tags may not contain enough relevant information for more precise retrieval.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a0d2eca7-f20c-49eb-9cd7-6a80d576903c": {"__data__": {"id_": "a0d2eca7-f20c-49eb-9cd7-6a80d576903c", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\production_rag.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d53d8b47de2c7a38bcb395a4c7e80275684dff56", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\production_rag.md", "author": "LlamaIndex"}, "hash": "436a5bdad83d762d174d2d321307650b30c7c1a75024c4ca5dd9fe734359a51d"}, "2": {"node_id": "6dcacd63-f1ce-4905-a2b3-a367a12576da", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\production_rag.md", "author": "LlamaIndex"}, "hash": "30fcc40a65b4e0ddfbcb679197b3d3a85469b7ad7d0f833920306039095a0999"}}, "hash": "45079b1b4fe5356c59fb205d2b7f4846d1bfd15ddd5bb4dedff3c5a90ab81bfd", "text": "Also tags represent keyword search at the document-level, doesn\u2019t allow for semantic lookups.Resources:\n**2.Store Document Hierarchies (summaries -> raw chunks) + Recursive Retrieval**\nEmbed document summaries and map to chunks per document.Fetch at the document-level first before chunk level.- Pros \u2705: Allows for semantic lookups at the document level.- Cons \ud83d\udeab: Doesn\u2019t allow for keyword lookups by structured tags (can be more precise than semantic search).Also autogenerating summaries can be expensive.**Resources**\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/vector_stores/chroma_auto_retriever.ipynb\n/examples/index_structs/doc_summary/DocSummary.ipynb\n/examples/query_engine/recursive_retriever_agents.ipynb\n/examples/retrievers/auto_vs_recursive_retriever.ipynb\n```\n\n## Dynamically Retrieve Chunks Depending on your Task\n\n![](/_static/production_rag/joint_qa_summary.png)\n\n#### Motivation\nRAG isn't just about question-answering about specific facts, which top-k similarity is optimized for.There can be a broad range of queries that a user might ask.Queries that are handled by naive RAG stacks include ones that ask about specific facts e.g.\"Tell me about the D&I initiatives for this company in 2023\" or \"What did the narrator do during his time at Google\".But queries can also include summarization e.g.\"Can you give me a high-level overview of this document\", or comparisons \"Can you compare/contrast X and Y\".All of these use cases may require different retrieval techniques.#### Key Techniques\n\nLlamaIndex provides some core abstractions to help you do task-specific retrieval.This includes our [router](/core_modules/query_modules/router/root.md) module.This also includes our [data agent](/core_modules/agent_modules/agents/root.md) module.This also includes some advanced query engine modules.This also include other modules that join structured and unstructured data.You can use these modules to do joint question-answering and summarization, or even combine structured queries with unstructured queries.**Core Module Resources**\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/core_modules/query_modules/query_engine/root.md\n/core_modules/query_modules/router/root.md\n/core_modules/agent_modules/agents/root.md\n```\n\n**Detailed Guide Resources**\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/sub_question_query_engine.ipynb\n/examples/query_engine/JointQASummary.ipynb\n/examples/query_engine/recursive_retriever_agents.ipynb\n/examples/query_engine/RouterQueryEngine.ipynb\n/examples/agent/openai_agent_query_cookbook.ipynb\n/examples/agent/openai_agent_query_plan.ipynb\n```\n\n\n\n## Optimize Context Embeddings\n\n#### Motivation\nThis is related to the motivation described above in \"decoupling chunks used for retrieval vs. synthesis\". \nWe want to make sure that the embeddings are optimized for better retrieval over your specific data corpus.\nPre-trained models may not capture the salient properties of the data relevant to your use case.\n\n### Key Techniques\nBeyond some of the techniques listed above, we can also try finetuning the embedding model.\nWe can actually do this over an unstructured text corpus, in a label-free way.\n\nCheck out our guides here:\n\n```{toctree}\n---\nmaxdepth: 1\n---\nEmbedding Fine-tuning Guide </examples/finetuning/embeddings/finetune_embedding.ipynb>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3b1f1bc5-e57a-4694-b624-7aa73d18f9d9": {"__data__": {"id_": "3b1f1bc5-e57a-4694-b624-7aa73d18f9d9", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\discover_llamaindex.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6249f3ec1980d3dbe7f502f0fb462939dfd3e88", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\discover_llamaindex.md", "author": "LlamaIndex"}, "hash": "f82949626c1cc48e2a38c3e4af744e41ec54acf9f491367ab7083036bc881372"}}, "hash": "2aed22e6b0373ae611ade14594ccf2d3bc067bb16a4ebf264d602b44222fe330", "text": "# Discover LlamaIndex Video Series\n\nThis page contains links to videos + associated notebooks for our ongoing video tutorial series \"Discover LlamaIndex\".\n\n## SubQuestionQueryEngine + 10K Analysis\n\nThis video covers the `SubQuestionQueryEngine` and how it can be applied to financial documents to help decompose complex queries into multiple sub-questions.\n\n[Youtube](https://www.youtube.com/watch?v=GT_Lsj3xj1o)\n\n[Notebook](../../examples/usecases/10k_sub_question.ipynb)\n\n## Discord Document Management\n\nThis video covers managing documents from a source that is consantly updating (i.e Discord) and how you can avoid document duplication and save embedding tokens.\n\n[Youtube](https://www.youtube.com/watch?v=j6dJcODLd_c)\n\n[Notebook + Supplimentary Material](https://github.com/jerryjliu/llama_index/tree/main/docs/examples/discover_llamaindex/document_management/)\n\n[Reference Docs](../../core_modules/data_modules/index/document_management.md)\n\n## Joint Text to SQL and Semantic Search\n\nThis video covers the tools built into LlamaIndex for combining SQL and semantic search into a single unified query interface.\n\n[Youtube](https://www.youtube.com/watch?v=ZIvcVJGtCrY)\n\n[Notebook](../../examples/query_engine/SQLAutoVectorQueryEngine.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9d950bef-82ba-44c8-8f18-975040b880f2": {"__data__": {"id_": "9d950bef-82ba-44c8-8f18-975040b880f2", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea304e8e84d3da5200044d14a761bd396d69d0b7", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "hash": "09078403b515eae4dd2db65362d7d97332f6efe3255e8b68c9d6524466d0b073"}, "3": {"node_id": "6e0f2a6d-1eb9-4f30-b31b-55ae0e800e41", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "hash": "4e4212570f5dba1c4f364cd21098c148792e2ca3b129d8c932eb82d81412f658"}}, "hash": "7f978dd29a114a69e94c4c651868cc9741f59794b5e8cac87d0230da25a2e4ee", "text": "# Finetuning\n\n## Overview\n\nFinetuning a model means updating the model itself over a set of data to improve the model in a variety of ways. This can include improving the quality of outputs, reducing hallucinations, memorizing more data holistically, and reducing latency/cost.\n\nThe core of our toolkit revolves around in-context learning / retrieval augmentation, which involves using the models in inference mode and not training the models themselves.\n\nWhile finetuning can be also used to \"augment\" a model with external data, finetuning can complement retrieval augmentation in a variety of ways:\n\n#### Embedding Finetuning Benefits\n- Finetuning the embedding model can allow for more meaningful embedding representations over a training distribution of data --> leads to better retrieval performance.\n\n#### LLM Finetuning Benefits\n- Allow it to learn a style over a given dataset\n- Allow it to learn a DSL that might be less represented in the training data (e.g. SQL) \n- Allow it to correct hallucinations/errors that might be hard to fix through prompt engineering\n- Allow it to distill a better model (e.g. GPT-4) into a simpler/cheaper model (e.g. gpt-3.5, Llama 2)\n\n\n## Integrations with LlamaIndex\n\nThis is an evolving guide, and there are currently three key integrations with LlamaIndex. Please check out the sections below for more details!\n- Finetuning embeddings for better retrieval performance\n- Finetuning Llama 2 for better text-to-SQL\n- Finetuning gpt-3.5-turbo to distill gpt-4\n\n\n### Finetuning Embeddings for Better Retrieval Performance", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6e0f2a6d-1eb9-4f30-b31b-55ae0e800e41": {"__data__": {"id_": "6e0f2a6d-1eb9-4f30-b31b-55ae0e800e41", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea304e8e84d3da5200044d14a761bd396d69d0b7", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "hash": "09078403b515eae4dd2db65362d7d97332f6efe3255e8b68c9d6524466d0b073"}, "2": {"node_id": "9d950bef-82ba-44c8-8f18-975040b880f2", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "hash": "7f978dd29a114a69e94c4c651868cc9741f59794b5e8cac87d0230da25a2e4ee"}, "3": {"node_id": "f1bc3745-7815-4391-8174-7fc8d283a3b4", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "hash": "17c3a23b6610863626930f439e929215820eb2be2cc10d76f418da720e6913af"}}, "hash": "4e4212570f5dba1c4f364cd21098c148792e2ca3b129d8c932eb82d81412f658", "text": "We created a comprehensive repo/guide showing you how to finetune an open-source embedding model (in this case, `bge`) over an unstructured text corpus. It consists of the following steps:\n1. Generating a synthetic question/answer dataset using LlamaIndex over any unstructed context.\n2. Finetuning the model\n3. Evaluating the model.\n\nFinetuning gives you a 5-10% increase in retrieval evaluation metrics. You can then plug this fine-tuned model into your RAG application with LlamaIndex. \n\n```{toctree}\n---\nmaxdepth: 1\n---\nFine-tuning an Adapter </examples/finetuning/embeddings/finetune_embedding_adapter.ipynb>\nEmbedding Fine-tuning Guide </examples/finetuning/embeddings/finetune_embedding.ipynb>\n```\n\n**Old**\n```{toctree}\n---\nmaxdepth: 1\n---\nEmbedding Fine-tuning Repo <https://github.com/run-llama/finetune-embedding>\nEmbedding Fine-tuning Blog <https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971>\n```\n\n### Finetuning GPT-3.5 to distill GPT-4\n\nWe have multiple guides showing how to use OpenAI's finetuning endpoints to fine-tune gpt-3.5-turbo to output GPT-4 responses for RAG/agents.\n\nWe use GPT-4 to automatically generate questions from any unstructured context, and use a GPT-4 query engine pipeline to generate \"ground-truth\" answers. Our `OpenAIFineTuningHandler` callback automatically logs questions/answers to a dataset. \n\nWe then launch a finetuning job, and get back a distilled model. We can evaluate this model with [Ragas](https://github.com/explodinggradients/ragas) to benchmark against a naive GPT-3.5 pipeline.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nGPT-3.5 Fine-tuning Notebook (Colab) <https://colab.research.google.com/drive/1NgyCJVyrC2xcZ5lxt2frTU862v6eJHlc?usp=sharing>\nGPT-3.5 Fine-tuning Notebook (Notebook link) </examples/finetuning/openai_fine_tuning.ipynb>\n/examples/finetuning/react_agent/react_agent_finetune.ipynb\n```\n\n**Old**\n\n```{toctree}\n---\nmaxdepth: 1\n---\nGPT-3.5 Fine-tuning Notebook (Colab) <https://colab.research.google.com/drive/1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo?usp=sharing>\nGPT-3.5 Fine-tuning Notebook (in Repo) <https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb>\n```\n\n### [WIP] Finetuning GPT-3.5 to Memorize Knowledge\n\nWe have a guide experimenting with showing how to use OpenAI fine-tuning to memorize a body of text.\nStill WIP! Not quite as good as RAG yet.\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/finetuning/knowledge/finetune_knowledge.ipynb\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f1bc3745-7815-4391-8174-7fc8d283a3b4": {"__data__": {"id_": "f1bc3745-7815-4391-8174-7fc8d283a3b4", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea304e8e84d3da5200044d14a761bd396d69d0b7", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "hash": "09078403b515eae4dd2db65362d7d97332f6efe3255e8b68c9d6524466d0b073"}, "2": {"node_id": "6e0f2a6d-1eb9-4f30-b31b-55ae0e800e41", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}, "hash": "4e4212570f5dba1c4f364cd21098c148792e2ca3b129d8c932eb82d81412f658"}}, "hash": "17c3a23b6610863626930f439e929215820eb2be2cc10d76f418da720e6913af", "text": "### Finetuning Llama 2 for Better Text-to-SQL \n\nIn this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using LlamaIndex abstractions.\n\nThe stack includes `sql-create-context` as the training dataset, OpenLLaMa as the base model, PEFT for finetuning, Modal for cloud compute, LlamaIndex for inference abstractions.\n\n```{toctree}\n---\nmaxdepth: 1\n---\nLlama 2 Text-to-SQL Fine-tuning (Repo) <https://github.com/run-llama/modal_finetune_sql>\nLlama 2 Text-to-SQL Fine-tuning (Notebook) <https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a607c6b3-397e-4db9-b6ae-e7abd7a7dcca": {"__data__": {"id_": "a607c6b3-397e-4db9-b6ae-e7abd7a7dcca", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\graphs.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "acf49db9f42ba4cd549ae928e76ce7cb58798434", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\graphs.md", "author": "LlamaIndex"}, "hash": "5fa3d1a85700d9ea5c00145d759f2801b6844fc1f27a13faeb883ee6e401b1cc"}}, "hash": "afbc37739612af5d47670bdefce216f94c051c4ad18c56a8daa40c682a907a41", "text": "# Knowledge Graphs\n\nLlamaIndex contains some fantastic guides for building with knowledge graphs.\n\nCheck out the end-to-end tutorials/workshops below. Also check out our knowledge graph query engine guides [here](/core_modules/query_modules/query_engine/modules.md).\n\n\n```{toctree}\n---\nmaxdepth: 1\n---\nLlamaIndex Workshop: Building RAG with Knowledge Graphs <https://colab.research.google.com/drive/1tLjOg2ZQuIClfuWrAC2LdiZHCov8oUbs>\nREBEL + Knowledge Graph Index <https://colab.research.google.com/drive/1G6pcR0pXvSkdMQlAK_P-IrYgo-_staxd?usp=sharing>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2ccd8336-2484-4eae-97b2-12f9eae6c1e6": {"__data__": {"id_": "2ccd8336-2484-4eae-97b2-12f9eae6c1e6", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5fcc24cbf7d36cf038767bf3b95393e7aa48a80", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "hash": "16f62e025d6c721f2e06706fc3d0fef59d7d6c1fb395607082eb43fdf8048ecf"}, "3": {"node_id": "ee52c155-2606-41fe-9846-bfd036b7b5e2", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "hash": "37878d6166d814ee25fa453f817f8e1aa40480f83c8d9668b660d40a73c781a2"}}, "hash": "8c2a894182ac293af3c0511a6dc1048d3c1996f2dfc3977868df77f862d287e8", "text": "(one-click-observability)=\n\n#  One-Click Observability\n\nLlamaIndex provides **one-click observability**  \ud83d\udd2d to allow you to build principled LLM applications in a production setting.\n\nA key requirement for principled development of LLM applications over your data (RAG systems, agents) is being able to observe, debug, and evaluate\nyour system - both as a whole and for each component.\n\nThis feature allows you to seamlessly integrate the LlamaIndex library with powerful observability/evaluation tools offered by our partners.\nConfigure a variable once, and you'll be able to do things like the following:\n- View LLM/prompt inputs/outputs\n- That that the outputs of any component (LLMs, embeddings) are performing as expected\n- View call traces for both indexing and querying\n\nEach provider has similarities and differences. Take a look below for the full set of guides for each one! \n\n## Usage Pattern\n\nTo toggle, you will generally just need to do the following:\n\n```python\n\nfrom llama_index import set_global_handler\n\n# general usage\nset_global_handler(\"<handler_name>\", **kwargs)\n\n# W&B example\n# set_global_handler(\"wandb\", run_args={\"project\": \"llamaindex\"})\n\n```\n\nNote that all `kwargs` to `set_global_handler` are passed to the underlying callback handler.\n\nAnd that's it! Executions will get seamlessly piped to downstream service (e.g. W&B Prompts) and you'll be able to access features such as viewing execution traces of your application.\n\n**NOTE**: TruLens (by TruEra) uses a different \"one-click\" experience. See below for details.\n\n## Partners\n\nWe offer a rich set of integrations with our partners. A short description + usage pattern, and guide is provided for each partner.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ee52c155-2606-41fe-9846-bfd036b7b5e2": {"__data__": {"id_": "ee52c155-2606-41fe-9846-bfd036b7b5e2", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5fcc24cbf7d36cf038767bf3b95393e7aa48a80", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "hash": "16f62e025d6c721f2e06706fc3d0fef59d7d6c1fb395607082eb43fdf8048ecf"}, "2": {"node_id": "2ccd8336-2484-4eae-97b2-12f9eae6c1e6", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "hash": "8c2a894182ac293af3c0511a6dc1048d3c1996f2dfc3977868df77f862d287e8"}, "3": {"node_id": "d1231944-90c4-4c97-b5d7-fde7e74ccc21", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "hash": "93025b14d8c1e6217b721dd899430f612317e057090fc2f6114a19da6a8a7163"}}, "hash": "37878d6166d814ee25fa453f817f8e1aa40480f83c8d9668b660d40a73c781a2", "text": "### Weights and Biases Prompts\n\nPrompts allows users to log/trace/inspect the execution flow of LlamaIndex during index construction and querying. It also allows users to version-control their indices.\n\n#### Usage Pattern\n\n```python\nfrom llama_index import set_global_handler\nset_global_handler(\"wandb\", run_args={\"project\": \"llamaindex\"})\n\n# NOTE: No need to do the following\n# from llama_index.callbacks import WandbCallbackHandler, CallbackManager\n# wandb_callback = WandbCallbackHandler(run_args={\"project\": \"llamaindex\"})\n# callback_manager = CallbackManager([wandb_callback])\n# service_context = ServiceContext.from_defaults(\n#     callback_manager=callback_manager\n# )\n\n# access additional methods on handler to persist index + load index\nimport llama_index\n\n# persist index\nllama_index.global_handler.persist_index(graph, index_name=\"composable_graph\")\n# load storage context\nstorage_context = llama_index.global_handler.load_storage_context(\n    artifact_url=\"ayut/llamaindex/composable_graph:v0\"\n)\n\n```\n\n![](/_static/integrations/wandb.png)\n\n#### Guides\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/callbacks/WandbCallbackHandler.ipynb\n```\n\n### OpenInference\n\n[OpenInference](https://github.com/Arize-ai/open-inference-spec) is an open standard for capturing and storing AI model inferences. It enables experimentation, visualization, and evaluation of LLM applications using LLM observability solutions such as [Phoenix](https://github.com/Arize-ai/phoenix).\n\n#### Usage Pattern\n\n```python\nimport llama_index\n\nllama_index.set_global_handler(\"openinference\")\n\n# NOTE: No need to do the following\n# from llama_index.callbacks import OpenInferenceCallbackHandler, CallbackManager\n# callback_handler = OpenInferenceCallbackHandler()\n# callback_manager = CallbackManager([callback_handler])\n# service_context = ServiceContext.from_defaults(\n#     callback_manager=callback_manager\n# )\n\n# Run your LlamaIndex application here...\nfor query in queries:\n    query_engine.query(query)\n\n# View your LLM app data as a dataframe in OpenInference format.\nfrom llama_index.callbacks.open_inference_callback import as_dataframe\n\nquery_data_buffer = llama_index.global_handler.flush_query_data_buffer()\nquery_dataframe = as_dataframe(query_data_buffer)\n```\n\n**NOTE**: To unlock capabilities of Phoenix, you will need to define additional steps to feed in query/ context dataframes. See below!\n\n#### Guides\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/callbacks/OpenInferenceCallback.ipynb\nEvaluating and Improving a LlamaIndex Search and Retrieval Application <https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d1231944-90c4-4c97-b5d7-fde7e74ccc21": {"__data__": {"id_": "d1231944-90c4-4c97-b5d7-fde7e74ccc21", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5fcc24cbf7d36cf038767bf3b95393e7aa48a80", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "hash": "16f62e025d6c721f2e06706fc3d0fef59d7d6c1fb395607082eb43fdf8048ecf"}, "2": {"node_id": "ee52c155-2606-41fe-9846-bfd036b7b5e2", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}, "hash": "37878d6166d814ee25fa453f817f8e1aa40480f83c8d9668b660d40a73c781a2"}}, "hash": "93025b14d8c1e6217b721dd899430f612317e057090fc2f6114a19da6a8a7163", "text": "### TruEra TruLens\n\nTruLens allows users to instrument/evaluate LlamaIndex applications, through features such as feedback functions and tracing.\n\n#### Usage Pattern + Guides\n\n```python\n# use trulens\nfrom trulens_eval import TruLlama\ntru_query_engine = TruLlama(query_engine)\n\n# query \ntru_query_engine.query(\"What did the author do growing up?\")\n\n```\n![](/_static/integrations/trulens.png)\n\n#### Guides\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/community/integrations/trulens.md\nQuickstart Guide with LlamaIndex + TruLens <https://github.com/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb>\nColab <https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/frameworks/llama_index/llama_index_quickstart.ipynb>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "45c9d7ae-cd4f-4588-b588-fdfee152552e": {"__data__": {"id_": "45c9d7ae-cd4f-4588-b588-fdfee152552e", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\principled_dev_practices.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a72939a6dbb1b7596cc1a485eba0eca5403d8664", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\principled_dev_practices.md", "author": "LlamaIndex"}, "hash": "9a9cf14ecc882bed9a0ea932fe5a1efa225b7592c37aa9adf7bc5094c49326b2"}}, "hash": "493af2c9cdd9d94b8c29c900a94de570ffa4d8f8f75abfc60b2b6366bc1a9cd3", "text": "# Principled Development Practices\n\nIn order to develop your application, it can help to implement some principled development practices.\n\nHere we provide some general guidance to help you better anticipate the challenges and concerns you may encounter as you develop your LLM application.\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/end_to_end_tutorials/dev_practices/development_pathway.md\n\n```\n\nWe've also accumulated some techniques for creating more performant RAG applications.\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/end_to_end_tutorials/dev_practices/production_rag.md\n\n```\n\nWith that said, we want to establish some general pillars of principled development for LLM and RAG applications.\n- The first pillar is **observability**: setting up initial tools to observe, debug your system and evaluate it on ad-hoc examples.\n- The next pillar is **evaluation**: being able to evaluate different components of your system so that you can experiment and improve it in a more systematic fashion.\n- The last pillar is **monitoring**: after the application is deployed, we want to continuously monitor and test that it is performing well in production.\n\n\n% TODO: also add UX patterns doc\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/end_to_end_tutorials/dev_practices/observability.md\n/end_to_end_tutorials/dev_practices/evaluation.md\n/end_to_end_tutorials/dev_practices/monitoring.md\n```\n\n## Contribute Your Insights!\nIf you have thoughts on sections to add or how to improve, please make a contribution ([link](https://github.com/jerryjliu/llama_index/blob/main/CONTRIBUTING.md))", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2b2b1191-5427-4142-af6c-010b7c64bcbc": {"__data__": {"id_": "2b2b1191-5427-4142-af6c-010b7c64bcbc", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\privacy.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20745498e5bc3c98851326b15f795894aa05d49b", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\privacy.md", "author": "LlamaIndex"}, "hash": "4752c30d757fdc8ddbebf395d8d11c7e252c18a5fdf21ce3969effa9a3a6278c"}}, "hash": "182e08525bcb5ef4c46865dc278c29d2f6e48c120a5f511a48d90efaad4b7b96", "text": "# Private Setup\n\nRelevant Resources:\n- [Using LlamaIndex with Local Models](https://colab.research.google.com/drive/16QMQePkONNlDpgiltOi7oRQgmB8dU5fl?usp=sharing)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7a0954fe-8e8c-402f-b89c-ebb4b163df9e": {"__data__": {"id_": "7a0954fe-8e8c-402f-b89c-ebb4b163df9e", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f13a08b36ae7d2805ad45b265730dd3fdea1243c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "fa8e87d0fe96d300bca82fe2c21150221dbc02e019246580c44cc654d224ad6f"}, "3": {"node_id": "8c1168ba-de1c-4147-a569-fbae40ef8a50", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "d6e738591e9790f92d463007064905d86767f9a75fc8448f78c73a26d2989153"}}, "hash": "691f2406a16aab7b445570345527c7013f55c9f3f66960a4814bcb7cfd4a3a9e", "text": "# Q&A over Documents\n\nAt a high-level, LlamaIndex gives you the ability to query your data for any downstream LLM use case,\nwhether it's question-answering, summarization, or a component in a chatbot.\n\nThis section describes the different ways you can query your data with LlamaIndex, roughly in order\nof simplest (top-k semantic search), to more advanced capabilities.\n\n### Semantic Search \n\nThe most basic example usage of LlamaIndex is through semantic search. We provide\na simple in-memory vector store for you to get started, but you can also choose\nto use any one of our [vector store integrations](/community/integrations/vector_stores.md):\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n\n```\n\n**Tutorials**\n- [Starter Tutorial](/getting_started/starter_example.md)\n- [Basic Usage Pattern](/end_to_end_tutorials/usage_pattern.md)\n\n**Guides**\n- [Example](../examples/vector_stores/SimpleIndexDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/tree/main/docs/examples/vector_stores/SimpleIndexDemo.ipynb))\n\n\n### Summarization\n\nA summarization query requires the LLM to iterate through many if not most documents in order to synthesize an answer.\nFor instance, a summarization query could look like one of the following: \n- \"What is a summary of this collection of text?\"\n- \"Give me a summary of person X's experience with the company.\"\n\nIn general, a summary index would be suited for this use case. A summary index by default goes through all the data.\n\nEmpirically, setting `response_mode=\"tree_summarize\"` also leads to better summarization results.\n\n```python\nindex = SummaryIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(\n    response_mode=\"tree_summarize\"\n)\nresponse = query_engine.query(\"<summarization_query>\")\n```\n\n### Queries over Structured Data\n\nLlamaIndex supports queries over structured data, whether that's a Pandas DataFrame or a SQL Database.\n\nHere are some relevant resources:\n\n**Tutorials**\n\n- [Guide on Text-to-SQL](/guides/tutorials/sql_guide.md)\n\n**Guides**\n- [SQL Guide (Core)](../examples/index_structs/struct_indices/SQLIndexDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/struct_indices/SQLIndexDemo.ipynb))\n- [Pandas Demo](../examples/query_engine/pandas_query_engine.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_engine/pandas_query_engine.ipynb))", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8c1168ba-de1c-4147-a569-fbae40ef8a50": {"__data__": {"id_": "8c1168ba-de1c-4147-a569-fbae40ef8a50", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f13a08b36ae7d2805ad45b265730dd3fdea1243c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "fa8e87d0fe96d300bca82fe2c21150221dbc02e019246580c44cc654d224ad6f"}, "2": {"node_id": "7a0954fe-8e8c-402f-b89c-ebb4b163df9e", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "691f2406a16aab7b445570345527c7013f55c9f3f66960a4814bcb7cfd4a3a9e"}, "3": {"node_id": "7b5e823d-e520-446c-962d-0235c8a2c5c5", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "4f8572d54920ab8a787711813fd3f11b34010c295dedc0dff92ae2e33f185f50"}}, "hash": "d6e738591e9790f92d463007064905d86767f9a75fc8448f78c73a26d2989153", "text": "### Synthesis over Heterogeneous Data\n\nLlamaIndex supports synthesizing across heterogeneous data sources. This can be done by composing a graph over your existing data.\nSpecifically, compose a summary index over your subindices. A summary index inherently combines information for each node; therefore\nit can synthesize information across your heterogeneous data sources.\n\n```python\nfrom llama_index import VectorStoreIndex, SummaryIndex\nfrom llama_index.indices.composability import ComposableGraph\n\nindex1 = VectorStoreIndex.from_documents(notion_docs)\nindex2 = VectorStoreIndex.from_documents(slack_docs)\n\ngraph = ComposableGraph.from_indices(SummaryIndex, [index1, index2], index_summaries=[\"summary1\", \"summary2\"])\nquery_engine = graph.as_query_engine()\nresponse = query_engine.query(\"<query_str>\")\n\n```\n\n**Guides**\n- [Composability](/core_modules/data_modules/index/composability.md)\n- [City Analysis](/examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.ipynb))\n\n\n\n### Routing over Heterogeneous Data\n\nLlamaIndex also supports routing over heterogeneous data sources with `RouterQueryEngine` - for instance, if you want to \"route\" a query to an \nunderlying Document or a sub-index.\n\n\nTo do this, first build the sub-indices over different data sources.Then construct the corresponding query engines, and give each query engine a description to obtain a `QueryEngineTool`.```python\nfrom llama_index import TreeIndex, VectorStoreIndex\nfrom llama_index.tools import QueryEngineTool\n\n...\n\n# define sub-indices\nindex1 = VectorStoreIndex.from_documents(notion_docs)\nindex2 = VectorStoreIndex.from_documents(slack_docs)\n\n# define query engines and tools\ntool1 = QueryEngineTool.from_defaults(\n    query_engine=index1.as_query_engine(), \n    description=\"Use this query engine to do...\",\n)\ntool2 = QueryEngineTool.from_defaults(\n    query_engine=index2.as_query_engine(), \n    description=\"Use this query engine for something else...\",\n)\n```\n\nThen, we define a `RouterQueryEngine` over them.By default, this uses a `LLMSingleSelector` as the router, which uses the LLM to choose the best sub-index to router the query to, given the descriptions.```python\nfrom llama_index.query_engine import RouterQueryEngine\n\nquery_engine = RouterQueryEngine.from_defaults(\n    query_engine_tools=[tool1, tool2]\n)\n\nresponse = query_engine.query(\n    \"In Notion, give me a summary of the product roadmap.\")\n\n```\n\n**Guides**\n- [Router Query Engine Guide](../examples/query_engine/RouterQueryEngine.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_engine/RouterQueryEngine.ipynb))\n- [City Analysis Unified Query Interface](../examples/composable_indices/city_analysis/City_Analysis-Unified-Query.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/PineconeDemo-CityAnalysis.ipynb))\n\n### Compare/Contrast Queries\nYou can explicitly perform compare/contrast queries with a **query transformation** module within a ComposableGraph.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7b5e823d-e520-446c-962d-0235c8a2c5c5": {"__data__": {"id_": "7b5e823d-e520-446c-962d-0235c8a2c5c5", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f13a08b36ae7d2805ad45b265730dd3fdea1243c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "fa8e87d0fe96d300bca82fe2c21150221dbc02e019246580c44cc654d224ad6f"}, "2": {"node_id": "8c1168ba-de1c-4147-a569-fbae40ef8a50", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "d6e738591e9790f92d463007064905d86767f9a75fc8448f78c73a26d2989153"}, "3": {"node_id": "6b920c82-667e-473f-a2c0-2b881870a3f2", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "e289a484c7779dde6acdfc6913943639913353dccbd46535d501d5e64a701e17"}}, "hash": "4f8572d54920ab8a787711813fd3f11b34010c295dedc0dff92ae2e33f185f50", "text": "```python\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    service_context.llm_predictor, verbose=True\n)\n```\n\nThis module will help break down a complex query into a simpler one over your existing index structure.**Guides**\n- [Query Transformations](/core_modules/query_modules/query_engine/advanced/query_transformations.md)\n- [City Analysis Compare/Contrast Example](/examples/composable_indices/city_analysis/City_Analysis-Decompose.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/composable_indices/city_analysis/City_Analysis-Decompose.ipynb))\n\nYou can also rely on the LLM to *infer* whether to perform compare/contrast queries (see Multi-Document Queries below).### Multi-Document Queries\n\nBesides the explicit synthesis/routing flows described above, LlamaIndex can support more general multi-document queries as well.It can do this through our `SubQuestionQueryEngine` class.Given a query, this query engine will generate a \"query plan\" containing\nsub-queries against sub-documents before synthesizing the final answer.To do this, first define an index for each document/data source, and wrap it with a `QueryEngineTool` (similar to above):\n\n```python\nfrom llama_index.tools import QueryEngineTool, ToolMetadata\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=sept_engine, \n        metadata=ToolMetadata(name='sept_22', description='Provides information about Uber quarterly financials ending September 2022')\n    ),\n    QueryEngineTool(\n        query_engine=june_engine, \n        metadata=ToolMetadata(name='june_22', description='Provides information about Uber quarterly financials ending June 2022')\n    ),\n    QueryEngineTool(\n        query_engine=march_engine, \n        metadata=ToolMetadata(name='march_22', description='Provides information about Uber quarterly financials ending March 2022')\n    ),\n]\n```\n\nThen, we define a `SubQuestionQueryEngine` over these tools:\n\n```python\nfrom llama_index.query_engine import SubQuestionQueryEngine\n\nquery_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)\n\n```\n\nThis query engine can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer.This makes it especially well-suited for compare/contrast queries across documents as well as queries pertaining to a specific document.**Guides**\n- [Sub Question Query Engine (Intro)](../examples/query_engine/sub_question_query_engine.ipynb)\n- [10Q Analysis (Uber)](../examples/usecases/10q_sub_question.ipynb)\n- [10K Analysis (Uber and Lyft)](../examples/usecases/10k_sub_question.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6b920c82-667e-473f-a2c0-2b881870a3f2": {"__data__": {"id_": "6b920c82-667e-473f-a2c0-2b881870a3f2", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f13a08b36ae7d2805ad45b265730dd3fdea1243c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "fa8e87d0fe96d300bca82fe2c21150221dbc02e019246580c44cc654d224ad6f"}, "2": {"node_id": "7b5e823d-e520-446c-962d-0235c8a2c5c5", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}, "hash": "4f8572d54920ab8a787711813fd3f11b34010c295dedc0dff92ae2e33f185f50"}}, "hash": "e289a484c7779dde6acdfc6913943639913353dccbd46535d501d5e64a701e17", "text": "### Multi-Step Queries\n\nLlamaIndex can also support iterative multi-step queries. Given a complex query, break it down into an initial subquestions,\nand sequentially generate subquestions based on returned answers until the final answer is returned.\n\nFor instance, given a question \"Who was in the first batch of the accelerator program the author started?\",\nthe module will first decompose the query into a simpler initial question \"What was the accelerator program the author started?\",\nquery the index, and then ask followup questions.\n\n**Guides**\n- [Query Transformations](/core_modules/query_modules/query_engine/advanced/query_transformations.md)\n- [Multi-Step Query Decomposition](../examples/query_transformations/HyDEQueryTransformDemo.ipynb) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb))\n\n\n### Temporal Queries\n\nLlamaIndex can support queries that require an understanding of time. It can do this in two ways:\n- Decide whether the query requires utilizing temporal relationships between nodes (prev/next relationships) in order to retrieve additional context to answer the question.\n- Sort by recency and filter outdated context.\n\n**Guides**\n- [Second-Stage Postprocessing Guide](/core_modules/query_modules/node_postprocessors/root.md)\n- [Prev/Next Postprocessing](/examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb)\n- [Recency Postprocessing](/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb)\n\n### Additional Resources\n- [A Guide to Creating a Unified Query Framework over your ndexes](/end_to_end_tutorials/question_and_answer/unified_query.md)\n- [A Guide to Extracting Terms and Definitions](/end_to_end_tutorials/question_and_answer/terms_definitions_tutorial.md)\n- [SEC 10k Analysis](https://medium.com/@jerryjliu98/how-unstructured-and-llamaindex-can-help-bring-the-power-of-llms-to-your-own-data-3657d063e30d)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "64db79af-099c-48d7-b5b9-85fd7709df64": {"__data__": {"id_": "64db79af-099c-48d7-b5b9-85fd7709df64", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "3": {"node_id": "258808ef-4ff1-4f26-a477-640304ad78bb", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "58ef4175337b20009ab881e514f70a12a1b68e7a9e8639b5fe9caf12aa82ab54"}}, "hash": "77168dcafbf13e430f3d2a0b034fe1575727b595b0d7250324b9dac393d78ff6", "text": "# A Guide to Extracting Terms and Definitions\r\n\r\nLlama Index has many use cases (semantic search, summarization, etc.)that are [well documented](/end_to_end_tutorials/use_cases.md).However, this doesn't mean we can't apply Llama Index to very specific use cases!In this tutorial, we will go through the design process of using Llama Index to extract terms and definitions from text, while allowing users to query those terms later.Using [Streamlit](https://streamlit.io/), we can provide an easy way to build frontend for running and testing all of this, and quickly iterate with our design.This tutorial assumes you have Python3.9+ and the following packages installed:\r\n\r\n- llama-index\r\n- streamlit\r\n\r\nAt the base level, our objective is to take text from a document, extract terms and definitions, and then provide a way for users to query that knowledge base of terms and definitions.The tutorial will go over features from both Llama Index and Streamlit, and hopefully provide some interesting solutions for common problems that come up.The final version of this tutorial can be found [here](https://github.com/logan-markewich/llama_index_starter_pack) and a live hosted demo is available on [Huggingface Spaces](https://huggingface.co/spaces/llamaindex/llama_index_term_definition_demo).## Uploading Text\r\n\r\nStep one is giving users a way to upload documents.Let\u2019s write some code using Streamlit to provide the interface for this!Use the following code and launch the app with `streamlit run app.py`.```python\r\nimport streamlit as st\r\n\r\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\r\n\r\ndocument_text = st.text_area(\"Or enter raw text\")\r\nif st.button(\"Extract Terms and Definitions\") and document_text:\r\n    with st.spinner(\"Extracting...\"):\r\n        extracted_terms = document text  # this is a placeholder!st.write(extracted_terms)\r\n```\r\n\r\nSuper simple right!But you'll notice that the app doesn't do anything useful yet.To use llama_index, we also need to setup our OpenAI LLM.There are a bunch of possible settings for the LLM, so we can let the user figure out what's best.We should also let the user set the prompt that will extract the terms (which will also help us debug what works best).## LLM Settings\r\n\r\nThis next step introduces some tabs to our app, to separate it into different panes that provide different features.Let's create a tab for LLM settings and for uploading text:\r\n\r\n```python\r\nimport os\r\nimport streamlit as st\r\n\r\nDEFAULT_TERM_STR = (\r\n    \"Make a list of terms and definitions that are defined in the context, \"\r\n    \"with one pair on each line. \"\"If a term is missing it's definition, use your best judgment. \"\"Write each line as as follows:\\nTerm: <term> Definition: <definition>\"\r\n)\r\n\r\nst.title(\"\ud83e\udd99 Llama Index Term Extractor \ud83e\udd99\")\r\n\r\nsetup_tab, upload_tab = st.tabs([\"Setup\", \"Upload/Extract Terms\"])\r\n\r\nwith setup_tab:\r\n    st.subheader(\"LLM Setup\")\r\n    api_key = st.text_input(\"Enter your OpenAI API key here\", type=\"password\")\r\n    llm_name = st.selectbox('Which LLM?', [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-4\"])\r\n    model_temperature = st.slider(\"LLM Temperature\", min_value=0.0, max_value=1.0, step=0.1)\r\n    term_extract_str = st.text_area(\"The query to extract terms and definitions with.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "258808ef-4ff1-4f26-a477-640304ad78bb": {"__data__": {"id_": "258808ef-4ff1-4f26-a477-640304ad78bb", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "2": {"node_id": "64db79af-099c-48d7-b5b9-85fd7709df64", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "77168dcafbf13e430f3d2a0b034fe1575727b595b0d7250324b9dac393d78ff6"}, "3": {"node_id": "5772201a-3ccc-4a5d-acaf-a9eb0639fe43", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "16cbf0769ba3acc8f1cbf901b586a48d423ebe0edb15a930f757d2501b9337a8"}}, "hash": "58ef4175337b20009ab881e514f70a12a1b68e7a9e8639b5fe9caf12aa82ab54", "text": "\", value=DEFAULT_TERM_STR)\r\n\r\nwith upload_tab:\r\n    st.subheader(\"Extract and Query Definitions\")\r\n    document_text = st.text_area(\"Or enter raw text\")\r\n    if st.button(\"Extract Terms and Definitions\") and document_text:\r\n        with st.spinner(\"Extracting...\"):\r\n            extracted_terms = document text  # this is a placeholder!st.write(extracted_terms)\r\n```\r\n\r\nNow our app has two tabs, which really helps with the organization.You'll also noticed I added a default prompt to extract terms -- you can change this later once you try extracting some terms, it's just the prompt I arrived at after experimenting a bit.Speaking of extracting terms, it's time to add some functions to do just that!## Extracting and Storing Terms\r\n\r\nNow that we are able to define LLM settings and upload text, we can try using Llama Index to extract the terms from text for us!We can add the following functions to both initialize our LLM, as well as use it to extract terms from the input text.```python\r\nfrom llama_index import Document, SummaryIndex, LLMPredictor, ServiceContext, load_index_from_storage\r\nfrom llama_index.llms import OpenAI\r\n\r\ndef get_llm(llm_name, model_temperature, api_key, max_tokens=256):\r\n    os.environ['OPENAI_API_KEY'] = api_key\r\n    return OpenAI(temperature=model_temperature, model=llm_name, max_tokens=max_tokens)\r\n\r\ndef extract_terms(documents, term_extract_str, llm_name, model_temperature, api_key):\r\n    llm = get_llm(llm_name, model_temperature, api_key, max_tokens=1024)\r\n\r\n    service_context = ServiceContext.from_defaults(llm=llm,\r\n                                                   chunk_size=1024)\r\n\r\n    temp_index = SummaryIndex.from_documents(documents, service_context=service_context)\r\n    query_engine = temp_index.as_query_engine(response_mode=\"tree_summarize\")\r\n    terms_definitions = str(query_engine.query(term_extract_str))\r\n    terms_definitions = [x for x in terms_definitions.split(\"\\n\") if x and 'Term:' in x and 'Definition:' in x]\r\n    # parse the text into a dict\r\n    terms_to_definition = {x.split(\"Definition:\")[0].split(\"Term:\")[-1].strip(): x.split(\"Definition:\")[-1].strip() for x in terms_definitions}\r\n    return terms_to_definition\r\n```\r\n\r\nNow, using the new functions, we can finally extract our terms!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5772201a-3ccc-4a5d-acaf-a9eb0639fe43": {"__data__": {"id_": "5772201a-3ccc-4a5d-acaf-a9eb0639fe43", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "2": {"node_id": "258808ef-4ff1-4f26-a477-640304ad78bb", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "58ef4175337b20009ab881e514f70a12a1b68e7a9e8639b5fe9caf12aa82ab54"}, "3": {"node_id": "a7854179-f654-4452-bee0-2e6374805dfc", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "65442e1f5b1d7e30ab831d52bafc5a6dccc90a54d9303491c618f1a72b4b1012"}}, "hash": "16cbf0769ba3acc8f1cbf901b586a48d423ebe0edb15a930f757d2501b9337a8", "text": "```python\r\n...\r\nwith upload_tab:\r\n    st.subheader(\"Extract and Query Definitions\")\r\n    document_text = st.text_area(\"Or enter raw text\")\r\n    if st.button(\"Extract Terms and Definitions\") and document_text:\r\n        with st.spinner(\"Extracting...\"):\r\n            extracted_terms = extract_terms([Document(text=document_text)],\r\n                                            term_extract_str, llm_name,\r\n                                            model_temperature, api_key)\r\n        st.write(extracted_terms)\r\n```\r\n\r\nThere's a lot going on now, let's take a moment to go over what is happening.`get_llm()` is instantiating the LLM based on the user configuration from the setup tab.Based on the model name, we need to use the appropriate class (`OpenAI` vs. `ChatOpenAI`).`extract_terms()` is where all the good stuff happens.First, we call `get_llm()` with `max_tokens=1024`, since we don't want to limit the model too much when it is extracting our terms and definitions (the default is 256 if not set).Then, we define our `ServiceContext` object, aligning `num_output` with our `max_tokens` value, as well as setting the chunk size to be no larger than the output.When documents are indexed by Llama Index, they are broken into chunks (also called nodes) if they are large, and `chunk_size` sets the size for these chunks.Next, we create a temporary summary index and pass in our service context.A summary index will read every single piece of text in our index, which is perfect for extracting terms.Finally, we use our pre-defined query text to extract terms, using `response_mode=\"tree_summarize`.This response mode will generate a tree of summaries from the bottom up, where each parent summarizes its children.Finally, the top of the tree is returned, which will contain all our extracted terms and definitions.Lastly, we do some minor post processing.We assume the model followed instructions and put a term/definition pair on each line.If a line is missing the `Term:` or `Definition:` labels, we skip it.Then, we convert this to a dictionary for easy storage!## Saving Extracted Terms\r\n\r\nNow that we can extract terms, we need to put them somewhere so that we can query for them later.A `VectorStoreIndex` should be a perfect choice for now!But in addition, our app should also keep track of which terms are inserted into the index so that we can inspect them later.Using `st.session_state`, we can store the current list of terms in a session dict, unique to each user!First things first though, let's add a feature to initialize a global vector index and another function to insert the extracted terms.```python\r\n...\r\nif 'all_terms' not in st.session_state:\r\n    st.session_state['all_terms'] = DEFAULT_TERMS\r\n...\r\n\r\ndef insert_terms(terms_to_definition):\r\n    for term, definition in terms_to_definition.items():\r\n        doc = Document(text=f\"Term: {term}\\nDefinition: {definition}\")\r\n        st.session_state['llama_index'].insert(doc)\r\n\r\n@st.cache_resource\r\ndef initialize_index(llm_name, model_temperature, api_key):\r\n    \"\"\"Create the VectorStoreIndex object.\"\"\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a7854179-f654-4452-bee0-2e6374805dfc": {"__data__": {"id_": "a7854179-f654-4452-bee0-2e6374805dfc", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "2": {"node_id": "5772201a-3ccc-4a5d-acaf-a9eb0639fe43", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "16cbf0769ba3acc8f1cbf901b586a48d423ebe0edb15a930f757d2501b9337a8"}, "3": {"node_id": "c536587f-791c-4772-8240-4ba228f5940c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "ed60503561b1e8051a9255043c10f3aee6341319c7742e5a831390469dc0c8c1"}}, "hash": "65442e1f5b1d7e30ab831d52bafc5a6dccc90a54d9303491c618f1a72b4b1012", "text": "llm = get_llm(llm_name, model_temperature, api_key)\r\n\r\n    service_context = ServiceContext.from_defaults(llm=llm)\r\n\r\n    index = VectorStoreIndex([], service_context=service_context)\r\n\r\n    return index\r\n\r\n...\r\n\r\nwith upload_tab:\r\n    st.subheader(\"Extract and Query Definitions\")\r\n    if st.button(\"Initialize Index and Reset Terms\"):\r\n        st.session_state['llama_index'] = initialize_index(llm_name, model_temperature, api_key)\r\n        st.session_state['all_terms'] = {}\r\n\r\n    if \"llama_index\" in st.session_state:\r\n        st.markdown(\"Either upload an image/screenshot of a document, or enter the text manually.\")document_text = st.text_area(\"Or enter raw text\")\r\n        if st.button(\"Extract Terms and Definitions\") and (uploaded_file or document_text):\r\n            st.session_state['terms'] = {}\r\n            terms_docs = {}\r\n            with st.spinner(\"Extracting...\"):\r\n                terms_docs.update(extract_terms([Document(text=document_text)], term_extract_str, llm_name, model_temperature, api_key))\r\n            st.session_state['terms'].update(terms_docs)\r\n\r\n        if \"terms\" in st.session_state and st.session_state[\"terms\"]::\r\n            st.markdown(\"Extracted terms\")\r\n            st.json(st.session_state['terms'])\r\n\r\n            if st.button(\"Insert terms?\"):\r\n                with st.spinner(\"Inserting terms\"):\r\n                    insert_terms(st.session_state['terms'])\r\n                st.session_state['all_terms'].update(st.session_state['terms'])\r\n                st.session_state['terms'] = {}\r\n                st.experimental_rerun()\r\n```\r\n\r\nNow you are really starting to leverage the power of streamlit!Let's start with the code under the upload tab.We added a button to initialize the vector index, and we store it in the global streamlit state dictionary, as well as resetting the currently extracted terms.Then, after extracting terms from the input text, we store it the extracted terms in the global state again and give the user a chance to review them before inserting.If the insert button is pressed, then we call our insert terms function, update our global tracking of inserted terms, and remove the most recently extracted terms from the session state.## Querying for Extracted Terms/Definitions\r\n\r\nWith the terms and definitions extracted and saved, how can we use them?And how will the user even remember what's previously been saved??We can simply add some more tabs to the app to handle these features.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c536587f-791c-4772-8240-4ba228f5940c": {"__data__": {"id_": "c536587f-791c-4772-8240-4ba228f5940c", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "2": {"node_id": "a7854179-f654-4452-bee0-2e6374805dfc", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "65442e1f5b1d7e30ab831d52bafc5a6dccc90a54d9303491c618f1a72b4b1012"}, "3": {"node_id": "b3d9f8c2-cbbc-48cc-9629-8c91b5fdb8bd", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "1ccc877fec10b7f3f269d3da92912e75606c343478f62b1b61a2eeed659c4861"}}, "hash": "ed60503561b1e8051a9255043c10f3aee6341319c7742e5a831390469dc0c8c1", "text": "```python\r\n...\r\nsetup_tab, terms_tab, upload_tab, query_tab = st.tabs(\r\n    [\"Setup\", \"All Terms\", \"Upload/Extract Terms\", \"Query Terms\"]\r\n)\r\n...\r\nwith terms_tab:\r\n    with terms_tab:\r\n    st.subheader(\"Current Extracted Terms and Definitions\")\r\n    st.json(st.session_state[\"all_terms\"])\r\n...\r\nwith query_tab:\r\n    st.subheader(\"Query for Terms/Definitions!\")st.markdown(\r\n        (\r\n            \"The LLM will attempt to answer your query, and augment it's answers using the terms/definitions you've inserted. \"\"If a term is not in the index, it will answer using it's internal knowledge.\")\r\n    )\r\n    if st.button(\"Initialize Index and Reset Terms\", key=\"init_index_2\"):\r\n        st.session_state[\"llama_index\"] = initialize_index(\r\n            llm_name, model_temperature, api_key\r\n        )\r\n        st.session_state[\"all_terms\"] = {}\r\n\r\n    if \"llama_index\" in st.session_state:\r\n        query_text = st.text_input(\"Ask about a term or definition:\")\r\n        if query_text:\r\n            query_text = query_text + \"\\nIf you can't find the answer, answer the query with the best of your knowledge.\"with st.spinner(\"Generating answer...\"):\r\n                response = st.session_state[\"llama_index\"].query(\r\n                    query_text, similarity_top_k=5, response_mode=\"compact\"\r\n                )\r\n            st.markdown(str(response))\r\n```\r\n\r\nWhile this is mostly basic, some important things to note:\r\n\r\n- Our initialize button has the same text as our other button.Streamlit will complain about this, so we provide a unique key instead.- Some additional text has been added to the query!This is to try and compensate for times when the index does not have the answer.- In our index query, we've specified two options:\r\n  - `similarity_top_k=5` means the index will fetch the top 5 closest matching terms/definitions to the query.- `response_mode=\"compact\"` means as much text as possible from the 5 matching terms/definitions will be used in each LLM call.Without this, the index would make at least 5 calls to the LLM, which can slow things down for the user.## Dry Run Test\r\n\r\nWell, actually I hope you've been testing as we went.But now, let's try one complete test.1.Refresh the app\r\n2.Enter your LLM settings\r\n3.Head over to the query tab\r\n4.Ask the following: `What is a bunnyhug?`\r\n5.The app should give some nonsense response.If you didn't know, a bunnyhug is another word for a hoodie, used by people from the Canadian Prairies!6.Let's add this definition to the app.Open the upload tab and enter the following text: `A bunnyhug is a common term used to describe a hoodie.This term is used by people from the Canadian Prairies.`\r\n7.Click the extract button.After a few moments, the app should display the correctly extracted term/definition.Click the insert term button to save it!8.If we open the terms tab, the term and definition we just extracted should be displayed\r\n9.Go back to the query tab and try asking what a bunnyhug is.Now, the answer should be correct!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b3d9f8c2-cbbc-48cc-9629-8c91b5fdb8bd": {"__data__": {"id_": "b3d9f8c2-cbbc-48cc-9629-8c91b5fdb8bd", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "2": {"node_id": "c536587f-791c-4772-8240-4ba228f5940c", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "ed60503561b1e8051a9255043c10f3aee6341319c7742e5a831390469dc0c8c1"}, "3": {"node_id": "c0afb204-4477-4656-aaf8-694298244255", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "7563d755ebf8542d6549debba0dcc61f251f4476621fde55fa9e497f3a898167"}}, "hash": "1ccc877fec10b7f3f269d3da92912e75606c343478f62b1b61a2eeed659c4861", "text": "## Improvement #1 - Create a Starting Index\r\n\r\nWith our base app working, it might feel like a lot of work to build up a useful index.What if we gave the user some kind of starting point to show off the app's query capabilities?We can do just that!First, let's make a small change to our app so that we save the index to disk after every upload:\r\n\r\n```python\r\ndef insert_terms(terms_to_definition):\r\n    for term, definition in terms_to_definition.items():\r\n        doc = Document(text=f\"Term: {term}\\nDefinition: {definition}\")\r\n        st.session_state['llama_index'].insert(doc)\r\n    # TEMPORARY - save to disk\r\n    st.session_state['llama_index'].storage_context.persist()\r\n```\r\n\r\nNow, we need some document to extract from!The repository for this project used the wikipedia page on New York City, and you can find the text [here](https://github.com/jerryjliu/llama_index/blob/main/examples/test_wiki/data/nyc_text.txt).If you paste the text into the upload tab and run it (it may take some time), we can insert the extracted terms.Make sure to also copy the text for the extracted terms into a notepad or similar before inserting into the index!We will need them in a second.After inserting, remove the line of code we used to save the index to disk.With a starting index now saved, we can modify our `initialize_index` function to look like this:\r\n\r\n```python\r\n@st.cache_resource\r\ndef initialize_index(llm_name, model_temperature, api_key):\r\n    \"\"\"Load the Index object.\"\"\"llm = get_llm(llm_name, model_temperature, api_key)\r\n\r\n    service_context = ServiceContext.from_defaults(llm=llm)\r\n\r\n    index = load_index_from_storage(service_context=service_context)\r\n\r\n    return index\r\n```\r\n\r\nDid you remember to save that giant list of extracted terms in a notepad?Now when our app initializes, we want to pass in the default terms that are in the index to our global terms state:\r\n\r\n```python\r\n...\r\nif \"all_terms\" not in st.session_state:\r\n    st.session_state[\"all_terms\"] = DEFAULT_TERMS\r\n...\r\n```\r\n\r\nRepeat the above anywhere where we were previously resetting the `all_terms` values.## Improvement #2 - (Refining) Better Prompts\r\n\r\nIf you play around with the app a bit now, you might notice that it stopped following our prompt!Remember, we added to our `query_str` variable that if the term/definition could not be found, answer to the best of its knowledge.But now if you try asking about random terms (like bunnyhug!), it may or may not follow those instructions.This is due to the concept of \"refining\" answers in Llama Index.Since we are querying across the top 5 matching results, sometimes all the results do not fit in a single prompt!OpenAI models typically have a max input size of 4097 tokens.So, Llama Index accounts for this by breaking up the matching results into chunks that will fit into the prompt.After Llama Index gets an initial answer from the first API call, it sends the next chunk to the API, along with the previous answer, and asks the model to refine that answer.So, the refine process seems to be messing with our results!Rather than appending extra instructions to the `query_str`, remove that, and Llama Index will let us provide our own custom prompts!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c0afb204-4477-4656-aaf8-694298244255": {"__data__": {"id_": "c0afb204-4477-4656-aaf8-694298244255", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "2": {"node_id": "b3d9f8c2-cbbc-48cc-9629-8c91b5fdb8bd", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "1ccc877fec10b7f3f269d3da92912e75606c343478f62b1b61a2eeed659c4861"}, "3": {"node_id": "de9ae151-e2de-44ac-b01a-b5e2646da0cb", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "3b3b2404236e9c496624c1a6dec4074edea25cb8648bba77aa9149d91ef0adf3"}}, "hash": "7563d755ebf8542d6549debba0dcc61f251f4476621fde55fa9e497f3a898167", "text": "Let's create those now, using the [default prompts](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/default_prompts.py) and [chat specific prompts](https://github.com/jerryjliu/llama_index/blob/main/llama_index/prompts/chat_prompts.py) as a guide.Using a new file `constants.py`, let's create some new query templates:\r\n\r\n```python\r\nfrom llama_index.prompts import PromptTemplate, SelectorPromptTemplate, ChatPromptTemplate\r\nfrom llama_index.prompts.utils import is_chat_model\r\nfrom llama_index.llms.base import ChatMessage, MessageRole\r\n\r\n# Text QA templates\r\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\r\n    \"Context information is below.\\n\"\r\n    \"---------------------\\n\"\r\n    \"{context_str}\"\r\n    \"\\n---------------------\\n\"\r\n    \"Given the context information answer the following question \"\r\n    \"(if you don't know the answer, use the best of your knowledge): {query_str}\\n\"\r\n)\r\nTEXT_QA_TEMPLATE = PromptTemplate(DEFAULT_TEXT_QA_PROMPT_TMPL)\r\n\r\n# Refine templates\r\nDEFAULT_REFINE_PROMPT_TMPL = (\r\n    \"The original question is as follows: {query_str}\\n\"\r\n    \"We have provided an existing answer: {existing_answer}\\n\"\r\n    \"We have the opportunity to refine the existing answer \"\r\n    \"(only if needed) with some more context below.\\n\"\r\n    \"------------\\n\"\r\n    \"{context_msg}\\n\"\r\n    \"------------\\n\"\r\n    \"Given the new context and using the best of your knowledge, improve the existing answer. \"\"If you can't improve the existing answer, just repeat it again.\")\r\nDEFAULT_REFINE_PROMPT = PromptTemplate(DEFAULT_REFINE_PROMPT_TMPL)\r\n\r\nCHAT_REFINE_PROMPT_TMPL_MSGS = [\r\n    ChatMessage(content=\"{query_str}\", role=MessageRole.USER),\r\n    ChatMessage(content=\"{existing_answer}\", role=MessageRole.ASSISTANT),\r\n    ChatMessage(\r\n        content=\"We have the opportunity to refine the above answer \"\r\n        \"(only if needed) with some more context below.\\n\"\r\n        \"------------\\n\"\r\n        \"{context_msg}\\n\"\r\n        \"------------\\n\"\r\n        \"Given the new context and using the best of your knowledge, improve the existing answer. \"\"If you can't improve the existing answer, just repeat it again.\",\r\n        role=MessageRole.USER,\r\n    ),\r\n]\r\n\r\nCHAT_REFINE_PROMPT = ChatPromptTemplate(CHAT_REFINE_PROMPT_TMPL_MSGS)\r\n\r\n# refine prompt selector\r\nREFINE_TEMPLATE = SelectorPromptTemplate(\r\n    default_template=DEFAULT_REFINE_PROMPT,\r\n    conditionals=[(is_chat_model, CHAT_REFINE_PROMPT)],\r\n)\r\n```\r\n\r\nThat seems like a lot of code, but it's not too bad!If you looked at the default prompts, you might have noticed that there are default prompts, and prompts specific to chat models.Continuing that trend, we do the same for our custom prompts.Then, using a prompt selector, we can combine both prompts into a single object.If the LLM being used is a chat model (ChatGPT, GPT-4), then the chat prompts are used.Otherwise, use the normal prompt templates.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "de9ae151-e2de-44ac-b01a-b5e2646da0cb": {"__data__": {"id_": "de9ae151-e2de-44ac-b01a-b5e2646da0cb", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "2": {"node_id": "c0afb204-4477-4656-aaf8-694298244255", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "7563d755ebf8542d6549debba0dcc61f251f4476621fde55fa9e497f3a898167"}, "3": {"node_id": "b8cc04a2-dcdc-4906-93a6-5680da2419b9", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "0dff16b5e8942273a8ab1a6e0637f32c4dd01fc3804d32b398dbe0bae8a9a93b"}}, "hash": "3b3b2404236e9c496624c1a6dec4074edea25cb8648bba77aa9149d91ef0adf3", "text": "Another thing to note is that we only defined one QA template.In a chat model, this will be converted to a single \"human\" message.So, now we can import these prompts into our app and use them during the query.```python\r\nfrom constants import REFINE_TEMPLATE, TEXT_QA_TEMPLATE\r\n...\r\n    if \"llama_index\" in st.session_state:\r\n        query_text = st.text_input(\"Ask about a term or definition:\")\r\n        if query_text:\r\n            query_text = query_text  # Notice we removed the old instructions\r\n            with st.spinner(\"Generating answer...\"):\r\n                response = st.session_state[\"llama_index\"].query(\r\n                    query_text, similarity_top_k=5, response_mode=\"compact\",\r\n                    text_qa_template=TEXT_QA_TEMPLATE, refine_template=REFINE_TEMPLATE\r\n                )\r\n            st.markdown(str(response))\r\n...\r\n```\r\n\r\nIf you experiment a bit more with queries, hopefully you notice that the responses follow our instructions a little better now!## Improvement #3 - Image Support\r\n\r\nLlama index also supports images!Using Llama Index, we can upload images of documents (papers, letters, etc.), and Llama Index handles extracting the text.We can leverage this to also allow users to upload images of their documents and extract terms and definitions from them.If you get an import error about PIL, install it using `pip install Pillow` first.```python\r\nfrom PIL import Image\r\nfrom llama_index.readers.file.base import DEFAULT_FILE_EXTRACTOR, ImageParser\r\n\r\n@st.cache_resource\r\ndef get_file_extractor():\r\n    image_parser = ImageParser(keep_image=True, parse_text=True)\r\n    file_extractor = DEFAULT_FILE_EXTRACTOR\r\n    file_extractor.update(\r\n        {\r\n            \".jpg\": image_parser,\r\n            \".png\": image_parser,\r\n            \".jpeg\": image_parser,\r\n        }\r\n    )\r\n\r\n    return file_extractor\r\n\r\nfile_extractor = get_file_extractor()\r\n...\r\nwith upload_tab:\r\n    st.subheader(\"Extract and Query Definitions\")\r\n    if st.button(\"Initialize Index and Reset Terms\", key=\"init_index_1\"):\r\n        st.session_state[\"llama_index\"] = initialize_index(\r\n            llm_name, model_temperature, api_key\r\n        )\r\n        st.session_state[\"all_terms\"] = DEFAULT_TERMS\r\n\r\n    if \"llama_index\" in st.session_state:\r\n        st.markdown(\r\n            \"Either upload an image/screenshot of a document, or enter the text manually.\")\r\n        uploaded_file = st.file_uploader(\r\n            \"Upload an image/screenshot of a document:\",", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8cc04a2-dcdc-4906-93a6-5680da2419b9": {"__data__": {"id_": "b8cc04a2-dcdc-4906-93a6-5680da2419b9", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "2": {"node_id": "de9ae151-e2de-44ac-b01a-b5e2646da0cb", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "3b3b2404236e9c496624c1a6dec4074edea25cb8648bba77aa9149d91ef0adf3"}, "3": {"node_id": "2e13ebfd-5b4e-4cf0-ada8-90a14f3b2460", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "3c18d0612ccf6edaee3d59a8dd1034b7e32e2f5163a8b873e4c400d693fde3a9"}}, "hash": "0dff16b5e8942273a8ab1a6e0637f32c4dd01fc3804d32b398dbe0bae8a9a93b", "text": "type=[\"png\", \"jpg\", \"jpeg\"]\r\n        )\r\n        document_text = st.text_area(\"Or enter raw text\")\r\n        if st.button(\"Extract Terms and Definitions\") and (\r\n            uploaded_file or document_text\r\n        ):\r\n            st.session_state[\"terms\"] = {}\r\n            terms_docs = {}\r\n            with st.spinner(\"Extracting (images may be slow).\"):\r\n                if document_text:\r\n                    terms_docs.update(\r\n                        extract_terms(\r\n                            [Document(text=document_text)],\r\n                            term_extract_str,\r\n                            llm_name,\r\n                            model_temperature,\r\n                            api_key,\r\n                        )\r\n                    )\r\n                if uploaded_file:\r\n                    Image.open(uploaded_file).convert(\"RGB\").save(\"temp.png\")\r\n                    img_reader = SimpleDirectoryReader(\r\n                        input_files=[\"temp.png\"], file_extractor=file_extractor\r\n                    )\r\n                    img_docs = img_reader.load_data()\r\n                    os.remove(\"temp.png\")\r\n                    terms_docs.update(\r\n                        extract_terms(\r\n                            img_docs,\r\n                            term_extract_str,\r\n                            llm_name,\r\n                            model_temperature,\r\n                            api_key,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2e13ebfd-5b4e-4cf0-ada8-90a14f3b2460": {"__data__": {"id_": "2e13ebfd-5b4e-4cf0-ada8-90a14f3b2460", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "c55e1b14aafa7ba8f46ea99154988b1462264ee322a845efbc93048a5710d575"}, "2": {"node_id": "b8cc04a2-dcdc-4906-93a6-5680da2419b9", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}, "hash": "0dff16b5e8942273a8ab1a6e0637f32c4dd01fc3804d32b398dbe0bae8a9a93b"}}, "hash": "3c18d0612ccf6edaee3d59a8dd1034b7e32e2f5163a8b873e4c400d693fde3a9", "text": ")\r\n                    )\r\n            st.session_state[\"terms\"].update(terms_docs)\r\n\r\n        if \"terms\" in st.session_state and st.session_state[\"terms\"]:\r\n            st.markdown(\"Extracted terms\")\r\n            st.json(st.session_state[\"terms\"])\r\n\r\n            if st.button(\"Insert terms?\"):\r\n                with st.spinner(\"Inserting terms\"):\r\n                    insert_terms(st.session_state[\"terms\"])\r\n                st.session_state[\"all_terms\"].update(st.session_state[\"terms\"])\r\n                st.session_state[\"terms\"] = {}\r\n                st.experimental_rerun()\r\n```\r\n\r\nHere, we added the option to upload a file using Streamlit.Then the image is opened and saved to disk (this seems hacky but it keeps things simple).Then we pass the image path to the reader, extract the documents/text, and remove our temp image file.Now that we have the documents, we can call `extract_terms()` the same as before.## Conclusion/TLDR\r\n\r\nIn this tutorial, we covered a ton of information, while solving some common issues and problems along the way:\r\n\r\n- Using different indexes for different use cases (List vs. Vector index)\r\n- Storing global state values with Streamlit's `session_state` concept\r\n- Customizing internal prompts with Llama Index\r\n- Reading text from images with Llama Index\r\n\r\nThe final version of this tutorial can be found [here](https://github.com/logan-markewich/llama_index_starter_pack) and a live hosted demo is available on [Huggingface Spaces](https://huggingface.co/spaces/llamaindex/llama_index_term_definition_demo).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "85dcd5ea-0c20-4794-b57b-921ea5d26d07": {"__data__": {"id_": "85dcd5ea-0c20-4794-b57b-921ea5d26d07", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4cbd683f9affb0df508057a7233bae3a8bffec39", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "hash": "5c0bec8e61dd09cf4c5f786d32f0cca40d38fce094fb90e8e3daa5b27175369b"}, "3": {"node_id": "dc9241f9-6eab-4ef3-9b1e-8d8b848793b1", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "hash": "ccc21f39f493252a924bff943196576db455a8f5b33ad30cb6e7e30daeffcd2b"}}, "hash": "48e02c11d5d6610f05b5636ebab188ec9643c477c2f718beedcec6657c6f7b53", "text": "# A Guide to Creating a Unified Query Framework over your Indexes\n\nLlamaIndex offers a variety of different [use cases](/end_to_end_tutorials/use_cases.md).For simple queries, we may want to use a single index data structure, such as a `VectorStoreIndex` for semantic search, or `SummaryIndex` for summarization.For more complex queries, we may want to use a composable graph.But how do we integrate indexes and graphs into our LLM application?Different indexes and graphs may be better suited for different types of queries that you may want to run.In this guide, we show how you can unify the diverse use cases of different index/graph structures under a **single** query framework.### Setup\n\nIn this example, we will analyze Wikipedia articles of different cities: Boston, Seattle, San Francisco, and more.The below code snippet downloads the relevant data into files.```python\n\nfrom pathlib import Path\nimport requests\n\nwiki_titles = [\"Toronto\", \"Seattle\", \"Chicago\", \"Boston\", \"Houston\"]\n\nfor title in wiki_titles:\n    response = requests.get(\n        'https://en.wikipedia.org/w/api.php',\n        params={\n            'action': 'query',\n            'format': 'json',\n            'titles': title,\n            'prop': 'extracts',\n            # 'exintro': True,\n            'explaintext': True,\n        }\n    ).json()\n    page = next(iter(response['query']['pages'].values()))\n    wiki_text = page['extract']\n\n    data_path = Path('data')\n    if not data_path.exists():\n        Path.mkdir(data_path)\n\n    with open(data_path / f\"{title}.txt\", 'w') as fp:\n        fp.write(wiki_text)\n\n```\n\nThe next snippet loads all files into Document objects.```python\n# Load all wiki documents\ncity_docs = {}\nfor wiki_title in wiki_titles:\n    city_docs[wiki_title] = SimpleDirectoryReader(input_files=[f\"data/{wiki_title}.txt\"]).load_data()\n\n```\n\n### Defining the Set of Indexes\n\nWe will now define a set of indexes and graphs over our data.You can think of each index/graph as a lightweight structure\nthat solves a distinct use case.We will first define a vector index over the documents of each city.```python\nfrom llama_index import VectorStoreIndex, ServiceContext, StorageContext\nfrom llama_index.llms import OpenAI\n\n# set service context\nllm_gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\nservice_context = ServiceContext.from_defaults(\n    llm=llm_gpt4, chunk_size=1024\n)\n\n# Build city document index\nvector_indices = {}\nfor wiki_title in wiki_titles:\n    storage_context = StorageContext.from_defaults()\n    # build vector index\n    vector_indices[wiki_title] = VectorStoreIndex.from_documents(\n        city_docs[wiki_title],\n        service_context=service_context,\n        storage_context=storage_context,\n    )\n    # set id for vector index\n    vector_indices[wiki_title].index_struct.index_id = wiki_title\n    # persist to disk\n    storage_context.persist(persist_dir=f'./storage/{wiki_title}')\n```\n\nQuerying a vector index lets us easily perform semantic search over a given city's documents.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dc9241f9-6eab-4ef3-9b1e-8d8b848793b1": {"__data__": {"id_": "dc9241f9-6eab-4ef3-9b1e-8d8b848793b1", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4cbd683f9affb0df508057a7233bae3a8bffec39", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "hash": "5c0bec8e61dd09cf4c5f786d32f0cca40d38fce094fb90e8e3daa5b27175369b"}, "2": {"node_id": "85dcd5ea-0c20-4794-b57b-921ea5d26d07", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "hash": "48e02c11d5d6610f05b5636ebab188ec9643c477c2f718beedcec6657c6f7b53"}, "3": {"node_id": "cc31c75a-073c-454c-afa5-386cca33651b", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "hash": "70e695f95b88b1b7b3a9fecd3a3dffbb55533fb180bc04e1c89ba8403a6ba2e0"}}, "hash": "ccc21f39f493252a924bff943196576db455a8f5b33ad30cb6e7e30daeffcd2b", "text": "```python\nresponse = vector_indices[\"Toronto\"].as_query_engine().query(\"What are the sports teams in Toronto?\")print(str(response))\n\n```\n\nExample response:\n\n```text\nThe sports teams in Toronto are the Toronto Maple Leafs (NHL), Toronto Blue Jays (MLB), Toronto Raptors (NBA), Toronto Argonauts (CFL), Toronto FC (MLS), Toronto Rock (NLL), Toronto Wolfpack (RFL), and Toronto Rush (NARL).```\n\n### Defining a Graph for Compare/Contrast Queries\n\nWe will now define a composed graph in order to run **compare/contrast** queries (see [use cases doc](/use_cases/queries.md)).This graph contains a keyword table composed on top of existing vector indexes.To do this, we first want to set the \"summary text\" for each vector index.```python\nindex_summaries = {}\nfor wiki_title in wiki_titles:\n    # set summary text for city\n    index_summaries[wiki_title] = (\n        f\"This content contains Wikipedia articles about {wiki_title}. \"f\"Use this index if you need to lookup specific facts about {wiki_title}.\\n\"\n        \"Do not use this index if you want to analyze multiple cities.\")\n```\n\nNext, we compose a keyword table on top of these vector indexes, with these indexes and summaries, in order to build the graph.```python\nfrom llama_index.indices.composability import ComposableGraph\n\ngraph = ComposableGraph.from_indices(\n    SimpleKeywordTableIndex,\n    [index for _, index in vector_indices.items()],\n    [summary for _, summary in index_summaries.items()],\n    max_keywords_per_chunk=50\n)\n\n# get root index\nroot_index = graph.get_index(graph.index_struct.root_id, SimpleKeywordTableIndex)\n# set id of root index\nroot_index.set_index_id(\"compare_contrast\")\nroot_summary = (\n    \"This index contains Wikipedia articles about multiple cities. \"\"Use this index if you want to compare multiple cities. \")\n\n```\n\nQuerying this graph (with a query transform module), allows us to easily compare/contrast between different cities.An example is shown below.```python\n# define decompose_transform\nfrom llama_index import LLMPredictor\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\n\ndecompose_transform = DecomposeQueryTransform(\n    LLMPredictor(llm=llm_gpt4), verbose=True\n)\n\n# define custom query engines\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\ncustom_query_engines = {}\nfor index in vector_indices.values():\n    query_engine = index.as_query_engine(service_context=service_context)\n    query_engine = TransformQueryEngine(\n        query_engine,\n        query_transform=decompose_transform,\n        transform_extra_info={'index_summary': index.index_struct.summary},\n    )\n    custom_query_engines[index.index_id] = query_engine\ncustom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n    retriever_mode='simple',\n    response_mode='tree_summarize',\n    service_context=service_context,\n)\n\n# define query engine\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\n\n# query the graph\nquery_str = (\n    \"Compare and contrast the arts and culture of Houston and Boston. \"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cc31c75a-073c-454c-afa5-386cca33651b": {"__data__": {"id_": "cc31c75a-073c-454c-afa5-386cca33651b", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4cbd683f9affb0df508057a7233bae3a8bffec39", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "hash": "5c0bec8e61dd09cf4c5f786d32f0cca40d38fce094fb90e8e3daa5b27175369b"}, "2": {"node_id": "dc9241f9-6eab-4ef3-9b1e-8d8b848793b1", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}, "hash": "ccc21f39f493252a924bff943196576db455a8f5b33ad30cb6e7e30daeffcd2b"}}, "hash": "70e695f95b88b1b7b3a9fecd3a3dffbb55533fb180bc04e1c89ba8403a6ba2e0", "text": ")\nresponse_chatgpt = query_engine.query(query_str)\n```\n\n### Defining the Unified Query Interface\n\nNow that we've defined the set of indexes/graphs, we want to build an **outer abstraction** layer that provides a unified query interface\nto our data structures.This means that during query-time, we can query this outer abstraction layer and trust that the right index/graph\nwill be used for the job.There are a few ways to do this, both within our framework as well as outside of it!- Build a **router query engine** on top of your existing indexes/graphs\n- Define each index/graph as a Tool within an agent framework (e.g.LangChain).For the purposes of this tutorial, we follow the former approach.If you want to take a look at how the latter approach works,\ntake a look at [our example tutorial here](/guides/tutorials/building_a_chatbot.md).Let's take a look at an example of building a router query engine to automatically \"route\" any query to the set of indexes/graphs that you have define under the hood.First, we define the query engines for the set of indexes/graph that we want to route our query to.We also give each a description (about what data it holds and what it's useful for) to help the router choose between them depending on the specific query.```python\nfrom llama_index.tools.query_engine import QueryEngineTool\n\nquery_engine_tools = []\n\n# add vector index tools\nfor wiki_title in wiki_titles:\n    index = vector_indices[wiki_title]\n    summary = index_summaries[wiki_title]\n\n    query_engine = index.as_query_engine(service_context=service_context)\n    vector_tool = QueryEngineTool.from_defaults(query_engine, description=summary)\n    query_engine_tools.append(vector_tool)\n\n\n# add graph tool\ngraph_description = (\n    \"This tool contains Wikipedia articles about multiple cities. \"\n    \"Use this tool if you want to compare multiple cities. \"\n)\ngraph_tool = QueryEngineTool.from_defaults(graph_query_engine, description=graph_description)\nquery_engine_tools.append(graph_tool)\n```\n\nNow, we can define the routing logic and overall router query engine.\nHere, we use the `LLMSingleSelector`, which uses LLM to choose a underlying query engine to route the query to.\n\n```python\nfrom llama_index.query_engine.router_query_engine import RouterQueryEngine\nfrom llama_index.selectors.llm_selectors import LLMSingleSelector\n\n\nrouter_query_engine = RouterQueryEngine(\n    selector=LLMSingleSelector.from_defaults(service_context=service_context),\n    query_engine_tools=query_engine_tools\n)\n```\n\n### Querying our Unified Interface\n\nThe advantage of a unified query interface is that it can now handle different types of queries.\n\nIt can now handle queries about specific cities (by routing to the specific city vector index), and also compare/contrast different cities.\n\nLet's take a look at a few examples!\n\n**Asking a Compare/Contrast Question**\n\n```python\n# ask a compare/contrast question\nresponse = router_query_engine.query(\n    \"Compare and contrast the arts and culture of Houston and Boston.\",\n)\nprint(str(response)\n```\n\n**Asking Questions about specific Cities**\n\n```python\n\nresponse = router_query_engine.query(\"What are the sports teams in Toronto?\")\nprint(str(response))\n\n```\n\nThis \"outer\" abstraction is able to handle different queries by routing to the right underlying abstractions.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cfb0fbc0-d960-4800-9444-02cb6807cec8": {"__data__": {"id_": "cfb0fbc0-d960-4800-9444-02cb6807cec8", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eceb8a6cb9b239b8ee4510c7381e95dd00442ca1", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data.md", "author": "LlamaIndex"}, "hash": "7b6229a1a1e6c2babc5cdb06d354b40279ae8d334e0437e737f8bf4e40e023cd"}}, "hash": "380ac8e195e74c5537e3a38ab2cbc8e86f98eef0e8faa2f6d80e80914ffc8214", "text": "# Structured Data\n\nRelevant Resources:\n- [A Guide to LlamaIndex + Structured Data](/end_to_end_tutorials/structured_data/sql_guide.md)\n- [Airbyte SQL Index Guide](/end_to_end_tutorials/structured_data/Airbyte_demo.ipynb)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "32cff3f3-52e3-49c6-aed1-ef7cf3e01a80": {"__data__": {"id_": "32cff3f3-52e3-49c6-aed1-ef7cf3e01a80", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data\\sql_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "985d3202c070a65b9b73f15defa8ce3e9f8185e5", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data\\sql_guide.md", "author": "LlamaIndex"}, "hash": "3eec435e0b59901056ce545e3a69fa791f4830a04341273a66a4be3805203f16"}, "3": {"node_id": "e752d1df-66ca-49a5-a082-c5c5924c6bde", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data\\sql_guide.md", "author": "LlamaIndex"}, "hash": "57118d835f901c436483f98a394dfcb1eb0a1187d0090c929e700f1b324dabd1"}}, "hash": "5de9630e44a732be44aa03351570eb7983a29b1a8796c842c68f0f5f8250d4ed", "text": "# A Guide to LlamaIndex + Structured Data\n\nA lot of modern data systems depend on structured data, such as a Postgres DB or a Snowflake data warehouse.LlamaIndex provides a lot of advanced features, powered by LLM's, to both create structured data from\nunstructured data, as well as analyze this structured data through augmented text-to-SQL capabilities.This guide helps walk through each of these capabilities.Specifically, we cover the following topics:\n- **Setup**: Defining up our example SQL Table.- **Building our Table Index**: How to go from sql database to a Table Schema Index\n- **Using natural language SQL queries**: How to query our SQL database using natural language.We will walk through a toy example table which contains city/population/country information.A notebook for this tutorial is [available here](../../examples/index_structs/struct_indices/SQLIndexDemo.ipynb).## Setup\n\nFirst, we use SQLAlchemy to setup a simple sqlite db:\n```python\nfrom sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n\nengine = create_engine(\"sqlite:///:memory:\")\nmetadata_obj = MetaData()\n```\n\nWe then create a toy `city_stats` table:\n```python\n# create city SQL table\ntable_name = \"city_stats\"\ncity_stats_table = Table(\n    table_name,\n    metadata_obj,\n    Column(\"city_name\", String(16), primary_key=True),\n    Column(\"population\", Integer),\n    Column(\"country\", String(16), nullable=False),\n)\nmetadata_obj.create_all(engine)\n```\n\nNow it's time to insert some datapoints!If you want to look into filling into this table by inferring structured datapoints\nfrom unstructured data, take a look at the below section.Otherwise, you can choose\nto directly populate this table:\n\n```python\nfrom sqlalchemy import insert\nrows = [\n    {\"city_name\": \"Toronto\", \"population\": 2731571, \"country\": \"Canada\"},\n    {\"city_name\": \"Tokyo\", \"population\": 13929286, \"country\": \"Japan\"},\n    {\"city_name\": \"Berlin\", \"population\": 600000, \"country\": \"Germany\"},\n]\nfor row in rows:\n    stmt = insert(city_stats_table).values(**row)\n    with engine.connect() as connection:\n        cursor = connection.execute(stmt)\n```\n\nFinally, we can wrap the SQLAlchemy engine with our SQLDatabase wrapper;\nthis allows the db to be used within LlamaIndex:\n\n```python\nfrom llama_index import SQLDatabase\n\nsql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n```\n\n## Natural language SQL\nOnce we have constructed our SQL database, we can use the NLSQLTableQueryEngine to\nconstruct natural language queries that are synthesized into SQL queries.Note that we need to specify the tables we want to use with this query engine.If we don't the query engine will pull all the schema context, which could\noverflow the context window of the LLM.```python\nquery_engine = NLSQLTableQueryEngine(\n    sql_database=sql_database,\n    tables=[\"city_stats\"],\n)\nquery_str = (\n    \"Which city has the highest population?\")\nresponse = query_engine.query(query_str)\n```\n\nThis query engine should used in any case where you can specify the tables you want\nto query over beforehand, or the total size of all the table schema plus the rest of\nthe prompt fits your context window.## Building our Table Index\nIf we don't know ahead of time which table we would like to use, and the total size of\nthe table schema overflows your context window size, we should store the table schema \nin an index so that during query time we can retrieve the right schema.The way we can do this is using the SQLTableNodeMapping object, which takes in a \nSQLDatabase and produces a Node object for each SQLTableSchema object passed \ninto the ObjectIndex constructor.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e752d1df-66ca-49a5-a082-c5c5924c6bde": {"__data__": {"id_": "e752d1df-66ca-49a5-a082-c5c5924c6bde", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data\\sql_guide.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "985d3202c070a65b9b73f15defa8ce3e9f8185e5", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data\\sql_guide.md", "author": "LlamaIndex"}, "hash": "3eec435e0b59901056ce545e3a69fa791f4830a04341273a66a4be3805203f16"}, "2": {"node_id": "32cff3f3-52e3-49c6-aed1-ef7cf3e01a80", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data\\sql_guide.md", "author": "LlamaIndex"}, "hash": "5de9630e44a732be44aa03351570eb7983a29b1a8796c842c68f0f5f8250d4ed"}}, "hash": "57118d835f901c436483f98a394dfcb1eb0a1187d0090c929e700f1b324dabd1", "text": "```python\ntable_node_mapping = SQLTableNodeMapping(sql_database)\ntable_schema_objs = [(SQLTableSchema(table_name=\"city_stats\")), ...] # one SQLTableSchema for each table\n\nobj_index = ObjectIndex.from_objects(\n    table_schema_objs,\n    table_node_mapping,\n    VectorStoreIndex,\n)\n```\n\nHere you can see we define our table_node_mapping, and a single SQLTableSchema with the\n\"city_stats\" table name.We pass these into the ObjectIndex constructor, along with the\nVectorStoreIndex class definition we want to use.This will give us a VectorStoreIndex where\neach Node contains table schema and other context information.You can also add any additional\ncontext information you'd like.```python\n# manually set extra context text\ncity_stats_text = (\n    \"This table gives information regarding the population and country of a given city.\\n\"\n    \"The user will query with codewords, where 'foo' corresponds to population and 'bar'\"\n    \"corresponds to city.\")\n\ntable_node_mapping = SQLTableNodeMapping(sql_database)\ntable_schema_objs = [(SQLTableSchema(table_name=\"city_stats\", context_str=city_stats_text))]\n```\n\n## Using natural language SQL queries\nOnce we have defined our table schema index obj_index, we can construct a SQLTableRetrieverQueryEngine\nby passing in our SQLDatabase, and a retriever constructed from our object index.```python\nquery_engine = SQLTableRetrieverQueryEngine(\n    sql_database, obj_index.as_retriever(similarity_top_k=1)\n)\nresponse = query_engine.query(\"Which city has the highest population?\")print(response)\n```\nNow when we query the retriever query engine, it will retrieve the relevant table schema\nand synthesize a SQL query and a response from the results of that query.## Concluding Thoughts\n\nThis is it for now!We're constantly looking for ways to improve our structured data support.If you have any questions let us know in [our Discord](https://discord.gg/dGcwcsnxhU).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5dadfaa5-7c77-47b0-8095-e55f75b47de2": {"__data__": {"id_": "5dadfaa5-7c77-47b0-8095-e55f75b47de2", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "684464f22f7dff5bf4e83508a84f2d4e79a71643f437d438abca53b8c4be5e96"}, "3": {"node_id": "f3cec787-bb9f-4a16-b5c1-8f2b60e67e51", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "1f997b9a576fea139ebfaec7cd506d22cbc04d807984d7c09d743bdba94b5d8d"}}, "hash": "337bf849ee59d0c6b66896e60f95cb49219f435317e6c830f03d88e6593f7746", "text": "# Basic Usage Pattern\n\nThe general usage pattern of LlamaIndex is as follows:\n\n1.Load in documents (either manually, or through a data loader)\n2.Parse the Documents into Nodes\n3.Construct Index (from Nodes or Documents)\n4.[Optional, Advanced] Building indices on top of other indices\n5.Query the index\n6.Parsing the response\n\n## 1.Load in Documents\n\nThe first step is to load in data.This data is represented in the form of `Document` objects.We provide a variety of [data loaders](/core_modules/data_modules/connector/root.md) which will load in Documents\nthrough the `load_data` function, e.g.:\n\n```python\nfrom llama_index import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('./data').load_data()\n```\n\nYou can also choose to construct documents manually.LlamaIndex exposes the `Document` struct.```python\nfrom llama_index import Document\n\ntext_list = [text1, text2, ...]\ndocuments = [Document(text=t) for t in text_list]\n```\n\nA Document represents a lightweight container around the data source.You can now choose to proceed with one of the\nfollowing steps:\n\n1.Feed the Document object directly into the index (see section 3).2.First convert the Document into Node objects (see section 2).## 2.Parse the Documents into Nodes\n\nThe next step is to parse these Document objects into Node objects.Nodes represent \"chunks\" of source Documents,\nwhether that is a text chunk, an image, or more.They also contain metadata and relationship information\nwith other nodes and index structures.Nodes are a first-class citizen in LlamaIndex.You can choose to define Nodes and all its attributes directly.You may also choose to \"parse\" source Documents into Nodes through our `NodeParser` classes.For instance, you can do\n\n```python\nfrom llama_index.node_parser import SimpleNodeParser\n\nparser = SimpleNodeParser.from_defaults()\n\nnodes = parser.get_nodes_from_documents(documents)\n```\n\nYou can also choose to construct Node objects manually and skip the first section.For instance,\n\n```python\nfrom llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n# set relationships\nnode1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(node_id=node2.node_id)\nnode2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(node_id=node1.node_id)\nnodes = [node1, node2]\n```\n\nThe `RelatedNodeInfo` class can also store additional `metadata` if needed:\n\n```python\nnode2.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(node_id=node1.node_id, metadata={\"key\": \"val\"})\n```\n\n## 3.Index Construction\n\nWe can now build an index over these Document objects.The simplest high-level abstraction is to load-in the Document objects during index initialization (this is relevant if you came directly from step 1 and skipped step 2).`from_documents` also takes an optional argument `show_progress`.Set it to `True` to display a progress bar during index construction.```python\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nYou can also choose to build an index over a set of Node objects directly (this is a continuation of step 2).```python\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex(nodes)\n```\n\nDepending on which index you use, LlamaIndex may make LLM calls in order to build the index.### Reusing Nodes across Index Structures\n\nIf you have multiple Node objects defined, and wish to share these Node\nobjects across multiple index structures, you can do that.Simply instantiate a StorageContext object,\nadd the Node objects to the underlying DocumentStore,\nand pass the StorageContext around.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f3cec787-bb9f-4a16-b5c1-8f2b60e67e51": {"__data__": {"id_": "f3cec787-bb9f-4a16-b5c1-8f2b60e67e51", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "684464f22f7dff5bf4e83508a84f2d4e79a71643f437d438abca53b8c4be5e96"}, "2": {"node_id": "5dadfaa5-7c77-47b0-8095-e55f75b47de2", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "337bf849ee59d0c6b66896e60f95cb49219f435317e6c830f03d88e6593f7746"}, "3": {"node_id": "cd8c80cc-4f13-4cd6-bce3-e120fd8e252e", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "8735c831ccd45785a0e574afa29aafc5acccb2eb7c3b375bf9295a146fd985a1"}}, "hash": "1f997b9a576fea139ebfaec7cd506d22cbc04d807984d7c09d743bdba94b5d8d", "text": "```python\nfrom llama_index import StorageContext\n\nstorage_context = StorageContext.from_defaults()\nstorage_context.docstore.add_documents(nodes)\n\nindex1 = VectorStoreIndex(nodes, storage_context=storage_context)\nindex2 = SummaryIndex(nodes, storage_context=storage_context)\n```\n\n**NOTE**: If the `storage_context` argument isn't specified, then it is implicitly\ncreated for each index during index construction.You can access the docstore\nassociated with a given index through `index.storage_context`.### Inserting Documents or Nodes\n\nYou can also take advantage of the `insert` capability of indices to insert Document objects\none at a time instead of during index construction.```python\nfrom llama_index import VectorStoreIndex\n\nindex = VectorStoreIndex([])\nfor doc in documents:\n    index.insert(doc)\n```\n\nIf you want to insert nodes on directly you can use `insert_nodes` function\ninstead.```python\nfrom llama_index import VectorStoreIndex\n\n# nodes: Sequence[Node]\nindex = VectorStoreIndex([])\nindex.insert_nodes(nodes)\n```\n\nSee the [Document Management How-To](/core_modules/data_modules/index/document_management.md) for more details on managing documents and an example notebook.### Customizing Documents\n\nWhen creating documents, you can also attach useful metadata.Any metadata added to a document will be copied to the nodes that get created from their respective source document.```python\ndocument = Document(\n    text='text',\n    metadata={\n        'filename': '<doc_file_name>',\n        'category': '<category>'\n    }\n)\n```\n\nMore information and approaches to this are discussed in the section [Customizing Documents](/core_modules/data_modules/documents_and_nodes/usage_documents.md).### Customizing LLM's\n\nBy default, we use OpenAI's `text-davinci-003` model.You may choose to use another LLM when constructing\nan index.```python\nfrom llama_index import VectorStoreIndex, ServiceContext, set_global_service_context\nfrom llama_index.llms import OpenAI\n\n...\n\n# define LLM\nllm = OpenAI(model=\"gpt-4\", temperature=0, max_tokens=256)\n\n# configure service context\nservice_context = ServiceContext.from_defaults(llm=llm)\nset_global_service_context(service_context)\n\n# build index\nindex = VectorStoreIndex.from_documents(\n    documents\n)\n```\n\nTo save costs, you may want to use a local model.```python\nfrom llama_index import ServiceContext\nservice_context = ServiceContext.from_defaults(llm=\"local\")\n```\n\nThis will use llama2-chat-13B from with LlamaCPP, and assumes you have `llama-cpp-python` installed.Full LlamaCPP usage guide is available in a [notebook here](/examples/llm/llama_2_llama_cpp.ipynb).See the [Custom LLM's How-To](/core_modules/model_modules/llms/usage_custom.md) for more details.### Global ServiceContext\n\nIf you wanted the service context from the last section to always be the default, you can configure one like so:\n\n```python\nfrom llama_index import set_global_service_context\nset_global_service_context(service_context)\n```\n\nThis service context will always be used as the default if not specified as a keyword argument in LlamaIndex functions.For more details on the service context, including how to create a global service context, see the page [Customizing the ServiceContext](/core_modules/supporting_modules/service_context.md).### Customizing Prompts\n\nDepending on the index used, we used default prompt templates for constructing the index (and also insertion/querying).See [Custom Prompts How-To](/core_modules/model_modules/prompts.md) for more details on how to customize your prompt.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cd8c80cc-4f13-4cd6-bce3-e120fd8e252e": {"__data__": {"id_": "cd8c80cc-4f13-4cd6-bce3-e120fd8e252e", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "684464f22f7dff5bf4e83508a84f2d4e79a71643f437d438abca53b8c4be5e96"}, "2": {"node_id": "f3cec787-bb9f-4a16-b5c1-8f2b60e67e51", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "1f997b9a576fea139ebfaec7cd506d22cbc04d807984d7c09d743bdba94b5d8d"}, "3": {"node_id": "8fd22a72-b9d0-44e8-a82d-daad4ee4d958", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "7fed7d5ce1f930ad0307c1b17b19cf4595bbca56064f03ed76a71eff55b5a5eb"}}, "hash": "8735c831ccd45785a0e574afa29aafc5acccb2eb7c3b375bf9295a146fd985a1", "text": "### Customizing embeddings\n\nFor embedding-based indices, you can choose to pass in a custom embedding model.See\n[Custom Embeddings How-To](/core_modules/model_modules/embeddings/usage_pattern.md) for more details.### Cost Analysis \n\nCreating an index, inserting to an index, and querying an index may use tokens.We can track\ntoken usage through the outputs of these operations.When running operations,\nthe token usage will be printed.You can also fetch the token usage through `TokenCountingCallback` handler.See [Cost Analysis How-To](/core_modules/supporting_modules/cost_analysis/usage_pattern.md) for more details.### [Optional] Save the index for future use\n\nBy default, data is stored in-memory.To persist to disk:\n\n```python\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\n```\n\nYou may omit persist_dir to persist to `./storage` by default.To reload from disk:\n\n```python\nfrom llama_index import StorageContext, load_index_from_storage\n\n# rebuild storage context\nstorage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n\n# load index\nindex = load_index_from_storage(storage_context)\n```\n\n**NOTE**: If you had initialized the index with a custom\n`ServiceContext` object, you will also need to pass in the same\nServiceContext during `load_index_from_storage` or ensure you have a global sevice context.```python\n\nservice_context = ServiceContext.from_defaults(llm=llm)\nset_global_service_context(service_context)\n\n# when first building the index\nindex = VectorStoreIndex.from_documents(\n    documents, # service_context=service_context -> optional if not using global\n)\n\n...\n\n# when loading the index from disk\nindex = load_index_from_storage(\n    StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n    # service_context=service_context -> optional if not using global\n)\n\n```\n\n## 4.[Optional, Advanced] Building indices on top of other indices\n\nYou can build indices on top of other indices!Composability gives you greater power in indexing your heterogeneous sources of data.For a discussion on relevant use cases,\nsee our [Query Use Cases](/end_to_end_tutorials/question_and_answer.md).For technical details and examples, see our [Composability How-To](/core_modules/data_modules/index/composability.md).## 5.Query the index.After building the index, you can now query it with a `QueryEngine`.Note that a \"query\" is simply an input to an LLM -\nthis means that you can use the index for question-answering, but you can also do more than that!### High-level API\n\nTo start, you can query an index with the default `QueryEngine` (i.e., using default configs), as follows:\n\n```python\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")print(response)\n\nresponse = query_engine.query(\"Write an email to the user given their background information.\")print(response)\n```\n\n### Low-level API\n\nWe also support a low-level composition API that gives you more granular control over the query logic.Below we highlight a few of the possible customizations.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8fd22a72-b9d0-44e8-a82d-daad4ee4d958": {"__data__": {"id_": "8fd22a72-b9d0-44e8-a82d-daad4ee4d958", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "684464f22f7dff5bf4e83508a84f2d4e79a71643f437d438abca53b8c4be5e96"}, "2": {"node_id": "cd8c80cc-4f13-4cd6-bce3-e120fd8e252e", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "8735c831ccd45785a0e574afa29aafc5acccb2eb7c3b375bf9295a146fd985a1"}, "3": {"node_id": "5d5471e1-ed95-4deb-83ec-eb06ae3147bf", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "0a7bb6baebb20ffe9889b60d89c30f15c7c2292e4c85c2402731e1b6658acb3e"}}, "hash": "7fed7d5ce1f930ad0307c1b17b19cf4595bbca56064f03ed76a71eff55b5a5eb", "text": "```python\nfrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.retrievers import VectorIndexRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n\n# configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=2,\n)\n\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer()\n\n# assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=0.7)\n    ]\n\n)\n\n# query\nresponse = query_engine.query(\"What did the author do growing up?\")print(response)\n```\n\nYou may also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.For a full list of implemented components and the supported configurations, please see the detailed [reference docs](/api_reference/query.rst).In the following, we discuss some commonly used configurations in detail.### Configuring retriever\n\nAn index can have a variety of index-specific retrieval modes.For instance, a summary index supports the default `SummaryIndexRetriever` that retrieves all nodes, and\n`SummaryIndexEmbeddingRetriever` that retrieves the top-k nodes by embedding similarity.For convienience, you can also use the following shorthand:\n\n```python\n    # SummaryIndexRetriever\n    retriever = index.as_retriever(retriever_mode='default')\n    # SummaryIndexEmbeddingRetriever\n    retriever = index.as_retriever(retriever_mode='embedding')\n```\n\nAfter choosing your desired retriever, you can construct your query engine:\n\n```python\nquery_engine = RetrieverQueryEngine(retriever)\nresponse = query_engine.query(\"What did the author do growing up?\")```\n\nThe full list of retrievers for each index (and their shorthand) is documented in the [Query Reference](/api_reference/query.rst).(setting-response-mode)=\n\n### Configuring response synthesis\nAfter a retriever fetches relevant nodes, a `BaseSynthesizer` synthesizes the final response by combining the information.You can configure it via\n\n```python\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode=<response_mode>)\n```\n\nRight now, we support the following options:\n\n- `default`: \"create and refine\" an answer by sequentially going through each retrieved `Node`;\n  This makes a separate LLM call per Node.Good for more detailed answers.- `compact`: \"compact\" the prompt during each LLM call by stuffing as\n  many `Node` text chunks that can fit within the maximum prompt size.If there are\n  too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n  multiple prompts.- `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree\n  and return the root node as the response.Good for summarization purposes.- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM,\n  without actually sending them.Then can be inspected by checking `response.source_nodes`.The response object is covered in more detail in Section 5.- `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text\n  chunk while accumulating the responses into an array.Returns a concatenated string of all\n  responses.Good for when you need to run the same query separately against each text\n  chunk.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5d5471e1-ed95-4deb-83ec-eb06ae3147bf": {"__data__": {"id_": "5d5471e1-ed95-4deb-83ec-eb06ae3147bf", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "684464f22f7dff5bf4e83508a84f2d4e79a71643f437d438abca53b8c4be5e96"}, "2": {"node_id": "8fd22a72-b9d0-44e8-a82d-daad4ee4d958", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}, "hash": "7fed7d5ce1f930ad0307c1b17b19cf4595bbca56064f03ed76a71eff55b5a5eb"}}, "hash": "0a7bb6baebb20ffe9889b60d89c30f15c7c2292e4c85c2402731e1b6658acb3e", "text": "```python\nindex = SummaryIndex.from_documents(documents)\nretriever = index.as_retriever()\n\n# default\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='default')\nresponse = query_engine.query(\"What did the author do growing up?\")# compact\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='compact')\nresponse = query_engine.query(\"What did the author do growing up?\")# tree summarize\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='tree_summarize')\nresponse = query_engine.query(\"What did the author do growing up?\")# no text\nquery_engine = RetrieverQueryEngine.from_args(retriever, response_mode='no_text')\nresponse = query_engine.query(\"What did the author do growing up?\")```\n\n### Configuring node postprocessors (i.e.filtering and augmentation)\n\nWe also support advanced `Node` filtering and augmentation that can further improve the relevancy of the retrieved `Node` objects.This can help reduce the time/number of LLM calls/cost or improve response quality.For example:\n\n- `KeywordNodePostprocessor`: filters nodes by `required_keywords` and `exclude_keywords`.- `SimilarityPostprocessor`: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\n- `PrevNextNodePostprocessor`: augments retrieved `Node` objects with additional relevant context based on `Node` relationships.The full list of node postprocessors is documented in the [Node Postprocessor Reference](/api_reference/node_postprocessor.rst).To configure the desired node postprocessors:\n\n```python\nnode_postprocessors = [\n    KeywordNodePostprocessor(\n        required_keywords=[\"Combinator\"],\n        exclude_keywords=[\"Italy\"]\n    )\n]\nquery_engine = RetrieverQueryEngine.from_args(\n    retriever, node_postprocessors=node_postprocessors\n)\nresponse = query_engine.query(\"What did the author do growing up?\")```\n\n## 6.Parsing the response\n\nThe object returned is a [`Response` object](/api_reference/response.rst).The object contains both the response text as well as the \"sources\" of the response:\n\n```python\nresponse = query_engine.query(\"<query_str>\")\n\n# get response\n# response.response\nstr(response)\n\n# get sources\nresponse.source_nodes\n# formatted sources\nresponse.get_formatted_sources()\n```\n\nAn example is shown below.![](/_static/response/response_1.jpeg)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e43348b4-496d-466c-ab26-1b0027fb8ede": {"__data__": {"id_": "e43348b4-496d-466c-ab26-1b0027fb8ede", "embedding": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\use_cases.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80cb7a9fc2697667f34d5456b06fa5642394affe", "node_type": null, "metadata": {"filename": "docs\\end_to_end_tutorials\\use_cases.md", "author": "LlamaIndex"}, "hash": "af8feb07141f32820692e5bcd154c4b8ea4053844b3d53928386c36860cb5ad0"}}, "hash": "bd56047409f1326b0ecdedaf5800e7851a717daee851dd8bf687acb697508232", "text": "# Use Cases\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/end_to_end_tutorials/question_and_answer.md\n/end_to_end_tutorials/chatbots.md\n/end_to_end_tutorials/agents.md\n/end_to_end_tutorials/graphs.md\n/end_to_end_tutorials/structured_data.md\n/end_to_end_tutorials/apps.md\n/end_to_end_tutorials/privacy.md\nFinetuning Llama 2 for Text-to-SQL <https://medium.com/llamaindex-blog/easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d>\nFinetuning GPT-3.5 to Distill GPT-4 <https://colab.research.google.com/drive/1vWeJBXdFEObuihO7Z8ui2CAYkdHQORqo?usp=sharing>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c586ca61-5735-49bc-ac31-dbc9677f884a": {"__data__": {"id_": "c586ca61-5735-49bc-ac31-dbc9677f884a", "embedding": null, "metadata": {"filename": "docs\\examples\\data_connectors\\README.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7608fa06109bba4ab1e5ba7ec192deac6eff023", "node_type": null, "metadata": {"filename": "docs\\examples\\data_connectors\\README.md", "author": "LlamaIndex"}, "hash": "d4c444a22d2cfedc7c82b98701d008a62dbe6fe7fdeae34f6fdd2d05ea105b84"}}, "hash": "9a3464b3eb7355697b6d749f7d6509648b81876bddbc6e669731d488f3d47d90", "text": "## Data Connector Examples\n\nEach of these notebooks showcase our readers which can read data from a variety of data sources.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5f685536-34c9-4246-b8de-d9babb267b3c": {"__data__": {"id_": "5f685536-34c9-4246-b8de-d9babb267b3c", "embedding": null, "metadata": {"filename": "docs\\getting_started\\FAQ.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32cce9075d6bdc38f5389e97b109b8163e31b622", "node_type": null, "metadata": {"filename": "docs\\getting_started\\FAQ.md", "author": "LlamaIndex"}, "hash": "480cdb050d7f3be7215f5a0bc590b25e3146431f9454dcb1da071a2685a383eb"}}, "hash": "fe5444432a4fc6e6cb01f9daf971fcaad9ba921f0c6a2bf28f0f925bab729503", "text": "# FAQ", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5863ca78-b666-457d-ac16-851c2e683cc9": {"__data__": {"id_": "5863ca78-b666-457d-ac16-851c2e683cc9", "embedding": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "417c7df2531ebb4b15a98589c62a77a1b2baab70", "node_type": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "hash": "bcb93461ff4be70fa2f586f086bb862bdd81fb03791a06b4632c5cf1147ba44c"}, "3": {"node_id": "36c049ca-bfa2-4835-9c50-b1e5f333e759", "node_type": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "hash": "a636c9cff9c8364de4094fcc5c8b8e24fcf8cf5ec3f30acd0a8a273db6cce2cd"}}, "hash": "28d47b5cb5728a59975802c9f1843993ae601cfdbd6b36a9557f594099f4a00d", "text": "# High-Level Concepts\n\n```{tip}\nIf you haven't, [install](/getting_started/installation.md) and complete [starter tutorial](/getting_started/starter_example.md) before you read this. It will make a lot more sense!\n```\n\nLlamaIndex helps you build LLM-powered applications (e.g. Q&A, chatbot, and agents) over custom data.\n\nIn this high-level concepts guide, you will learn:\n* the retrieval augmented generation (RAG) paradigm for combining LLM with custom data,\n* key concepts and modules in LlamaIndex for composing your own RAG pipeline.\n\n## Retrieval Augmented Generation (RAG)\nRetrieval augmented generation (RAG) is a paradigm for augmenting LLM with custom data.\nIt generally consists of two stages: \n1) **indexing stage**: preparing a knowledge base, and\n2) **querying stage**: retrieving relevant context from the knowledge to assist the LLM in responding to a question\n\n![](/_static/getting_started/rag.jpg)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "36c049ca-bfa2-4835-9c50-b1e5f333e759": {"__data__": {"id_": "36c049ca-bfa2-4835-9c50-b1e5f333e759", "embedding": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "417c7df2531ebb4b15a98589c62a77a1b2baab70", "node_type": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "hash": "bcb93461ff4be70fa2f586f086bb862bdd81fb03791a06b4632c5cf1147ba44c"}, "2": {"node_id": "5863ca78-b666-457d-ac16-851c2e683cc9", "node_type": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "hash": "28d47b5cb5728a59975802c9f1843993ae601cfdbd6b36a9557f594099f4a00d"}, "3": {"node_id": "41bc4137-2d62-416e-9f3d-c92b75b40965", "node_type": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "hash": "640796281170a32ebc0fbbf4947a3fb6d221d948eab8d87e0d5c8be4b2a30307"}}, "hash": "a636c9cff9c8364de4094fcc5c8b8e24fcf8cf5ec3f30acd0a8a273db6cce2cd", "text": "LlamaIndex provides the essential toolkit for making both steps super easy.\nLet's explore each stage in detail.\n\n### Indexing Stage\nLlamaIndex help you prepare the knowledge base with a suite of data connectors and indexes.\n![](/_static/getting_started/indexing.jpg) \n\n[**Data Connectors**](/core_modules/data_modules/connector/root.md):\nA data connector (i.e. `Reader`) ingest data from different data sources and data formats into a simple `Document` representation (text and simple metadata).\n\n[**Documents / Nodes**](/core_modules/data_modules/documents_and_nodes/root.md): A `Document` is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A `Node` is the atomic unit of data in LlamaIndex and represents a \"chunk\" of a source `Document`. It's a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations.\n\n[**Data Indexes**](/core_modules/data_modules/index/root.md): \nOnce you've ingested your data, LlamaIndex will help you index the data into a format that's easy to retrieve. Under the hood, LlamaIndex parses the raw documents into intermediate representations, calculates vector embeddings, and infers metadata. The most commonly used index is the [VectorStoreIndex](/core_modules/data_modules/index/vector_store_guide.ipynb)\n\n### Querying Stage\nIn the querying stage, the RAG pipeline retrieves the most relevant context given a user query,\nand pass that to the LLM (along with the query) to synthesize a response.\nThis gives the LLM up-to-date knowledge that is not in its original training data,\n(also reducing hallucination).\nThe key challenge in the querying stage is retrieval, orchestration, and reasoning over (potentially many) knowledge bases.\n\nLlamaIndex provides composable modules that help you build and integrate RAG pipelines for Q&A (query engine), chatbot (chat engine), or as part of an agent.\nThese building blocks can be customized to reflect ranking preferences, as well as composed to reason over multiple knowledge bases in a structured way.\n\n![](/_static/getting_started/querying.jpg)\n\n#### Building Blocks\n[**Retrievers**](/core_modules/query_modules/retriever/root.md): \nA retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query.\nThe specific retrieval logic differs for different indices, the most popular being dense retrieval against a vector index.\n\n[**Node Postprocessors**](/core_modules/query_modules/node_postprocessors/root.md):\nA node postprocessor takes in a set of nodes, then apply transformation, filtering, or re-ranking logic to them. \n\n[**Response Synthesizers**](/core_modules/query_modules/response_synthesizers/root.md):\nA response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.  \n\n#### Pipelines\n\n[**Query Engines**](/core_modules/query_modules/query_engine/root.md):\nA query engine is an end-to-end pipeline that allow you to ask question over your data.\nIt takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "41bc4137-2d62-416e-9f3d-c92b75b40965": {"__data__": {"id_": "41bc4137-2d62-416e-9f3d-c92b75b40965", "embedding": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "417c7df2531ebb4b15a98589c62a77a1b2baab70", "node_type": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "hash": "bcb93461ff4be70fa2f586f086bb862bdd81fb03791a06b4632c5cf1147ba44c"}, "2": {"node_id": "36c049ca-bfa2-4835-9c50-b1e5f333e759", "node_type": null, "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}, "hash": "a636c9cff9c8364de4094fcc5c8b8e24fcf8cf5ec3f30acd0a8a273db6cce2cd"}}, "hash": "640796281170a32ebc0fbbf4947a3fb6d221d948eab8d87e0d5c8be4b2a30307", "text": "[**Chat Engines**](/core_modules/query_modules/chat_engines/root.md): \nA chat engine is an end-to-end pipeline for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).\n\n[**Agents**](/core_modules/agent_modules/agents/root.md): \nAn agent is an automated decision maker (powered by an LLM) that interacts with the world via a set of tools.\nAgent may be used in the same fashion as query engines or chat engines. \nThe main distinction is that an agent dynamically decides the best sequence of actions, instead of following a predetermined logic.\nThis gives it additional flexibility to tackle more complex tasks.\n\n```{admonition} Next Steps\n* tell me how to [customize things](/getting_started/customization.rst).\n* curious about a specific module? Check out the module guides \ud83d\udc48\n* have a use case in mind? Check out the [end-to-end tutorials](/end_to_end_tutorials/use_cases.md)\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4a242449-8afc-476b-b24b-71b5aadb4cfe": {"__data__": {"id_": "4a242449-8afc-476b-b24b-71b5aadb4cfe", "embedding": null, "metadata": {"filename": "docs\\getting_started\\installation.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa38de4744f41b9080a3dc7bcbcea59b5d78862e", "node_type": null, "metadata": {"filename": "docs\\getting_started\\installation.md", "author": "LlamaIndex"}, "hash": "d0403811548e6174c888abbc0e44d6df3475987ab5964b98ce6e6ba1d5da6f94"}}, "hash": "35e4cb4aeed07ec3c277d134d44a58f4f0d3ca6b1174ca05d13aaa25cd3c9d8b", "text": "# Installation and Setup\n\n## Installation from Pip\n\nYou can simply do:\n\n```\npip install llama-index\n```\n\n**NOTE:** LlamaIndex may download and store local files for various packages (NLTK, HuggingFace, ...). Use the environment variable \"LLAMA_INDEX_CACHE_DIR\" to control where these files are saved.\n\n## Installation from Source\n\nGit clone this repository: `git clone https://github.com/jerryjliu/llama_index.git`. Then do:\n\n- `pip install -e .` if you want to do an editable install (you can modify source files) of just the package itself.\n- `pip install -r requirements.txt` if you want to install optional dependencies + dependencies used for development (e.g. unit testing).\n\n## OpenAI Environment Setup\n\nBy default, we use the OpenAI `gpt-3.5-turbo` model for text generation and `text-embedding-ada-002` for retrieval and embeddings. In order to use this, you must have an OPENAI_API_KEY setup.\nYou can register an API key by logging into [OpenAI's page and creating a new API token](https://beta.openai.com/account/api-keys).\n\n```{tip}\nYou can also [customize the underlying LLM](/core_modules/model_modules/llms/usage_custom.md). You may\nneed additional environment keys + tokens setup depending on the LLM provider.\n```\n\n## Local Environment Setup\n\nIf you don't wish to use OpenAI, the environment will automatically fallback to using `LlamaCPP` and `llama2-chat-13B` for text generation and `BAAI/bge-small-en` for retrieval and embeddings. This models will all run locally.\n\nIn order to use `LlamaCPP`, follow the installation guide [here](/examples/llm/llama_2_llama_cpp.ipynb). You'll need to install the `llama-cpp-python` package, preferably compiled to support your GPU. This will use aronund 11.5GB of memory across the CPU and GPU.\n\nIn order to use the local embeddings, simply run `pip install sentence-transformers`. The local embedding model uses about 500MB of memory.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1348302c-f1a8-4312-90db-958594383c5a": {"__data__": {"id_": "1348302c-f1a8-4312-90db-958594383c5a", "embedding": null, "metadata": {"filename": "docs\\getting_started\\starter_example.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4de173267fcf10f8781c8c28c618365c3d03b0d", "node_type": null, "metadata": {"filename": "docs\\getting_started\\starter_example.md", "author": "LlamaIndex"}, "hash": "9a428789589df905a3d8b362a0453884b66640243463404615c190bdc290ac9f"}, "3": {"node_id": "f1aaf379-77fe-4e70-adbe-adc62ec58f48", "node_type": null, "metadata": {"filename": "docs\\getting_started\\starter_example.md", "author": "LlamaIndex"}, "hash": "aaf4aae6ce15b1eb6ae7892c58fb840f92a77f2ee3fce7db63dcf85d433bf64c"}}, "hash": "c713b8bdd885040318b7967d84129dddc6e97db7c75533f5001ff6f537c1afb5", "text": "# Starter Tutorial\n\n```{tip}\nMake sure you've followed the [installation](installation.md) steps first.\n```\nHere is a starter example for using LlamaIndex. \n\n### Download\n\nLlamaIndex examples can be found in the `examples` folder of the LlamaIndex repository.\nWe first want to download this `examples` folder. An easy way to do this is to just clone the repo:\n\n```bash\n$ git clone https://github.com/jerryjliu/llama_index.git\n```\n\nNext, navigate to your newly-cloned repository, and verify the contents:\n\n```bash\n$ cd llama_index\n$ ls\nLICENSE                data_requirements.txt  tests/\nMANIFEST.in            examples/              pyproject.toml\nMakefile               experimental/          requirements.txt\nREADME.md              llama_index/             setup.py\n```\n\nWe now want to navigate to the following folder:\n\n```bash\n$ cd examples/paul_graham_essay\n```\n\nThis contains LlamaIndex examples around Paul Graham's essay, [\"What I Worked On\"](http://paulgraham.com/worked.html). A comprehensive set of examples are already provided in `TestEssay.ipynb`. For the purposes of this tutorial, we can focus on a simple example of getting LlamaIndex up and running.\n\n### Build and Query Index\n\nCreate a new `.py` file with the following:\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nThis builds an index over the documents in the `data` folder (which in this case just consists of the essay text). We then run the following\n\n```python\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\nprint(response)\n```\n\nYou should get back a response similar to the following: `The author wrote short stories and tried to program on an IBM 1401.`\n\n### Viewing Queries and Events Using Logging\n\nIn a Jupyter notebook, you can view info and/or debugging logging using the following snippet:\n\n```python\nimport logging\nimport sys\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n```\n\nYou can set the level to `DEBUG` for verbose output, or use `level=logging.INFO` for less.\n\nTo view all requests made to your LLMs, you can set the `openai` logging flag:\n\n```python\nopenai.log = \"debug\"\n```\n\nThis will print out every request and response made via `openai`. Change it back to `None` to turn off.\n\n### Saving and Loading\n\nBy default, data is stored in-memory.\nTo persist to disk (under `./storage`):\n\n```python\nindex.storage_context.persist()\n```\n\nTo reload from disk:\n```python\nfrom llama_index import StorageContext, load_index_from_storage\n\n# rebuild storage context\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n# load index\nindex = load_index_from_storage(storage_context)\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f1aaf379-77fe-4e70-adbe-adc62ec58f48": {"__data__": {"id_": "f1aaf379-77fe-4e70-adbe-adc62ec58f48", "embedding": null, "metadata": {"filename": "docs\\getting_started\\starter_example.md", "author": "LlamaIndex"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4de173267fcf10f8781c8c28c618365c3d03b0d", "node_type": null, "metadata": {"filename": "docs\\getting_started\\starter_example.md", "author": "LlamaIndex"}, "hash": "9a428789589df905a3d8b362a0453884b66640243463404615c190bdc290ac9f"}, "2": {"node_id": "1348302c-f1a8-4312-90db-958594383c5a", "node_type": null, "metadata": {"filename": "docs\\getting_started\\starter_example.md", "author": "LlamaIndex"}, "hash": "c713b8bdd885040318b7967d84129dddc6e97db7c75533f5001ff6f537c1afb5"}}, "hash": "aaf4aae6ce15b1eb6ae7892c58fb840f92a77f2ee3fce7db63dcf85d433bf64c", "text": "```{admonition} Next Steps\n* learn more about the [high-level concepts](/getting_started/concepts.md).\n* tell me how to [customize things](/getting_started/customization.rst).\n* curious about a specific module? check out the guides \ud83d\udc48\n* have a use case in mind? check out the [end-to-end tutorials](/end_to_end_tutorials/use_cases.md)\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"4a7df91f0acf9a51e5f123e147fd874bac68c34e": {"node_ids": ["4437d6b6-ead6-4a1d-9251-df93995d95b8"], "metadata": {"filename": "docs\\DOCS_README.md", "author": "LlamaIndex"}}, "5cac394d1eca7c5b0b16ee0b4d7e69393162fa47": {"node_ids": ["e6af2f51-3bf0-45b1-8297-620133b65139", "e75ac405-364a-43a9-b9c3-95e2bf6f4185"], "metadata": {"filename": "docs\\community\\app_showcase.md", "author": "LlamaIndex"}}, "e72ada566eb09c9ef1cf9fbf1019e9b5c166bcc8": {"node_ids": ["679f0feb-a0e4-450e-9d1d-f93372bc8235"], "metadata": {"filename": "docs\\community\\integrations.md", "author": "LlamaIndex"}}, "37f793377e680750a22fe70a490a7f17f741b9ba": {"node_ids": ["1e6e7b26-278d-414d-a59e-556fbe6e211f", "afa369ba-648a-4758-803a-1386ec8dfff9"], "metadata": {"filename": "docs\\community\\integrations\\chatgpt_plugins.md", "author": "LlamaIndex"}}, "77dce2cb9834e28bbcfe2f1a4a4ff4cd81bd7993": {"node_ids": ["5bf8d116-1a65-47ea-b92c-dea5bee72bf3", "b025c8a5-eb7a-481d-9fed-688ab2e976be"], "metadata": {"filename": "docs\\community\\integrations\\deepeval.md", "author": "LlamaIndex"}}, "9587d18ae8acb48c4713cd3148ef022248288bf4": {"node_ids": ["30e455b0-1e7b-43d2-a247-65f89200d59b"], "metadata": {"filename": "docs\\community\\integrations\\graph_stores.md", "author": "LlamaIndex"}}, "1d37221ea4421e3af53adf48114a4bcd231d5375": {"node_ids": ["5ebcf58d-2516-4f5e-8d00-2cb8d086edaa"], "metadata": {"filename": "docs\\community\\integrations\\graphsignal.md", "author": "LlamaIndex"}}, "ce73c9b78c31e9ee169fe6fa976142f455793da9": {"node_ids": ["8122e965-68d2-4c8c-be75-f230747c599e", "cce95214-17e7-4280-9ff7-f0339a6cc7b1"], "metadata": {"filename": "docs\\community\\integrations\\guidance.md", "author": "LlamaIndex"}}, "6b418ac328d3bbc3276a01383850e35961589c7a": {"node_ids": ["e2e0ae66-1caa-4ffb-b490-0e086abef319"], "metadata": {"filename": "docs\\community\\integrations\\managed_indices.md", "author": "LlamaIndex"}}, "b7727424611f613fda805d0a0f3ed0be0566f5c6": {"node_ids": ["3b4a69ca-9f98-4e7e-8444-82b567e8452d"], "metadata": {"filename": "docs\\community\\integrations\\trulens.md", "author": "LlamaIndex"}}, "ddbbfe258246bdaf76b336208d662ffcc94029fd": {"node_ids": ["219e3285-8514-4d57-aabc-b9be90bcee77"], "metadata": {"filename": "docs\\community\\integrations\\using_with_langchain.md", "author": "LlamaIndex"}}, "e592684371cf1fa151073a9305ce733372ac6ea0": {"node_ids": ["f426cf7b-db7b-4b36-970b-111415f503fb", "c0db3248-daff-412a-995f-9c26aee4eeb7", "811a1f10-f697-4c37-9591-b6f8b87a9e11", "9f280e4b-20e3-47bf-bef0-cff81dd64831", "474c71b0-ce49-4b88-85a4-e1ec94ce3738", "f8101c5f-71f6-4812-9e3b-6547502b5211", "f7597301-8057-424e-9d46-f2e59157cd91", "f2053e36-16d6-4f9e-bd97-ee8e5e4faad9"], "metadata": {"filename": "docs\\community\\integrations\\vector_stores.md", "author": "LlamaIndex"}}, "b260ba3a258f903d4d49ce82b595252218bb9d16": {"node_ids": ["9f9535a8-5be2-48d7-bcfa-2345258ee81a"], "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\modules.md", "author": "LlamaIndex"}}, "9e34b8d5855a000ee45bf3ecb04d36acc9ab8370": {"node_ids": ["bd56e86b-54cc-4e3a-baa0-5e37c98227aa"], "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\root.md", "author": "LlamaIndex"}}, "c2e68fd0c5731804f83484ac6beeca31d139f005": {"node_ids": ["01e3977e-f6d5-45a2-b686-b5130b963879", "c8702fc4-96f5-4890-800b-41eab8d9e957"], "metadata": {"filename": "docs\\core_modules\\agent_modules\\agents\\usage_pattern.md", "author": "LlamaIndex"}}, "eac4592e14222ac5b22febaf8c9678cb1592da15": {"node_ids": ["1c9e7a9f-0ee3-4670-9074-978616b35410"], "metadata": {"filename": "docs\\core_modules\\agent_modules\\tools\\llamahub_tools_guide.md", "author": "LlamaIndex"}}, "f86aa28d06ab040ca6f5a1a5d12361cbcd9dbf32": {"node_ids": ["6f9c0fbd-5a60-4de4-8edc-b0c70177461d"], "metadata": {"filename": "docs\\core_modules\\agent_modules\\tools\\root.md", "author": "LlamaIndex"}}, "146d9440fa4554141c53de93868ec5fece9810a5": {"node_ids": ["7c2aec87-a8df-4e9a-a304-1e62a164efce"], "metadata": {"filename": "docs\\core_modules\\agent_modules\\tools\\usage_pattern.md", "author": "LlamaIndex"}}, "b5e2d1fa6c9b797a40ef3c4f19110a50dba0aa78": {"node_ids": ["e52f36a5-f33b-43c1-9cb8-bb1b85b5911b"], "metadata": {"filename": "docs\\core_modules\\data_modules\\connector\\modules.md", "author": "LlamaIndex"}}, "ceed552efc04544ff10ba9f5d5b178c2541e5d47": {"node_ids": ["7dea053a-d622-4c32-b6ba-eb1b8c64a10f"], "metadata": {"filename": "docs\\core_modules\\data_modules\\connector\\root.md", "author": "LlamaIndex"}}, "71447be69982fa3b15d4372930dc3e91a19047af": {"node_ids": ["d73fccdc-8c0f-4b39-b308-1a16bb28b10c"], "metadata": {"filename": "docs\\core_modules\\data_modules\\connector\\usage_pattern.md", "author": "LlamaIndex"}}, "2e23d3959e82ba207ff9430d50b00606f09c38cf": {"node_ids": ["07ec6934-f93b-4bae-828e-38808b9e6bc6"], "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\root.md", "author": "LlamaIndex"}}, "e73764aa8a96c83aee87723d56ab097f3af40fd0": {"node_ids": ["67c83569-5531-4a11-b0cb-5d95f16b7450", "f9c4f6bb-62d7-42d6-8a92-5a1f035e0b70"], "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_documents.md", "author": "LlamaIndex"}}, "087ad205fe5bcd3338ced6aec685964837187258": {"node_ids": ["e22c2099-983c-4151-a2ef-b7e3ac87623a"], "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_metadata_extractor.md", "author": "LlamaIndex"}}, "aed5917630b9b7cb0db8a92e6978c52670a5dda4": {"node_ids": ["8dbd8b26-46bf-4ee8-866b-02e979cfea67"], "metadata": {"filename": "docs\\core_modules\\data_modules\\documents_and_nodes\\usage_nodes.md", "author": "LlamaIndex"}}, "c36dc0ed7769cb44337179d2daede7635a4b6211": {"node_ids": ["44715510-67b8-431c-9ad6-321b8cffc5d1", "25180ddc-2ed7-409a-abb1-82f600017635"], "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\composability.md", "author": "LlamaIndex"}}, "31d3a47c3db5e1bafe59905d24647bc931dcbae4": {"node_ids": ["6cd88956-15fe-496c-b768-9bdc4107c57c", "3a3b62b8-4da6-4dd5-ba96-338bbc9deb24"], "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\document_management.md", "author": "LlamaIndex"}}, "a1aa2285c19382d89e58d784594751d622683655": {"node_ids": ["46562a93-963b-4a61-ba5c-c5cf7876b8b2"], "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\index_guide.md", "author": "LlamaIndex"}}, "ee73fa5bd93f4243e01056b1de61b3008d468363": {"node_ids": ["27040026-fa6b-4c67-86b4-71232e5e56da", "506de4fd-7b68-4a6b-ab6e-151903c85672"], "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\metadata_extraction.md", "author": "LlamaIndex"}}, "19db959161964ef1f07c9d41b941fe5d7d4dfec4": {"node_ids": ["f9f1105f-8f5d-4942-a700-13f2bb1a6abb"], "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\modules.md", "author": "LlamaIndex"}}, "aa048880f3a151c73ee65e041ad052064a76d4c1": {"node_ids": ["b6977624-9a24-46bb-a840-4d6132bdfe80"], "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\root.md", "author": "LlamaIndex"}}, "b387014631ce191702efc3c3fb9caef3c8272025": {"node_ids": ["735bc8a0-d89a-422d-8b09-53b423ad58fd"], "metadata": {"filename": "docs\\core_modules\\data_modules\\index\\usage_pattern.md", "author": "LlamaIndex"}}, "b3b3a1fd1da1034beecfcefaaba3657a21c7ab71": {"node_ids": ["bfb19442-7092-4b42-a9a7-9c064aa615f8"], "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\root.md", "author": "LlamaIndex"}}, "959043b9ea13ce947637c8838704ba205ea07dfc": {"node_ids": ["e8f9b733-7c89-4eca-b99d-9a07fcf20152", "1308b8dc-b079-497f-b853-0b147528dfd2"], "metadata": {"filename": "docs\\core_modules\\data_modules\\node_parsers\\usage_pattern.md", "author": "LlamaIndex"}}, "9018b3ef050474178de0ce352ac24e9a4ad25394": {"node_ids": ["811b2cb7-db86-4a14-a8a9-46b0a494c161", "c1162b51-89f2-4f63-947a-8e53179cea32"], "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\customization.md", "author": "LlamaIndex"}}, "5029286744cf4d84add453002b699319fcfe1eba": {"node_ids": ["d8f9b13b-6791-42b3-bd6e-b000d8b06da2", "9b96a9ee-9180-4349-a49c-01882c8c2379"], "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\docstores.md", "author": "LlamaIndex"}}, "9a09614387a4f717b34205dfe0248c46bc13580f": {"node_ids": ["7fcb8820-8880-4da2-a3a4-89c1899defc9"], "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\index_stores.md", "author": "LlamaIndex"}}, "5e03ed535bc23260d16c06aa9f38ebfa5a41f719": {"node_ids": ["d82c4727-508f-414e-bd64-5a5e6ee875c7"], "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\kv_stores.md", "author": "LlamaIndex"}}, "b47e16f4293839f02416d7fb9158cc92ab030f7f": {"node_ids": ["5576d7f8-447c-4cbb-84c3-314a78d44f8f"], "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\root.md", "author": "LlamaIndex"}}, "426fce42ccb2d16c3128dd7e64e3e5cecb989028": {"node_ids": ["e48b0d18-726d-471d-9702-0a63215a1d2e", "7acbc142-6cba-4d56-8100-21465098753a"], "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\save_load.md", "author": "LlamaIndex"}}, "19d595ddd7cebd27fdcb8993e483cc23affad291": {"node_ids": ["626a612e-5a29-45d3-a4e0-bf4e7c89b2d5", "f73077f6-5d32-4c13-af90-2452491bcd54", "4dab9719-d28e-4d95-a862-eb6e16b925c7", "d8206bbc-6e15-4345-964f-0db151da7a97"], "metadata": {"filename": "docs\\core_modules\\data_modules\\storage\\vector_stores.md", "author": "LlamaIndex"}}, "28292083a2785e839264533d778b316bba73207a": {"node_ids": ["e0caea2b-f168-4262-9b95-224c6b8996b2"], "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\modules.md", "author": "LlamaIndex"}}, "fc2c971016bc70e8e1399bdcb84e7aee7dea5b84": {"node_ids": ["c2b9f3c1-0183-4266-8ee0-e3b421a1e7f0"], "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\root.md", "author": "LlamaIndex"}}, "ae8128392f38a8f48feacb5048750e968617c647": {"node_ids": ["7d0f937c-a29c-4a7c-9ef3-682d8123adab", "38868d0e-6a25-4e21-a37c-139717704171"], "metadata": {"filename": "docs\\core_modules\\model_modules\\embeddings\\usage_pattern.md", "author": "LlamaIndex"}}, "60006b15efd1cdbb60bc4a787f1cb859931fb1b0": {"node_ids": ["a9858431-5d39-4132-8408-d13cfcd97201"], "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\modules.md", "author": "LlamaIndex"}}, "cacbec8e223592c7809ce4ab3c2bbaac75ad1c4b": {"node_ids": ["985acb4b-717c-4e17-b03f-4cad246da1c7"], "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\root.md", "author": "LlamaIndex"}}, "1b905380f49f65854b7375f1eba5576f6c82785a": {"node_ids": ["a612f8cc-967f-4289-bd5c-b1fd178bcd40", "28c994a0-580e-4746-a12e-d5ac10b1f26f", "caa6858d-0b2d-462a-9e7d-a6b5ddfdbf0c", "5a2d46f6-4ddf-48ce-b498-1b0d24cc2cb2"], "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_custom.md", "author": "LlamaIndex"}}, "3beb398c11263f63cc6e311727ca48d9a658e036": {"node_ids": ["bccebf27-b755-4da4-9864-3e2098e6da3e"], "metadata": {"filename": "docs\\core_modules\\model_modules\\llms\\usage_standalone.md", "author": "LlamaIndex"}}, "1d078d2a781feb5172a10b102a6b0740cc9dd121": {"node_ids": ["ac8b9270-ffc5-4520-b9d2-5b2834a8b691", "05d8bf6c-6183-4751-8a8a-1ae83a70a652"], "metadata": {"filename": "docs\\core_modules\\model_modules\\prompts.md", "author": "LlamaIndex"}}, "ea0eab0ae33fbaf69ee1c99f6b7cc09ae090a277": {"node_ids": ["dccd2d0b-197d-4871-9aaf-f42ea6d4c119"], "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\modules.md", "author": "LlamaIndex"}}, "26371396dff491ae107ecac5d03730c7ddcf78b5": {"node_ids": ["40f8c65b-652a-434b-83d5-d413348c015e"], "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\root.md", "author": "LlamaIndex"}}, "883a57fc7e0241f919fb6e3213d1ba4571f61fe3": {"node_ids": ["61aeaf9c-7e1b-4638-9dbb-aa23f87aefb4", "10dfb162-591a-4c8a-af2b-ad51ac5635a4"], "metadata": {"filename": "docs\\core_modules\\query_modules\\chat_engines\\usage_pattern.md", "author": "LlamaIndex"}}, "dc633e6e45e519b3fb15f4ac3c40e799d3ae70a2": {"node_ids": ["f9b0a12b-11c9-4a58-ae7f-31b9e2a8c3a9", "330c65bc-2b0e-4d90-89f0-e8c59f1a0b2e", "030b0cd8-541b-47e7-befb-69aaeeb69fd0"], "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\modules.md", "author": "LlamaIndex"}}, "aa77fb97241f1e45d8c60e1776ce303765269714": {"node_ids": ["76b26bd0-4bde-4854-b485-e097abae0a0f"], "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\root.md", "author": "LlamaIndex"}}, "ecaaca47b8df50ccde682547956edfdda32b99f2": {"node_ids": ["2551cd80-a535-4dec-8f2c-50095a3dfa39"], "metadata": {"filename": "docs\\core_modules\\query_modules\\node_postprocessors\\usage_pattern.md", "author": "LlamaIndex"}}, "9c818be1d3a7d8d18fe7cbbdbc93f5eaa92679eb": {"node_ids": ["d8920524-7a92-4572-9c2a-b8a3e37a3970", "2fd9c628-f1f2-4c0c-8f1f-5acc1000985b"], "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\advanced\\query_transformations.md", "author": "LlamaIndex"}}, "533eeabd853e4c0b1ba266ac25fe1f9f3240debe": {"node_ids": ["5be3f2b3-b666-4d89-afe9-9308b99dddfa"], "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\modules.md", "author": "LlamaIndex"}}, "ebf561d9ceed54572c4415becf9b77af17470a70": {"node_ids": ["8e27449d-52ac-41cb-ba21-9dbf8b23ee0a"], "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\response_modes.md", "author": "LlamaIndex"}}, "60aee9e1672669b7ccdcf974baa3507749fbaf2e": {"node_ids": ["53d695ce-fc96-4ce3-9f81-a6f4bc42c7d9"], "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\root.md", "author": "LlamaIndex"}}, "0ed385b8ae778071d6879ced4f97763ab2192994": {"node_ids": ["60e2a7b0-3822-4db0-b36f-fe4c5f79749f"], "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\streaming.md", "author": "LlamaIndex"}}, "8b2c65800dbf3b0098c8a42d7df7f1bca587259c": {"node_ids": ["2f005cd4-360b-4e92-8a7a-7638c571428c"], "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\supporting_modules.md", "author": "LlamaIndex"}}, "d66e54f11c8266c84b58d0370639a3e9458b9fb9": {"node_ids": ["f848b2dc-c545-438a-aac6-2da67a02ad7d"], "metadata": {"filename": "docs\\core_modules\\query_modules\\query_engine\\usage_pattern.md", "author": "LlamaIndex"}}, "ad7717012c6c9751b3d5eecefd6b0bb5cbb57fe7": {"node_ids": ["eb6b3f75-8e8f-4b4d-8ea7-405bad15aa42"], "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\modules.md", "author": "LlamaIndex"}}, "cdd5a30d575666616ff440c5be5c77c58aac858a": {"node_ids": ["dc78f6aa-4eb3-4885-b984-635e33d81e1a"], "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\root.md", "author": "LlamaIndex"}}, "4b6b6268a452a67680cd9a4516eaba7ac3e85f46": {"node_ids": ["b35c455a-9d46-4ade-8432-722997229b01", "1f0e9a72-a9c4-468b-8f4f-7104f003068e"], "metadata": {"filename": "docs\\core_modules\\query_modules\\response_synthesizers\\usage_pattern.md", "author": "LlamaIndex"}}, "6324fe6c4228f812b6918760fecbd7d5362c7ad9": {"node_ids": ["2169d002-ca5c-476c-8df3-eb9e39c74eaf"], "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\modules.md", "author": "LlamaIndex"}}, "61fc10afcd33f050116c937e56db5d97add4abfa": {"node_ids": ["5ba2a608-17fa-4791-89f8-23f07a77bc3d"], "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\retriever_modes.md", "author": "LlamaIndex"}}, "922638c6d1f38d3de450d8af8edf0e17a51b0c35": {"node_ids": ["d10b62f5-9f13-484e-8f54-bb8ccaff358a"], "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\root.md", "author": "LlamaIndex"}}, "30c07b32076ea2869a8752ed3160786bca681845": {"node_ids": ["0c2a9fc8-fcd4-4543-ab26-deee440d6f69"], "metadata": {"filename": "docs\\core_modules\\query_modules\\retriever\\usage_pattern.md", "author": "LlamaIndex"}}, "1a267352c1477cc2f1a3ed8097dfed96bda38964": {"node_ids": ["8eee734c-38a3-417c-b234-8cbb9df1b1aa"], "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\modules.md", "author": "LlamaIndex"}}, "a2f004252113ef51ed6070e8129422a4ebbbbc4d": {"node_ids": ["701411f3-d6cf-472f-9044-21e5e86f5f4a"], "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\root.md", "author": "LlamaIndex"}}, "679bed9d3c4ad324f657f311e28c0eec3f8cd7fc": {"node_ids": ["c40fb52a-59ce-4300-93ed-e52bc1c0a18f", "fa87eaf5-9ccf-4d36-bb6e-6619ef22e5e2"], "metadata": {"filename": "docs\\core_modules\\query_modules\\router\\usage_pattern.md", "author": "LlamaIndex"}}, "f2ba5328b2570353c7b0860760a3b61c100e95b9": {"node_ids": ["0912f890-b478-4670-9e15-d994aac2cf44", "696f5357-db8c-4c4b-81f0-5259c91ce418"], "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\output_parser.md", "author": "LlamaIndex"}}, "a79ee9daf554d84400c742faa6c48580606f1860": {"node_ids": ["96540789-826e-40b9-ba7a-6137b1eae712"], "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\pydantic_program.md", "author": "LlamaIndex"}}, "a32026b5e77ab742f2fc15773420557e36820274": {"node_ids": ["6f88b8dc-f3a2-4c1a-bd4b-7b50716f06c6"], "metadata": {"filename": "docs\\core_modules\\query_modules\\structured_outputs\\root.md", "author": "LlamaIndex"}}, "d06e4f1fa8264cb1f94541623ab28f70a7224bbb": {"node_ids": ["1bbaca72-5f2b-4df7-9b82-2fbc645f8eb2"], "metadata": {"filename": "docs\\core_modules\\supporting_modules\\callbacks\\root.md", "author": "LlamaIndex"}}, "c92e40ce73179f4aae1b8ed918b805e3a585ffdd": {"node_ids": ["20d0b081-42d7-4dfc-890f-a3f22e25a4ca"], "metadata": {"filename": "docs\\core_modules\\supporting_modules\\callbacks\\token_counting_migration.md", "author": "LlamaIndex"}}, "2f23a92cc6de0d269bc901bee4a5e97b8891a9e5": {"node_ids": ["0696e8c1-c965-456c-a011-9ef3e297b71a", "96252df0-81a0-4037-bb4f-3fdc546ef84d"], "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\root.md", "author": "LlamaIndex"}}, "145c0d35c448a994b56de44800293d21f344c849": {"node_ids": ["c3288a8a-9ee9-4db9-b48b-1d5a7d091913"], "metadata": {"filename": "docs\\core_modules\\supporting_modules\\cost_analysis\\usage_pattern.md", "author": "LlamaIndex"}}, "7fe5150420891da3c4bef00958f002d61217ae1e": {"node_ids": ["948fa402-50f5-47b4-bbf9-cfb44fb9b649"], "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\modules.md", "author": "LlamaIndex"}}, "a196c68d6f3e5add99049b21f34ff4c0314eba73": {"node_ids": ["0dbc1fbc-a574-4dd3-b0a0-df32bd0218b0"], "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\root.md", "author": "LlamaIndex"}}, "3a6836444066c29794155c38d9b80271c4f86a6c": {"node_ids": ["127950f3-614b-4ab1-aa62-17af686c4039", "a495af7c-c87a-496b-a868-5b8f30953f42"], "metadata": {"filename": "docs\\core_modules\\supporting_modules\\evaluation\\usage_pattern.md", "author": "LlamaIndex"}}, "a13b8e777811d7031e201f14669e7caf0f6bb66f": {"node_ids": ["d75a6a73-65df-4a57-96bf-41bf1d77ab17"], "metadata": {"filename": "docs\\core_modules\\supporting_modules\\playground\\root.md", "author": "LlamaIndex"}}, "44dba472d4169443c3401d14c5631465eab0725f": {"node_ids": ["8c21c0d0-cf6b-4a03-80c6-66f9bb5c153b", "3e553d2d-28b3-4576-bd6a-e4833517ee4b"], "metadata": {"filename": "docs\\core_modules\\supporting_modules\\service_context.md", "author": "LlamaIndex"}}, "46bf7a7099a0456a4f30cb6bcb5ac0ce59bac8aa": {"node_ids": ["290972af-dd34-49b7-8c81-43d512b3d500"], "metadata": {"filename": "docs\\deprecated_terms.md", "author": "LlamaIndex"}}, "5777d9a1f01e0f708202499624b34da6c33300a5": {"node_ids": ["d9f52f29-8ac7-40e5-81e6-b4e1cf135561"], "metadata": {"filename": "docs\\development\\privacy.md", "author": "LlamaIndex"}}, "728ccebf2fce50b57bf06af7f576fcf7aaf61e19": {"node_ids": ["6558c03c-1f31-416b-9f95-f9a8e51c90f8", "30cc6acc-df1d-4249-be11-3c1e90cbc9a9"], "metadata": {"filename": "docs\\end_to_end_tutorials\\agents.md", "author": "LlamaIndex"}}, "61b1a0d2bcdf1ffef5187fde6c0e0ae4450e6db4": {"node_ids": ["d0afa067-1613-42a3-a67d-d440ad8901b2"], "metadata": {"filename": "docs\\end_to_end_tutorials\\apps.md", "author": "LlamaIndex"}}, "1ec44e4994c4535771b70c620e0ac2eb1a60662a": {"node_ids": ["e319874d-fc82-4b5a-a288-52af9c990a82", "81d03ce7-6627-45a7-8a9f-66374ec48eb0", "c27b9bc8-c421-4da3-8b0b-2a72343235a9", "4b7ca0e6-132d-4928-a648-c728ad3171a1", "afdaba8b-1eea-488d-b4da-ebab174cec70", "8c626d5e-a15a-43a8-be3f-18a0a556655a"], "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_app_guide.md", "author": "LlamaIndex"}}, "29b95163662843db2b80f3ab36f28a1d05ca76ea": {"node_ids": ["98988af5-7ed4-42ce-a24e-b2eae1327a55", "bfa861a8-eead-4df4-b4b9-0987c129ad3e", "31d74118-6633-45bd-939d-20eb4f24a622", "97ad579d-fce9-484d-8665-0c18c2151d28", "2bc3ad40-0112-49aa-8bf1-7aa42d89642f", "cd444059-4829-454b-84a7-5890a6c92a80", "b5736017-4998-4aea-ba88-7fcd6b544808", "cc232f0b-69fb-45af-bbc9-77e97ff5e84c", "45e5b4de-d494-4270-880b-1fa853e039ab", "49ce9594-5f43-407c-9e20-4366e340ff72", "6868a6e6-abd1-4a18-a7d4-0a8bc78aaa29", "824f712c-18a8-49e1-b746-94906a75383d"], "metadata": {"filename": "docs\\end_to_end_tutorials\\apps\\fullstack_with_delphic.md", "author": "LlamaIndex"}}, "3756835c7d1eaf6d708e55b30ffc2cb879bb429c": {"node_ids": ["d35927e0-0092-4e38-b844-9f1d050d2862"], "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots.md", "author": "LlamaIndex"}}, "13037c457040479ca229000a12561167ba21e892": {"node_ids": ["efa12611-2447-4d58-8a08-26a3704535b8", "8a8f7393-61ee-48f4-a416-8165f53882ac", "657efc36-df83-4e56-933f-89039a0395a5", "72a3bcd2-7945-444e-965b-c18a766f7a5d", "1d32a6ac-f244-4992-bd34-440a2182a532"], "metadata": {"filename": "docs\\end_to_end_tutorials\\chatbots\\building_a_chatbot.md", "author": "LlamaIndex"}}, "cd8f1ca277aa1815e930b0daa1b13b2f678cfe96": {"node_ids": ["01a3a473-0083-40fa-8a11-75f394861e2d"], "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\component_wise_evaluation.md", "author": "LlamaIndex"}}, "21b8dd44a82617ea07b8f295ca1d583d2541b8f1": {"node_ids": ["15707a2c-3f1e-4723-86fd-67091dcf67ba", "8c778fa6-168d-4457-a731-fcefc050a6d3"], "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\development_pathway.md", "author": "LlamaIndex"}}, "ebf0bea9e9892ed101bd9eb7114fe44a4d4679cb": {"node_ids": ["52e7f52d-519c-48fd-85e3-73a0d296b135"], "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\e2e_evaluation.md", "author": "LlamaIndex"}}, "0d29918512506aca78286a2e0b139a9fc88580df": {"node_ids": ["2fb6d4f0-f671-424e-8640-8accc6adab3b"], "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\evaluation.md", "author": "LlamaIndex"}}, "cfafd62b0e9f5cdb3eb121d03a96198ae64feff9": {"node_ids": ["96dca026-efdc-4ac7-90bf-906b643f1573"], "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\monitoring.md", "author": "LlamaIndex"}}, "d3554ad15f7802465c8587c648b3494e572ab167": {"node_ids": ["a27331a1-1bad-4ccd-9bd0-429465d80172"], "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\observability.md", "author": "LlamaIndex"}}, "d53d8b47de2c7a38bcb395a4c7e80275684dff56": {"node_ids": ["6dcacd63-f1ce-4905-a2b3-a367a12576da", "a0d2eca7-f20c-49eb-9cd7-6a80d576903c"], "metadata": {"filename": "docs\\end_to_end_tutorials\\dev_practices\\production_rag.md", "author": "LlamaIndex"}}, "b6249f3ec1980d3dbe7f502f0fb462939dfd3e88": {"node_ids": ["3b1f1bc5-e57a-4694-b624-7aa73d18f9d9"], "metadata": {"filename": "docs\\end_to_end_tutorials\\discover_llamaindex.md", "author": "LlamaIndex"}}, "ea304e8e84d3da5200044d14a761bd396d69d0b7": {"node_ids": ["9d950bef-82ba-44c8-8f18-975040b880f2", "6e0f2a6d-1eb9-4f30-b31b-55ae0e800e41", "f1bc3745-7815-4391-8174-7fc8d283a3b4"], "metadata": {"filename": "docs\\end_to_end_tutorials\\finetuning.md", "author": "LlamaIndex"}}, "acf49db9f42ba4cd549ae928e76ce7cb58798434": {"node_ids": ["a607c6b3-397e-4db9-b6ae-e7abd7a7dcca"], "metadata": {"filename": "docs\\end_to_end_tutorials\\graphs.md", "author": "LlamaIndex"}}, "f5fcc24cbf7d36cf038767bf3b95393e7aa48a80": {"node_ids": ["2ccd8336-2484-4eae-97b2-12f9eae6c1e6", "ee52c155-2606-41fe-9846-bfd036b7b5e2", "d1231944-90c4-4c97-b5d7-fde7e74ccc21"], "metadata": {"filename": "docs\\end_to_end_tutorials\\one_click_observability.md", "author": "LlamaIndex"}}, "a72939a6dbb1b7596cc1a485eba0eca5403d8664": {"node_ids": ["45c9d7ae-cd4f-4588-b588-fdfee152552e"], "metadata": {"filename": "docs\\end_to_end_tutorials\\principled_dev_practices.md", "author": "LlamaIndex"}}, "20745498e5bc3c98851326b15f795894aa05d49b": {"node_ids": ["2b2b1191-5427-4142-af6c-010b7c64bcbc"], "metadata": {"filename": "docs\\end_to_end_tutorials\\privacy.md", "author": "LlamaIndex"}}, "f13a08b36ae7d2805ad45b265730dd3fdea1243c": {"node_ids": ["7a0954fe-8e8c-402f-b89c-ebb4b163df9e", "8c1168ba-de1c-4147-a569-fbae40ef8a50", "7b5e823d-e520-446c-962d-0235c8a2c5c5", "6b920c82-667e-473f-a2c0-2b881870a3f2"], "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer.md", "author": "LlamaIndex"}}, "9113ec251c368aadd2aeee6ca8c23dbdc02817c3": {"node_ids": ["64db79af-099c-48d7-b5b9-85fd7709df64", "258808ef-4ff1-4f26-a477-640304ad78bb", "5772201a-3ccc-4a5d-acaf-a9eb0639fe43", "a7854179-f654-4452-bee0-2e6374805dfc", "c536587f-791c-4772-8240-4ba228f5940c", "b3d9f8c2-cbbc-48cc-9629-8c91b5fdb8bd", "c0afb204-4477-4656-aaf8-694298244255", "de9ae151-e2de-44ac-b01a-b5e2646da0cb", "b8cc04a2-dcdc-4906-93a6-5680da2419b9", "2e13ebfd-5b4e-4cf0-ada8-90a14f3b2460"], "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\terms_definitions_tutorial.md", "author": "LlamaIndex"}}, "4cbd683f9affb0df508057a7233bae3a8bffec39": {"node_ids": ["85dcd5ea-0c20-4794-b57b-921ea5d26d07", "dc9241f9-6eab-4ef3-9b1e-8d8b848793b1", "cc31c75a-073c-454c-afa5-386cca33651b"], "metadata": {"filename": "docs\\end_to_end_tutorials\\question_and_answer\\unified_query.md", "author": "LlamaIndex"}}, "eceb8a6cb9b239b8ee4510c7381e95dd00442ca1": {"node_ids": ["cfb0fbc0-d960-4800-9444-02cb6807cec8"], "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data.md", "author": "LlamaIndex"}}, "985d3202c070a65b9b73f15defa8ce3e9f8185e5": {"node_ids": ["32cff3f3-52e3-49c6-aed1-ef7cf3e01a80", "e752d1df-66ca-49a5-a082-c5c5924c6bde"], "metadata": {"filename": "docs\\end_to_end_tutorials\\structured_data\\sql_guide.md", "author": "LlamaIndex"}}, "76970e90ac64f5d1caf2abc934d1f3fe5246213d": {"node_ids": ["5dadfaa5-7c77-47b0-8095-e55f75b47de2", "f3cec787-bb9f-4a16-b5c1-8f2b60e67e51", "cd8c80cc-4f13-4cd6-bce3-e120fd8e252e", "8fd22a72-b9d0-44e8-a82d-daad4ee4d958", "5d5471e1-ed95-4deb-83ec-eb06ae3147bf"], "metadata": {"filename": "docs\\end_to_end_tutorials\\usage_pattern.md", "author": "LlamaIndex"}}, "80cb7a9fc2697667f34d5456b06fa5642394affe": {"node_ids": ["e43348b4-496d-466c-ab26-1b0027fb8ede"], "metadata": {"filename": "docs\\end_to_end_tutorials\\use_cases.md", "author": "LlamaIndex"}}, "a7608fa06109bba4ab1e5ba7ec192deac6eff023": {"node_ids": ["c586ca61-5735-49bc-ac31-dbc9677f884a"], "metadata": {"filename": "docs\\examples\\data_connectors\\README.md", "author": "LlamaIndex"}}, "32cce9075d6bdc38f5389e97b109b8163e31b622": {"node_ids": ["5f685536-34c9-4246-b8de-d9babb267b3c"], "metadata": {"filename": "docs\\getting_started\\FAQ.md", "author": "LlamaIndex"}}, "417c7df2531ebb4b15a98589c62a77a1b2baab70": {"node_ids": ["5863ca78-b666-457d-ac16-851c2e683cc9", "36c049ca-bfa2-4835-9c50-b1e5f333e759", "41bc4137-2d62-416e-9f3d-c92b75b40965"], "metadata": {"filename": "docs\\getting_started\\concepts.md", "author": "LlamaIndex"}}, "fa38de4744f41b9080a3dc7bcbcea59b5d78862e": {"node_ids": ["4a242449-8afc-476b-b24b-71b5aadb4cfe"], "metadata": {"filename": "docs\\getting_started\\installation.md", "author": "LlamaIndex"}}, "b4de173267fcf10f8781c8c28c618365c3d03b0d": {"node_ids": ["1348302c-f1a8-4312-90db-958594383c5a", "f1aaf379-77fe-4e70-adbe-adc62ec58f48"], "metadata": {"filename": "docs\\getting_started\\starter_example.md", "author": "LlamaIndex"}}}, "docstore/metadata": {"4437d6b6-ead6-4a1d-9251-df93995d95b8": {"doc_hash": "54416aac34b61b6621f1454d655bdb090c5c581815eb503601402e6383d0a7aa", "ref_doc_id": "4a7df91f0acf9a51e5f123e147fd874bac68c34e"}, "e6af2f51-3bf0-45b1-8297-620133b65139": {"doc_hash": "62d895fcae9e4d60968a63ed62b2f00bf5abf3e4599896874c52774e70e82e90", "ref_doc_id": "5cac394d1eca7c5b0b16ee0b4d7e69393162fa47"}, "e75ac405-364a-43a9-b9c3-95e2bf6f4185": {"doc_hash": "26a738b98bc633850a7729bdc68df43ec587206bacad1291e3aff4b78c058470", "ref_doc_id": "5cac394d1eca7c5b0b16ee0b4d7e69393162fa47"}, "679f0feb-a0e4-450e-9d1d-f93372bc8235": {"doc_hash": "a1054e802779c7c929f71f93080ad19647cbdc66580ef7d891ed14bed02ac019", "ref_doc_id": "e72ada566eb09c9ef1cf9fbf1019e9b5c166bcc8"}, "1e6e7b26-278d-414d-a59e-556fbe6e211f": {"doc_hash": "2982d52aabc142e93f025dd11666ac514edc50a20e73581b4956013392d6f1d4", "ref_doc_id": "37f793377e680750a22fe70a490a7f17f741b9ba"}, "afa369ba-648a-4758-803a-1386ec8dfff9": {"doc_hash": "2b3d84dc959562a13212748500bb80d6d93108172cd7ce6f010c1e9376406122", "ref_doc_id": "37f793377e680750a22fe70a490a7f17f741b9ba"}, "5bf8d116-1a65-47ea-b92c-dea5bee72bf3": {"doc_hash": "0f673f6ceb9420d97a2ec06aeddff48b4cd0f87ba95963a9cfdbdde4d0a2a87b", "ref_doc_id": "77dce2cb9834e28bbcfe2f1a4a4ff4cd81bd7993"}, "b025c8a5-eb7a-481d-9fed-688ab2e976be": {"doc_hash": "9f32801c51014e7f3215590820ee025df9c6c8571479a4aa15a18b1fb5f9a29a", "ref_doc_id": "77dce2cb9834e28bbcfe2f1a4a4ff4cd81bd7993"}, "30e455b0-1e7b-43d2-a247-65f89200d59b": {"doc_hash": "d2da005c5363afa2d7806914575af44dd75ff3ec57bd165cbb952692356b97f4", "ref_doc_id": "9587d18ae8acb48c4713cd3148ef022248288bf4"}, "5ebcf58d-2516-4f5e-8d00-2cb8d086edaa": {"doc_hash": "730cb957c364bbc2bc43d5731db74b325962d15fec1204edce402cb82ec562ee", "ref_doc_id": "1d37221ea4421e3af53adf48114a4bcd231d5375"}, "8122e965-68d2-4c8c-be75-f230747c599e": {"doc_hash": "d0b7e22be1b1eac643edc2304ec62e010f3ee77d4ae2ed4456bb4290416f2efe", "ref_doc_id": "ce73c9b78c31e9ee169fe6fa976142f455793da9"}, "cce95214-17e7-4280-9ff7-f0339a6cc7b1": {"doc_hash": "3fd45a05a63f8d921b620bb65dbc0c8e0f04d1281d2d09db6b11f1fe333813c9", "ref_doc_id": "ce73c9b78c31e9ee169fe6fa976142f455793da9"}, "e2e0ae66-1caa-4ffb-b490-0e086abef319": {"doc_hash": "9e6a1e4bd7304b644c8579701d17fe732434551765cc5ed4c8ea91940e0f1de7", "ref_doc_id": "6b418ac328d3bbc3276a01383850e35961589c7a"}, "3b4a69ca-9f98-4e7e-8444-82b567e8452d": {"doc_hash": "5cd3a265b1b1cf20d11c0a42065882f932b88ea631610a113f20a1f7d4f4333d", "ref_doc_id": "b7727424611f613fda805d0a0f3ed0be0566f5c6"}, "219e3285-8514-4d57-aabc-b9be90bcee77": {"doc_hash": "c19c07143c8cf3611b1d3d88bf9b8af72950d08cb19aa72ce03f5bdbc5370e6d", "ref_doc_id": "ddbbfe258246bdaf76b336208d662ffcc94029fd"}, "f426cf7b-db7b-4b36-970b-111415f503fb": {"doc_hash": "885033109dfd3032b5b8a1300291101e790b618106e60b32efe17daa8416bc86", "ref_doc_id": "e592684371cf1fa151073a9305ce733372ac6ea0"}, "c0db3248-daff-412a-995f-9c26aee4eeb7": {"doc_hash": "f0e1ccf3d420ef3c462b43de57cda1a7c36eb386c3f02ecd4c83e094985f15c2", "ref_doc_id": "e592684371cf1fa151073a9305ce733372ac6ea0"}, "811a1f10-f697-4c37-9591-b6f8b87a9e11": {"doc_hash": "4ee4961f638ce57e0f16d3899818ac67f897a8ab9972eca577457bacb00dccee", "ref_doc_id": "e592684371cf1fa151073a9305ce733372ac6ea0"}, "9f280e4b-20e3-47bf-bef0-cff81dd64831": {"doc_hash": "fc317673a98bc847f750feef8ed8e7d9b17b300d0ba6e625920d8aaa94310b33", "ref_doc_id": "e592684371cf1fa151073a9305ce733372ac6ea0"}, "474c71b0-ce49-4b88-85a4-e1ec94ce3738": {"doc_hash": "45ff97913ead16fa1f3b7f9b7b108dc722d24f931f11d948cb939827fcbc3da9", "ref_doc_id": "e592684371cf1fa151073a9305ce733372ac6ea0"}, "f8101c5f-71f6-4812-9e3b-6547502b5211": {"doc_hash": "fad7677c8d95fa28a0293e09ca4ed6e5550087feb045122f1e01bce0aabdfd80", "ref_doc_id": "e592684371cf1fa151073a9305ce733372ac6ea0"}, "f7597301-8057-424e-9d46-f2e59157cd91": {"doc_hash": "23210c17f3bd8c5528a762121fcff6ff063f369dc86eb4d50b5abdc94d5b58a4", "ref_doc_id": "e592684371cf1fa151073a9305ce733372ac6ea0"}, "f2053e36-16d6-4f9e-bd97-ee8e5e4faad9": {"doc_hash": "096525cb79940d6921d11c57c61bb00f41e2e41ae08798040c789aef8e715303", "ref_doc_id": "e592684371cf1fa151073a9305ce733372ac6ea0"}, "9f9535a8-5be2-48d7-bcfa-2345258ee81a": {"doc_hash": "5dbe1cd99d65a6a770b0ec54584f301df281694216dc73b668cbf9a740f24626", "ref_doc_id": "b260ba3a258f903d4d49ce82b595252218bb9d16"}, "bd56e86b-54cc-4e3a-baa0-5e37c98227aa": {"doc_hash": "8925161b32ca12ccffd02e42add71896b58aade3250afbf7e1712a9fa46c2ebb", "ref_doc_id": "9e34b8d5855a000ee45bf3ecb04d36acc9ab8370"}, "01e3977e-f6d5-45a2-b686-b5130b963879": {"doc_hash": "b2fa3b9ac077048c55f2e0321583f988fcf2feb82512df360419847388421063", "ref_doc_id": "c2e68fd0c5731804f83484ac6beeca31d139f005"}, "c8702fc4-96f5-4890-800b-41eab8d9e957": {"doc_hash": "a1810318ef9a0bdc529a784545edf4477f55394605aaaf5c2e9d7d3f2216984e", "ref_doc_id": "c2e68fd0c5731804f83484ac6beeca31d139f005"}, "1c9e7a9f-0ee3-4670-9074-978616b35410": {"doc_hash": "3bdc78218e89fe88a2d40a45393e320c5917df8347adb6305e09a23baaeb48c2", "ref_doc_id": "eac4592e14222ac5b22febaf8c9678cb1592da15"}, "6f9c0fbd-5a60-4de4-8edc-b0c70177461d": {"doc_hash": "d475798bc4269673d4b628fa9893612023dca9e1c624551133f8e0edbe9b2465", "ref_doc_id": "f86aa28d06ab040ca6f5a1a5d12361cbcd9dbf32"}, "7c2aec87-a8df-4e9a-a304-1e62a164efce": {"doc_hash": "298caacdf9081e4594a3b5fdea61136b628bc2b4aada3526265af1284f267905", "ref_doc_id": "146d9440fa4554141c53de93868ec5fece9810a5"}, "e52f36a5-f33b-43c1-9cb8-bb1b85b5911b": {"doc_hash": "308b29ea8a798fedc91f6aae4aea92ee110d94f67fe78102c8193908320cd198", "ref_doc_id": "b5e2d1fa6c9b797a40ef3c4f19110a50dba0aa78"}, "7dea053a-d622-4c32-b6ba-eb1b8c64a10f": {"doc_hash": "cfbe73d33c197dd74725e1f8736e9adbff4971f6750f38a13d2cf738cc3a5896", "ref_doc_id": "ceed552efc04544ff10ba9f5d5b178c2541e5d47"}, "d73fccdc-8c0f-4b39-b308-1a16bb28b10c": {"doc_hash": "f772aff4118fa71966a3f267b7a286f0a8864daeebe5044bfdee22cff165c112", "ref_doc_id": "71447be69982fa3b15d4372930dc3e91a19047af"}, "07ec6934-f93b-4bae-828e-38808b9e6bc6": {"doc_hash": "88acb1de87361aceba1410426fe722e3c882c642c142cd9f5d0f44f0139829b9", "ref_doc_id": "2e23d3959e82ba207ff9430d50b00606f09c38cf"}, "67c83569-5531-4a11-b0cb-5d95f16b7450": {"doc_hash": "dd440f8383b1df4577f9d4068a2a593988eaa57f2d58a5a0e3a3d6a084563d62", "ref_doc_id": "e73764aa8a96c83aee87723d56ab097f3af40fd0"}, "f9c4f6bb-62d7-42d6-8a92-5a1f035e0b70": {"doc_hash": "56e933fc379a2659958376fb31d3c95b3e184c0ab5e2bd503ed583c64f40d91e", "ref_doc_id": "e73764aa8a96c83aee87723d56ab097f3af40fd0"}, "e22c2099-983c-4151-a2ef-b7e3ac87623a": {"doc_hash": "d3b377d7aac1205a64dc5b54b3e4b9cf160e7bbb6e01fa913b9dce7d9a5b4264", "ref_doc_id": "087ad205fe5bcd3338ced6aec685964837187258"}, "8dbd8b26-46bf-4ee8-866b-02e979cfea67": {"doc_hash": "72969e377a5eaa1c1c8a8e177cc1a0f02b4c8cd38e2241d6ff91abb7ff7a61df", "ref_doc_id": "aed5917630b9b7cb0db8a92e6978c52670a5dda4"}, "44715510-67b8-431c-9ad6-321b8cffc5d1": {"doc_hash": "c87e991012490d16a5306a54d265a287e9e3153552e6ffacde1c136708d964ec", "ref_doc_id": "c36dc0ed7769cb44337179d2daede7635a4b6211"}, "25180ddc-2ed7-409a-abb1-82f600017635": {"doc_hash": "8a300ac07da7caea9e11b47cdf643613815f437d3138a97d5059e5463c8e2e6d", "ref_doc_id": "c36dc0ed7769cb44337179d2daede7635a4b6211"}, "6cd88956-15fe-496c-b768-9bdc4107c57c": {"doc_hash": "b6bc21791fd1f1fa23638579b1dd7fa035e31931e8742e5645fafde9c1663be9", "ref_doc_id": "31d3a47c3db5e1bafe59905d24647bc931dcbae4"}, "3a3b62b8-4da6-4dd5-ba96-338bbc9deb24": {"doc_hash": "7632c83e5026cdca36065cb6a93355d391e8853b85002e56917cc2c78884ed3c", "ref_doc_id": "31d3a47c3db5e1bafe59905d24647bc931dcbae4"}, "46562a93-963b-4a61-ba5c-c5cf7876b8b2": {"doc_hash": "93e0769d8cf319dbf0a4b2b7366d94ebbc286a8a32bdf61762e3dea18f18485e", "ref_doc_id": "a1aa2285c19382d89e58d784594751d622683655"}, "27040026-fa6b-4c67-86b4-71232e5e56da": {"doc_hash": "675fadc48f45221b65641e1912c44f1711d87d3841b7850b4c2bb2af3660a377", "ref_doc_id": "ee73fa5bd93f4243e01056b1de61b3008d468363"}, "506de4fd-7b68-4a6b-ab6e-151903c85672": {"doc_hash": "e97cb0b67cc4f5db4f684d7b97895fa8318f05668268afe44a0e5050c1b184ca", "ref_doc_id": "ee73fa5bd93f4243e01056b1de61b3008d468363"}, "f9f1105f-8f5d-4942-a700-13f2bb1a6abb": {"doc_hash": "998f7395a9c7f3908b06e2233a5edf2c3be259bfed0fb8e9e1cb1ae1abc40af1", "ref_doc_id": "19db959161964ef1f07c9d41b941fe5d7d4dfec4"}, "b6977624-9a24-46bb-a840-4d6132bdfe80": {"doc_hash": "99d18c1387735a31631f7d27a3ff2ac6e3648fede012e8c9cab30d541d1342f3", "ref_doc_id": "aa048880f3a151c73ee65e041ad052064a76d4c1"}, "735bc8a0-d89a-422d-8b09-53b423ad58fd": {"doc_hash": "15365a17d4b2d9edabc270353a7702faeabdee7fcace0d42eb9107ce4865336f", "ref_doc_id": "b387014631ce191702efc3c3fb9caef3c8272025"}, "bfb19442-7092-4b42-a9a7-9c064aa615f8": {"doc_hash": "00a75bc30d4be1e04ae16627eb3bc33b8aa0203e739595d5349d5b26aa14ff5c", "ref_doc_id": "b3b3a1fd1da1034beecfcefaaba3657a21c7ab71"}, "e8f9b733-7c89-4eca-b99d-9a07fcf20152": {"doc_hash": "7a133c105334ea3659ace9e2276e79e7b34c582acc5e6760058063f8f557c20f", "ref_doc_id": "959043b9ea13ce947637c8838704ba205ea07dfc"}, "1308b8dc-b079-497f-b853-0b147528dfd2": {"doc_hash": "491efdd4c28f0b684acfc283fcd117fa5920e15bce84611665b6470d3736c41a", "ref_doc_id": "959043b9ea13ce947637c8838704ba205ea07dfc"}, "811b2cb7-db86-4a14-a8a9-46b0a494c161": {"doc_hash": "8888a9ba121f361b339d1af0a31e2faf7ff13dc566cc83e0e6205839c68f9411", "ref_doc_id": "9018b3ef050474178de0ce352ac24e9a4ad25394"}, "c1162b51-89f2-4f63-947a-8e53179cea32": {"doc_hash": "1d691bbcfbf65e5c36644196bd6edc66b0276b7b18ef7242a31bb9add6d2f564", "ref_doc_id": "9018b3ef050474178de0ce352ac24e9a4ad25394"}, "d8f9b13b-6791-42b3-bd6e-b000d8b06da2": {"doc_hash": "57bea01a9062d5537a4004457ceba2a5213803372803fc499fe8a935fb07beda", "ref_doc_id": "5029286744cf4d84add453002b699319fcfe1eba"}, "9b96a9ee-9180-4349-a49c-01882c8c2379": {"doc_hash": "70259ccca05c88762fc860bd44e7fa4a17aa598364b47d83d8016603c30cc2d1", "ref_doc_id": "5029286744cf4d84add453002b699319fcfe1eba"}, "7fcb8820-8880-4da2-a3a4-89c1899defc9": {"doc_hash": "507fdc76bb2a59d946f6b27b1b8ec29fd9ffdb017eaf1f5542db26f0eae5098f", "ref_doc_id": "9a09614387a4f717b34205dfe0248c46bc13580f"}, "d82c4727-508f-414e-bd64-5a5e6ee875c7": {"doc_hash": "26c110b1cf86f50e665345421180f45f1294bee74b63c3c4867a0f7bdc7b6d39", "ref_doc_id": "5e03ed535bc23260d16c06aa9f38ebfa5a41f719"}, "5576d7f8-447c-4cbb-84c3-314a78d44f8f": {"doc_hash": "ced1facc3f0adba3e1c986d5aa1087f87d04435a01bebeb4bd2332bcc3a6077c", "ref_doc_id": "b47e16f4293839f02416d7fb9158cc92ab030f7f"}, "e48b0d18-726d-471d-9702-0a63215a1d2e": {"doc_hash": "f796c13402bc57fa63538c399bf42057966d4da112ecadaa59e7a251b083b279", "ref_doc_id": "426fce42ccb2d16c3128dd7e64e3e5cecb989028"}, "7acbc142-6cba-4d56-8100-21465098753a": {"doc_hash": "5ec48d018f2b45c1067463f413a7dd85b79582ae45c15946f8cb5d9e4b28003f", "ref_doc_id": "426fce42ccb2d16c3128dd7e64e3e5cecb989028"}, "626a612e-5a29-45d3-a4e0-bf4e7c89b2d5": {"doc_hash": "af87fdcc81c205688cbd83b403923fd1dfdd8f3c4e281f660bb2ab145d3461c7", "ref_doc_id": "19d595ddd7cebd27fdcb8993e483cc23affad291"}, "f73077f6-5d32-4c13-af90-2452491bcd54": {"doc_hash": "81c59a4a55cd357190215c931696226c117ca45b955e9bb13a3b115127e38bdb", "ref_doc_id": "19d595ddd7cebd27fdcb8993e483cc23affad291"}, "4dab9719-d28e-4d95-a862-eb6e16b925c7": {"doc_hash": "99a91306e77afa74ca17bc7ed130a09e2d7d2394c03262b1b84402ef164440c1", "ref_doc_id": "19d595ddd7cebd27fdcb8993e483cc23affad291"}, "d8206bbc-6e15-4345-964f-0db151da7a97": {"doc_hash": "f84674fc000606238dbb887f74bdd6c09b3fe720785d8217978f1e25afc5d5f0", "ref_doc_id": "19d595ddd7cebd27fdcb8993e483cc23affad291"}, "e0caea2b-f168-4262-9b95-224c6b8996b2": {"doc_hash": "7a2804805ff7d212e32db65ee720f036028f184b7447a906e2c7703a4f2f205b", "ref_doc_id": "28292083a2785e839264533d778b316bba73207a"}, "c2b9f3c1-0183-4266-8ee0-e3b421a1e7f0": {"doc_hash": "6cf44e6e934cfe00c52a3cef8fe485e30c578ebd2a64542dc56b4e57ef208126", "ref_doc_id": "fc2c971016bc70e8e1399bdcb84e7aee7dea5b84"}, "7d0f937c-a29c-4a7c-9ef3-682d8123adab": {"doc_hash": "a61af16a5b233eadc7a4c9cd1761a75fb24a115084fc064d4e0a33bf204bac8c", "ref_doc_id": "ae8128392f38a8f48feacb5048750e968617c647"}, "38868d0e-6a25-4e21-a37c-139717704171": {"doc_hash": "b35ee73e6ffaa3296908288636fd8d6acd5303619c9e7d93e60301b64cc327c1", "ref_doc_id": "ae8128392f38a8f48feacb5048750e968617c647"}, "a9858431-5d39-4132-8408-d13cfcd97201": {"doc_hash": "ed4716a2ac2d98378083356f1cf96a759d0948ff59e9e2f63dc2b9c8452c7176", "ref_doc_id": "60006b15efd1cdbb60bc4a787f1cb859931fb1b0"}, "985acb4b-717c-4e17-b03f-4cad246da1c7": {"doc_hash": "8c5d6b104512a2e8b22c67e7d334f6af126294c967036242dac1f4046b6a86fc", "ref_doc_id": "cacbec8e223592c7809ce4ab3c2bbaac75ad1c4b"}, "a612f8cc-967f-4289-bd5c-b1fd178bcd40": {"doc_hash": "e51eb91b538755e438e62eddac79408e2fdf873cabdda771cb5e66f775ce9708", "ref_doc_id": "1b905380f49f65854b7375f1eba5576f6c82785a"}, "28c994a0-580e-4746-a12e-d5ac10b1f26f": {"doc_hash": "63bd734407e4feb612a33a6b27a5e7e11dc2306dfa6c7074a2d0244202de71ff", "ref_doc_id": "1b905380f49f65854b7375f1eba5576f6c82785a"}, "caa6858d-0b2d-462a-9e7d-a6b5ddfdbf0c": {"doc_hash": "36005b0dc69bc595f46e851e623ddbe0cb71a1603b407b8dbd6e7ce01e0a78d2", "ref_doc_id": "1b905380f49f65854b7375f1eba5576f6c82785a"}, "5a2d46f6-4ddf-48ce-b498-1b0d24cc2cb2": {"doc_hash": "54dd80858c2fdbab27029223e55fc3a09811a416b18b6b548d365d09a8f0082f", "ref_doc_id": "1b905380f49f65854b7375f1eba5576f6c82785a"}, "bccebf27-b755-4da4-9864-3e2098e6da3e": {"doc_hash": "6eecf62895fd5634a121884fd10e106a61c1bec4e8dd2ab9e4d451c08d08f77d", "ref_doc_id": "3beb398c11263f63cc6e311727ca48d9a658e036"}, "ac8b9270-ffc5-4520-b9d2-5b2834a8b691": {"doc_hash": "43699fe770c7efe131be1c9a1b7cc16f2f5768b7d5ae0cc7b2c8bc52cbf6d770", "ref_doc_id": "1d078d2a781feb5172a10b102a6b0740cc9dd121"}, "05d8bf6c-6183-4751-8a8a-1ae83a70a652": {"doc_hash": "5de57d308268658a1385fac170764434a652febd6baea8e5174b7992e79d94ad", "ref_doc_id": "1d078d2a781feb5172a10b102a6b0740cc9dd121"}, "dccd2d0b-197d-4871-9aaf-f42ea6d4c119": {"doc_hash": "05132679a8476ec526232a27f888f40be9880ae0b572f0ee1a31949c1d4e163f", "ref_doc_id": "ea0eab0ae33fbaf69ee1c99f6b7cc09ae090a277"}, "40f8c65b-652a-434b-83d5-d413348c015e": {"doc_hash": "b405cdde2e788c3383d9e324d5e1d271ccb117482da6b7bcd6312f2469a2af26", "ref_doc_id": "26371396dff491ae107ecac5d03730c7ddcf78b5"}, "61aeaf9c-7e1b-4638-9dbb-aa23f87aefb4": {"doc_hash": "520b182b37b2c1b8eb619a2d295e2956503efde0704f67c9eeae420bcbd2e0db", "ref_doc_id": "883a57fc7e0241f919fb6e3213d1ba4571f61fe3"}, "10dfb162-591a-4c8a-af2b-ad51ac5635a4": {"doc_hash": "47bf355207cbabd0b10d40f16c176c9f559c72c7ebbffe711b8e2983dd6e2740", "ref_doc_id": "883a57fc7e0241f919fb6e3213d1ba4571f61fe3"}, "f9b0a12b-11c9-4a58-ae7f-31b9e2a8c3a9": {"doc_hash": "23c6e9d31205941dc11b13b40b9a1c074863d0054037adbf7cfcfd3e5b9c6edf", "ref_doc_id": "dc633e6e45e519b3fb15f4ac3c40e799d3ae70a2"}, "330c65bc-2b0e-4d90-89f0-e8c59f1a0b2e": {"doc_hash": "b8cd25d0b990064576c8506a936a379be48892c4a56f70fb0fb6d87aa8622cd0", "ref_doc_id": "dc633e6e45e519b3fb15f4ac3c40e799d3ae70a2"}, "030b0cd8-541b-47e7-befb-69aaeeb69fd0": {"doc_hash": "22eb2ce505e49e6684b5d40b20accf9e76470d018b2755fdfeb3ad8560c3306f", "ref_doc_id": "dc633e6e45e519b3fb15f4ac3c40e799d3ae70a2"}, "76b26bd0-4bde-4854-b485-e097abae0a0f": {"doc_hash": "fbc018df149f0210f37158a5203664398d0207e93b17f7a361a54e624b3eca61", "ref_doc_id": "aa77fb97241f1e45d8c60e1776ce303765269714"}, "2551cd80-a535-4dec-8f2c-50095a3dfa39": {"doc_hash": "e22203ff70aca1f74b1f536a1cfd826f87555a6aabf49f04e9c76baaa46bfca0", "ref_doc_id": "ecaaca47b8df50ccde682547956edfdda32b99f2"}, "d8920524-7a92-4572-9c2a-b8a3e37a3970": {"doc_hash": "3eb3fb35df81796fa750b925d79e813c4c77216770a220934125d9ef4c6c4a74", "ref_doc_id": "9c818be1d3a7d8d18fe7cbbdbc93f5eaa92679eb"}, "2fd9c628-f1f2-4c0c-8f1f-5acc1000985b": {"doc_hash": "ef0251e9e5acfedee5bf2cece1f7dbb16092f4818a747e841a009df4c3bada18", "ref_doc_id": "9c818be1d3a7d8d18fe7cbbdbc93f5eaa92679eb"}, "5be3f2b3-b666-4d89-afe9-9308b99dddfa": {"doc_hash": "84407cde45e84690ea192e1ac30d814814d1712be12fdd56b604bd750b398d6e", "ref_doc_id": "533eeabd853e4c0b1ba266ac25fe1f9f3240debe"}, "8e27449d-52ac-41cb-ba21-9dbf8b23ee0a": {"doc_hash": "c8af3febefaf135f9c690f56b8f498c38f8a9473a1d49a53cd794c47d74751f5", "ref_doc_id": "ebf561d9ceed54572c4415becf9b77af17470a70"}, "53d695ce-fc96-4ce3-9f81-a6f4bc42c7d9": {"doc_hash": "26b603751f91854004098523882b3807988e6f0f4b813fb240664f1b301191bf", "ref_doc_id": "60aee9e1672669b7ccdcf974baa3507749fbaf2e"}, "60e2a7b0-3822-4db0-b36f-fe4c5f79749f": {"doc_hash": "70e3a15099a6741cdda3473920cb507fef242b6abb2d7f13ee4da161dd358d62", "ref_doc_id": "0ed385b8ae778071d6879ced4f97763ab2192994"}, "2f005cd4-360b-4e92-8a7a-7638c571428c": {"doc_hash": "0958328ac44134b9f3fc5adaa54b200b2f580a578b646585b0eb9f30888783ad", "ref_doc_id": "8b2c65800dbf3b0098c8a42d7df7f1bca587259c"}, "f848b2dc-c545-438a-aac6-2da67a02ad7d": {"doc_hash": "063d6c2ecd2cdc25635ef0aaa1893fba9b231f609cc4fd7d78bee0c858166c98", "ref_doc_id": "d66e54f11c8266c84b58d0370639a3e9458b9fb9"}, "eb6b3f75-8e8f-4b4d-8ea7-405bad15aa42": {"doc_hash": "0eec079d4752edbe16c9106d7ebbd8613b0c1854a9f7bfd4baf999fb5adf8dd1", "ref_doc_id": "ad7717012c6c9751b3d5eecefd6b0bb5cbb57fe7"}, "dc78f6aa-4eb3-4885-b984-635e33d81e1a": {"doc_hash": "267021d7e6d90267d607995dfc499d1f2ed22c1af3a6f5b9d6b32c674f287d49", "ref_doc_id": "cdd5a30d575666616ff440c5be5c77c58aac858a"}, "b35c455a-9d46-4ade-8432-722997229b01": {"doc_hash": "5d3540b49390df54ecfe38ad52474daa71f7b0771cf0fdefbcf894b0a5302a8d", "ref_doc_id": "4b6b6268a452a67680cd9a4516eaba7ac3e85f46"}, "1f0e9a72-a9c4-468b-8f4f-7104f003068e": {"doc_hash": "b2824ec8c7523f8634c0e557df3945135b22db073b747264d6e33cd242dbaea2", "ref_doc_id": "4b6b6268a452a67680cd9a4516eaba7ac3e85f46"}, "2169d002-ca5c-476c-8df3-eb9e39c74eaf": {"doc_hash": "00cd478682974e67e1ec7865ddc39cd3682c6f53ae163f3ec48c87820bef3f74", "ref_doc_id": "6324fe6c4228f812b6918760fecbd7d5362c7ad9"}, "5ba2a608-17fa-4791-89f8-23f07a77bc3d": {"doc_hash": "28df6210c37f36259c9c7817162e2bf6547306e9cb18c7d0bb89b6c1ebb8dc9e", "ref_doc_id": "61fc10afcd33f050116c937e56db5d97add4abfa"}, "d10b62f5-9f13-484e-8f54-bb8ccaff358a": {"doc_hash": "ca333b5a81c37fbbf8ae2bea121b5627d166fb181e4d1c3543ca785c7e60bf4c", "ref_doc_id": "922638c6d1f38d3de450d8af8edf0e17a51b0c35"}, "0c2a9fc8-fcd4-4543-ab26-deee440d6f69": {"doc_hash": "35013f307cd7e0571ada923a2422ae7ecc728bedae82973970382160ba9110cc", "ref_doc_id": "30c07b32076ea2869a8752ed3160786bca681845"}, "8eee734c-38a3-417c-b234-8cbb9df1b1aa": {"doc_hash": "fd06dc89e5747f7393cec6fb03457a40c64acbb3c8cb83238fc547bc147b0772", "ref_doc_id": "1a267352c1477cc2f1a3ed8097dfed96bda38964"}, "701411f3-d6cf-472f-9044-21e5e86f5f4a": {"doc_hash": "b3b775ab87e45d49ff98aa1268c28c66fe8da302e590112a1274ad86d4fdc643", "ref_doc_id": "a2f004252113ef51ed6070e8129422a4ebbbbc4d"}, "c40fb52a-59ce-4300-93ed-e52bc1c0a18f": {"doc_hash": "4f9e69a0b6f1781b11740c8b3533093b1a9f5d2602bfe288b14324148322d431", "ref_doc_id": "679bed9d3c4ad324f657f311e28c0eec3f8cd7fc"}, "fa87eaf5-9ccf-4d36-bb6e-6619ef22e5e2": {"doc_hash": "78abba2e4a2e8d5c49588826a6526a1564fc22834e7add77c43bf0714e91ead4", "ref_doc_id": "679bed9d3c4ad324f657f311e28c0eec3f8cd7fc"}, "0912f890-b478-4670-9e15-d994aac2cf44": {"doc_hash": "3c8842d14f5da72f9cb5592e9b58c279ca7e491e38fb34bd65b2cce30d56dc15", "ref_doc_id": "f2ba5328b2570353c7b0860760a3b61c100e95b9"}, "696f5357-db8c-4c4b-81f0-5259c91ce418": {"doc_hash": "0e35dd7fabc39802f262683a5b9a560bfcd5c972cdde7904d3670bc94b75d6ae", "ref_doc_id": "f2ba5328b2570353c7b0860760a3b61c100e95b9"}, "96540789-826e-40b9-ba7a-6137b1eae712": {"doc_hash": "6a3b35c9c60d2f282072516069d12db036524d43887d42f89ef799d1ce7e10cf", "ref_doc_id": "a79ee9daf554d84400c742faa6c48580606f1860"}, "6f88b8dc-f3a2-4c1a-bd4b-7b50716f06c6": {"doc_hash": "ae32bb4aa38f57e981f58d1b93979d63fa15502899208c3536a9d0f5fbfbe6aa", "ref_doc_id": "a32026b5e77ab742f2fc15773420557e36820274"}, "1bbaca72-5f2b-4df7-9b82-2fbc645f8eb2": {"doc_hash": "3fa4f09b007b737cd1a6a809b2457441a3dc3edff65ffc7f3bebf7cce9c0f2da", "ref_doc_id": "d06e4f1fa8264cb1f94541623ab28f70a7224bbb"}, "20d0b081-42d7-4dfc-890f-a3f22e25a4ca": {"doc_hash": "142ce378bdc07312a2a3352b00e73313f1990a48f55fd420b6a8d97662f53c6b", "ref_doc_id": "c92e40ce73179f4aae1b8ed918b805e3a585ffdd"}, "0696e8c1-c965-456c-a011-9ef3e297b71a": {"doc_hash": "98ebcefcffdda14107d710ac3b028441d05a811adf6ecaee84d26176b2f09000", "ref_doc_id": "2f23a92cc6de0d269bc901bee4a5e97b8891a9e5"}, "96252df0-81a0-4037-bb4f-3fdc546ef84d": {"doc_hash": "5a7c5bd3f1a37a82cc05e2af423ecce6a26b34a9d4dc784c5703d5438a5deefe", "ref_doc_id": "2f23a92cc6de0d269bc901bee4a5e97b8891a9e5"}, "c3288a8a-9ee9-4db9-b48b-1d5a7d091913": {"doc_hash": "404a31719092827e17badea9673445eb2a29662ef5f6daafee8d5b4b23c5722c", "ref_doc_id": "145c0d35c448a994b56de44800293d21f344c849"}, "948fa402-50f5-47b4-bbf9-cfb44fb9b649": {"doc_hash": "2e7c203423eb24339587efba0b603233f6014cbfc4e527be04e201929d7f3460", "ref_doc_id": "7fe5150420891da3c4bef00958f002d61217ae1e"}, "0dbc1fbc-a574-4dd3-b0a0-df32bd0218b0": {"doc_hash": "e57b66a64e44aefdff1d96e1740807653dc898feb4aea3369e048a59d796796f", "ref_doc_id": "a196c68d6f3e5add99049b21f34ff4c0314eba73"}, "127950f3-614b-4ab1-aa62-17af686c4039": {"doc_hash": "135d25875613bd046e01e8d809b5c8506c7301f0ccd6ebea2a23af7e02c16c1c", "ref_doc_id": "3a6836444066c29794155c38d9b80271c4f86a6c"}, "a495af7c-c87a-496b-a868-5b8f30953f42": {"doc_hash": "13e9c0ec3f7cb21beb852f3ab1dac572bf9524ad655eaf4dea5e44339dc89da1", "ref_doc_id": "3a6836444066c29794155c38d9b80271c4f86a6c"}, "d75a6a73-65df-4a57-96bf-41bf1d77ab17": {"doc_hash": "c960715ccd733b8c265467ee42251f72d5a4211c6200367c97ea8f0720686f33", "ref_doc_id": "a13b8e777811d7031e201f14669e7caf0f6bb66f"}, "8c21c0d0-cf6b-4a03-80c6-66f9bb5c153b": {"doc_hash": "0f061e4c66b6baea9909eb681e9c49e60021fffdcbb1a9ae6f1908a7e56b644d", "ref_doc_id": "44dba472d4169443c3401d14c5631465eab0725f"}, "3e553d2d-28b3-4576-bd6a-e4833517ee4b": {"doc_hash": "cea98ec4625ad2ec5438ecbca5948aebb90d33480d61d0ce892ff8e93cbebaaf", "ref_doc_id": "44dba472d4169443c3401d14c5631465eab0725f"}, "290972af-dd34-49b7-8c81-43d512b3d500": {"doc_hash": "8d3f13c03446e27b399655d112b6bf5e24203303e81d5d96433904f38c4b0f27", "ref_doc_id": "46bf7a7099a0456a4f30cb6bcb5ac0ce59bac8aa"}, "d9f52f29-8ac7-40e5-81e6-b4e1cf135561": {"doc_hash": "b7b64ad1aead9e30066ded87c32b28a0b7968900a12c314f05422512e856cb72", "ref_doc_id": "5777d9a1f01e0f708202499624b34da6c33300a5"}, "6558c03c-1f31-416b-9f95-f9a8e51c90f8": {"doc_hash": "0072dae9f054e195140bf3f1fff2c58b811cd87b56d47d08fcfab4f9d019791f", "ref_doc_id": "728ccebf2fce50b57bf06af7f576fcf7aaf61e19"}, "30cc6acc-df1d-4249-be11-3c1e90cbc9a9": {"doc_hash": "e2df54bcff160038c82fbe1f7a6f925a7aeead515ec48fdeeb6027b6d910dfe0", "ref_doc_id": "728ccebf2fce50b57bf06af7f576fcf7aaf61e19"}, "d0afa067-1613-42a3-a67d-d440ad8901b2": {"doc_hash": "77ee8d4a95928507632d3b2e6fbdb25942b52b308f15153264f5b9b1eff64b8a", "ref_doc_id": "61b1a0d2bcdf1ffef5187fde6c0e0ae4450e6db4"}, "e319874d-fc82-4b5a-a288-52af9c990a82": {"doc_hash": "bae9d00b3c1da0b24a201c7443a371ce99dbd6b07b75538992b59cf17c4e787b", "ref_doc_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a"}, "81d03ce7-6627-45a7-8a9f-66374ec48eb0": {"doc_hash": "35d69d0ac8405fae54303f8d65863e65be1d6ac2d2486785623e2391363bf2f6", "ref_doc_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a"}, "c27b9bc8-c421-4da3-8b0b-2a72343235a9": {"doc_hash": "9f80e5cc8dbfe224cab229fe9316d4043a20eb5f69540f0593c71e294802b363", "ref_doc_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a"}, "4b7ca0e6-132d-4928-a648-c728ad3171a1": {"doc_hash": "dc58d4b355c7021e9ed7a4192b8dec7aa23f11f3f08c6e07219dbdcdf6e0b3c8", "ref_doc_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a"}, "afdaba8b-1eea-488d-b4da-ebab174cec70": {"doc_hash": "fe72dbe3608876c21b26f2b4b34f70f669f1ae4053261f2a695c36822523cf8d", "ref_doc_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a"}, "8c626d5e-a15a-43a8-be3f-18a0a556655a": {"doc_hash": "502f03d9715a3df01b8ede451909869b88e383a20c216c0d5a04b75cffc65c41", "ref_doc_id": "1ec44e4994c4535771b70c620e0ac2eb1a60662a"}, "98988af5-7ed4-42ce-a24e-b2eae1327a55": {"doc_hash": "2bd538d38672f5ef397d089a423503ace67dd04f5b5d863dcc47d78b75629718", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "bfa861a8-eead-4df4-b4b9-0987c129ad3e": {"doc_hash": "440e975e87495e13306fe28be45a617e134ed9b87248bcd34d77fa8b6f7b352a", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "31d74118-6633-45bd-939d-20eb4f24a622": {"doc_hash": "f54660b0ac1aecb6ae88f111bbca94ca78f7e28f145a8ae35955d2637c0b7417", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "97ad579d-fce9-484d-8665-0c18c2151d28": {"doc_hash": "bf3e4fc756f86d78388550d1837e700a75bdf2ec0261c5c6b9512be5b97f02cf", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "2bc3ad40-0112-49aa-8bf1-7aa42d89642f": {"doc_hash": "c9ca6cc8937b9f169648e0fec41940f0222705ae0eb53f11b1da90ed743bcc94", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "cd444059-4829-454b-84a7-5890a6c92a80": {"doc_hash": "a1eef3694e9fe6fd2f21d88e7783423b95e297cc0af67ed896147cf63d8d84a4", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "b5736017-4998-4aea-ba88-7fcd6b544808": {"doc_hash": "37a1dbcf312086c692ff2d844da4123f3a0e0753622166dd1419c95b54488044", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "cc232f0b-69fb-45af-bbc9-77e97ff5e84c": {"doc_hash": "d486dcca18b6179017b988caba2c46e5e56c082c8a8908d663d42c4e297e0d8c", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "45e5b4de-d494-4270-880b-1fa853e039ab": {"doc_hash": "efccb38ecc7fa4bedf62dddfe4f1817307ed26fb2adfe2861b6ffa97ccd30245", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "49ce9594-5f43-407c-9e20-4366e340ff72": {"doc_hash": "0289a609a73837ab3d038af9bb6c727dfaa3e6c4a5073ac05d152186102b8c72", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "6868a6e6-abd1-4a18-a7d4-0a8bc78aaa29": {"doc_hash": "6247c2b7bec0659953bee83111a106e1d5a56b9baf191f9fc90ab09a309c247c", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "824f712c-18a8-49e1-b746-94906a75383d": {"doc_hash": "72ac9c6a226d10c2ddeeb809525e78d51f4b700cf362e760653e10b38da3dc5e", "ref_doc_id": "29b95163662843db2b80f3ab36f28a1d05ca76ea"}, "d35927e0-0092-4e38-b844-9f1d050d2862": {"doc_hash": "59b1cf51c868749231e0562d8c99a5a354e00bfc7b8bc2b3ce721a5a657c3a47", "ref_doc_id": "3756835c7d1eaf6d708e55b30ffc2cb879bb429c"}, "efa12611-2447-4d58-8a08-26a3704535b8": {"doc_hash": "a943d7c12a844c9fa57217796c5838b8dfd46ca375b22353e602189f12232204", "ref_doc_id": "13037c457040479ca229000a12561167ba21e892"}, "8a8f7393-61ee-48f4-a416-8165f53882ac": {"doc_hash": "e5c2a8f710cfafd24fd608163df09005bc4f022c25ae602440fdc75492963bdb", "ref_doc_id": "13037c457040479ca229000a12561167ba21e892"}, "657efc36-df83-4e56-933f-89039a0395a5": {"doc_hash": "b73161384c4eb326e2ce3f0b40aa73b59494723a401cf5bd2de5d8cb7ce1a0f4", "ref_doc_id": "13037c457040479ca229000a12561167ba21e892"}, "72a3bcd2-7945-444e-965b-c18a766f7a5d": {"doc_hash": "b40caad1fa14017a011e7e81edd35fed396a4e8b76e3d7541790870a3ff99b96", "ref_doc_id": "13037c457040479ca229000a12561167ba21e892"}, "1d32a6ac-f244-4992-bd34-440a2182a532": {"doc_hash": "37724116fd79334c8471fc4693766e1d5bcfdc1820b6cf608447eca8fe921b0e", "ref_doc_id": "13037c457040479ca229000a12561167ba21e892"}, "01a3a473-0083-40fa-8a11-75f394861e2d": {"doc_hash": "4dbb499390887428a0c8568a3f46a5a2d56d14c8c82dfceaea91bb40378f981e", "ref_doc_id": "cd8f1ca277aa1815e930b0daa1b13b2f678cfe96"}, "15707a2c-3f1e-4723-86fd-67091dcf67ba": {"doc_hash": "29219bc57f7adc0b338dab601f016d1b84d51537a7cde9102a4489f0955fe3ab", "ref_doc_id": "21b8dd44a82617ea07b8f295ca1d583d2541b8f1"}, "8c778fa6-168d-4457-a731-fcefc050a6d3": {"doc_hash": "db85ad406ce9957c2cebb636256c0fc57a5c15997213b97f2c104dc1469f4e19", "ref_doc_id": "21b8dd44a82617ea07b8f295ca1d583d2541b8f1"}, "52e7f52d-519c-48fd-85e3-73a0d296b135": {"doc_hash": "0dd2ac184799a0582127610a808432fd6215443a12567ee83cf1207b19bf5d11", "ref_doc_id": "ebf0bea9e9892ed101bd9eb7114fe44a4d4679cb"}, "2fb6d4f0-f671-424e-8640-8accc6adab3b": {"doc_hash": "c9ec9783a0edc9c950b3ef3442f152cd6a98ea0283b9c3505548f79efa707f9f", "ref_doc_id": "0d29918512506aca78286a2e0b139a9fc88580df"}, "96dca026-efdc-4ac7-90bf-906b643f1573": {"doc_hash": "8bb44d40620f4354ebe12d9a8832cd965ac0ab7ac33713fc20cb0575f012c9c4", "ref_doc_id": "cfafd62b0e9f5cdb3eb121d03a96198ae64feff9"}, "a27331a1-1bad-4ccd-9bd0-429465d80172": {"doc_hash": "f3ee53f6ae3a67f583202e210b25902cd5dd562c3be54ceaf56b149725644a22", "ref_doc_id": "d3554ad15f7802465c8587c648b3494e572ab167"}, "6dcacd63-f1ce-4905-a2b3-a367a12576da": {"doc_hash": "30fcc40a65b4e0ddfbcb679197b3d3a85469b7ad7d0f833920306039095a0999", "ref_doc_id": "d53d8b47de2c7a38bcb395a4c7e80275684dff56"}, "a0d2eca7-f20c-49eb-9cd7-6a80d576903c": {"doc_hash": "45079b1b4fe5356c59fb205d2b7f4846d1bfd15ddd5bb4dedff3c5a90ab81bfd", "ref_doc_id": "d53d8b47de2c7a38bcb395a4c7e80275684dff56"}, "3b1f1bc5-e57a-4694-b624-7aa73d18f9d9": {"doc_hash": "2aed22e6b0373ae611ade14594ccf2d3bc067bb16a4ebf264d602b44222fe330", "ref_doc_id": "b6249f3ec1980d3dbe7f502f0fb462939dfd3e88"}, "9d950bef-82ba-44c8-8f18-975040b880f2": {"doc_hash": "7f978dd29a114a69e94c4c651868cc9741f59794b5e8cac87d0230da25a2e4ee", "ref_doc_id": "ea304e8e84d3da5200044d14a761bd396d69d0b7"}, "6e0f2a6d-1eb9-4f30-b31b-55ae0e800e41": {"doc_hash": "4e4212570f5dba1c4f364cd21098c148792e2ca3b129d8c932eb82d81412f658", "ref_doc_id": "ea304e8e84d3da5200044d14a761bd396d69d0b7"}, "f1bc3745-7815-4391-8174-7fc8d283a3b4": {"doc_hash": "17c3a23b6610863626930f439e929215820eb2be2cc10d76f418da720e6913af", "ref_doc_id": "ea304e8e84d3da5200044d14a761bd396d69d0b7"}, "a607c6b3-397e-4db9-b6ae-e7abd7a7dcca": {"doc_hash": "afbc37739612af5d47670bdefce216f94c051c4ad18c56a8daa40c682a907a41", "ref_doc_id": "acf49db9f42ba4cd549ae928e76ce7cb58798434"}, "2ccd8336-2484-4eae-97b2-12f9eae6c1e6": {"doc_hash": "8c2a894182ac293af3c0511a6dc1048d3c1996f2dfc3977868df77f862d287e8", "ref_doc_id": "f5fcc24cbf7d36cf038767bf3b95393e7aa48a80"}, "ee52c155-2606-41fe-9846-bfd036b7b5e2": {"doc_hash": "37878d6166d814ee25fa453f817f8e1aa40480f83c8d9668b660d40a73c781a2", "ref_doc_id": "f5fcc24cbf7d36cf038767bf3b95393e7aa48a80"}, "d1231944-90c4-4c97-b5d7-fde7e74ccc21": {"doc_hash": "93025b14d8c1e6217b721dd899430f612317e057090fc2f6114a19da6a8a7163", "ref_doc_id": "f5fcc24cbf7d36cf038767bf3b95393e7aa48a80"}, "45c9d7ae-cd4f-4588-b588-fdfee152552e": {"doc_hash": "493af2c9cdd9d94b8c29c900a94de570ffa4d8f8f75abfc60b2b6366bc1a9cd3", "ref_doc_id": "a72939a6dbb1b7596cc1a485eba0eca5403d8664"}, "2b2b1191-5427-4142-af6c-010b7c64bcbc": {"doc_hash": "182e08525bcb5ef4c46865dc278c29d2f6e48c120a5f511a48d90efaad4b7b96", "ref_doc_id": "20745498e5bc3c98851326b15f795894aa05d49b"}, "7a0954fe-8e8c-402f-b89c-ebb4b163df9e": {"doc_hash": "691f2406a16aab7b445570345527c7013f55c9f3f66960a4814bcb7cfd4a3a9e", "ref_doc_id": "f13a08b36ae7d2805ad45b265730dd3fdea1243c"}, "8c1168ba-de1c-4147-a569-fbae40ef8a50": {"doc_hash": "d6e738591e9790f92d463007064905d86767f9a75fc8448f78c73a26d2989153", "ref_doc_id": "f13a08b36ae7d2805ad45b265730dd3fdea1243c"}, "7b5e823d-e520-446c-962d-0235c8a2c5c5": {"doc_hash": "4f8572d54920ab8a787711813fd3f11b34010c295dedc0dff92ae2e33f185f50", "ref_doc_id": "f13a08b36ae7d2805ad45b265730dd3fdea1243c"}, "6b920c82-667e-473f-a2c0-2b881870a3f2": {"doc_hash": "e289a484c7779dde6acdfc6913943639913353dccbd46535d501d5e64a701e17", "ref_doc_id": "f13a08b36ae7d2805ad45b265730dd3fdea1243c"}, "64db79af-099c-48d7-b5b9-85fd7709df64": {"doc_hash": "77168dcafbf13e430f3d2a0b034fe1575727b595b0d7250324b9dac393d78ff6", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "258808ef-4ff1-4f26-a477-640304ad78bb": {"doc_hash": "58ef4175337b20009ab881e514f70a12a1b68e7a9e8639b5fe9caf12aa82ab54", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "5772201a-3ccc-4a5d-acaf-a9eb0639fe43": {"doc_hash": "16cbf0769ba3acc8f1cbf901b586a48d423ebe0edb15a930f757d2501b9337a8", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "a7854179-f654-4452-bee0-2e6374805dfc": {"doc_hash": "65442e1f5b1d7e30ab831d52bafc5a6dccc90a54d9303491c618f1a72b4b1012", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "c536587f-791c-4772-8240-4ba228f5940c": {"doc_hash": "ed60503561b1e8051a9255043c10f3aee6341319c7742e5a831390469dc0c8c1", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "b3d9f8c2-cbbc-48cc-9629-8c91b5fdb8bd": {"doc_hash": "1ccc877fec10b7f3f269d3da92912e75606c343478f62b1b61a2eeed659c4861", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "c0afb204-4477-4656-aaf8-694298244255": {"doc_hash": "7563d755ebf8542d6549debba0dcc61f251f4476621fde55fa9e497f3a898167", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "de9ae151-e2de-44ac-b01a-b5e2646da0cb": {"doc_hash": "3b3b2404236e9c496624c1a6dec4074edea25cb8648bba77aa9149d91ef0adf3", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "b8cc04a2-dcdc-4906-93a6-5680da2419b9": {"doc_hash": "0dff16b5e8942273a8ab1a6e0637f32c4dd01fc3804d32b398dbe0bae8a9a93b", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "2e13ebfd-5b4e-4cf0-ada8-90a14f3b2460": {"doc_hash": "3c18d0612ccf6edaee3d59a8dd1034b7e32e2f5163a8b873e4c400d693fde3a9", "ref_doc_id": "9113ec251c368aadd2aeee6ca8c23dbdc02817c3"}, "85dcd5ea-0c20-4794-b57b-921ea5d26d07": {"doc_hash": "48e02c11d5d6610f05b5636ebab188ec9643c477c2f718beedcec6657c6f7b53", "ref_doc_id": "4cbd683f9affb0df508057a7233bae3a8bffec39"}, "dc9241f9-6eab-4ef3-9b1e-8d8b848793b1": {"doc_hash": "ccc21f39f493252a924bff943196576db455a8f5b33ad30cb6e7e30daeffcd2b", "ref_doc_id": "4cbd683f9affb0df508057a7233bae3a8bffec39"}, "cc31c75a-073c-454c-afa5-386cca33651b": {"doc_hash": "70e695f95b88b1b7b3a9fecd3a3dffbb55533fb180bc04e1c89ba8403a6ba2e0", "ref_doc_id": "4cbd683f9affb0df508057a7233bae3a8bffec39"}, "cfb0fbc0-d960-4800-9444-02cb6807cec8": {"doc_hash": "380ac8e195e74c5537e3a38ab2cbc8e86f98eef0e8faa2f6d80e80914ffc8214", "ref_doc_id": "eceb8a6cb9b239b8ee4510c7381e95dd00442ca1"}, "32cff3f3-52e3-49c6-aed1-ef7cf3e01a80": {"doc_hash": "5de9630e44a732be44aa03351570eb7983a29b1a8796c842c68f0f5f8250d4ed", "ref_doc_id": "985d3202c070a65b9b73f15defa8ce3e9f8185e5"}, "e752d1df-66ca-49a5-a082-c5c5924c6bde": {"doc_hash": "57118d835f901c436483f98a394dfcb1eb0a1187d0090c929e700f1b324dabd1", "ref_doc_id": "985d3202c070a65b9b73f15defa8ce3e9f8185e5"}, "5dadfaa5-7c77-47b0-8095-e55f75b47de2": {"doc_hash": "337bf849ee59d0c6b66896e60f95cb49219f435317e6c830f03d88e6593f7746", "ref_doc_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d"}, "f3cec787-bb9f-4a16-b5c1-8f2b60e67e51": {"doc_hash": "1f997b9a576fea139ebfaec7cd506d22cbc04d807984d7c09d743bdba94b5d8d", "ref_doc_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d"}, "cd8c80cc-4f13-4cd6-bce3-e120fd8e252e": {"doc_hash": "8735c831ccd45785a0e574afa29aafc5acccb2eb7c3b375bf9295a146fd985a1", "ref_doc_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d"}, "8fd22a72-b9d0-44e8-a82d-daad4ee4d958": {"doc_hash": "7fed7d5ce1f930ad0307c1b17b19cf4595bbca56064f03ed76a71eff55b5a5eb", "ref_doc_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d"}, "5d5471e1-ed95-4deb-83ec-eb06ae3147bf": {"doc_hash": "0a7bb6baebb20ffe9889b60d89c30f15c7c2292e4c85c2402731e1b6658acb3e", "ref_doc_id": "76970e90ac64f5d1caf2abc934d1f3fe5246213d"}, "e43348b4-496d-466c-ab26-1b0027fb8ede": {"doc_hash": "bd56047409f1326b0ecdedaf5800e7851a717daee851dd8bf687acb697508232", "ref_doc_id": "80cb7a9fc2697667f34d5456b06fa5642394affe"}, "c586ca61-5735-49bc-ac31-dbc9677f884a": {"doc_hash": "9a3464b3eb7355697b6d749f7d6509648b81876bddbc6e669731d488f3d47d90", "ref_doc_id": "a7608fa06109bba4ab1e5ba7ec192deac6eff023"}, "5f685536-34c9-4246-b8de-d9babb267b3c": {"doc_hash": "fe5444432a4fc6e6cb01f9daf971fcaad9ba921f0c6a2bf28f0f925bab729503", "ref_doc_id": "32cce9075d6bdc38f5389e97b109b8163e31b622"}, "5863ca78-b666-457d-ac16-851c2e683cc9": {"doc_hash": "28d47b5cb5728a59975802c9f1843993ae601cfdbd6b36a9557f594099f4a00d", "ref_doc_id": "417c7df2531ebb4b15a98589c62a77a1b2baab70"}, "36c049ca-bfa2-4835-9c50-b1e5f333e759": {"doc_hash": "a636c9cff9c8364de4094fcc5c8b8e24fcf8cf5ec3f30acd0a8a273db6cce2cd", "ref_doc_id": "417c7df2531ebb4b15a98589c62a77a1b2baab70"}, "41bc4137-2d62-416e-9f3d-c92b75b40965": {"doc_hash": "640796281170a32ebc0fbbf4947a3fb6d221d948eab8d87e0d5c8be4b2a30307", "ref_doc_id": "417c7df2531ebb4b15a98589c62a77a1b2baab70"}, "4a242449-8afc-476b-b24b-71b5aadb4cfe": {"doc_hash": "35e4cb4aeed07ec3c277d134d44a58f4f0d3ca6b1174ca05d13aaa25cd3c9d8b", "ref_doc_id": "fa38de4744f41b9080a3dc7bcbcea59b5d78862e"}, "1348302c-f1a8-4312-90db-958594383c5a": {"doc_hash": "c713b8bdd885040318b7967d84129dddc6e97db7c75533f5001ff6f537c1afb5", "ref_doc_id": "b4de173267fcf10f8781c8c28c618365c3d03b0d"}, "f1aaf379-77fe-4e70-adbe-adc62ec58f48": {"doc_hash": "aaf4aae6ce15b1eb6ae7892c58fb840f92a77f2ee3fce7db63dcf85d433bf64c", "ref_doc_id": "b4de173267fcf10f8781c8c28c618365c3d03b0d"}}}